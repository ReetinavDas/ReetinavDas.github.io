[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi there! My name is Reetinav Das, and I’m a fourth year student at UCLA majoring in Applied Mathematics with a Specialization in Computing and minoring in Data Science Engineering! (That’s a mouthful.) My primary interests are in computer vision and machine learning, and I especially love when I get the chance to combine the both of them."
  },
  {
    "objectID": "posts/web-scraping/index.html",
    "href": "posts/web-scraping/index.html",
    "title": "Webscraping with Scrapy!",
    "section": "",
    "text": "We will learn how to scrape the web using the scrapy package. We’ll explore this by figuring out how to acquire data from a movie website.\nWe will start by creating a file named “tmdb_spider.py”. From there we will add these lines:\n# to run \n# scrapy crawl tmdb_spider -o movies.csv\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    start_urls = ['https://www.themoviedb.org/movie/872585-oppenheimer']\nAs you can see from the starting url, we will be analyzing the movie Oppenheimer. What we plan to do is to make a recommender which uses information of the cast to recommend new movies. To do this, we need to create “spiders,” which will “crawl” the web and extract the information we need! We will have three methods which each do an important task. Our first class method will be the easiest:\n    def parse(self, response):\n        box = response.css(\"section.panel.top_billed.scroller\")\n        button = box.css(\"p.new_button\")\n        link = \"https://www.themoviedb.org\" + button.css(\"a::attr(href)\").get()\n        yield scrapy.Request(url=link, callback=self.parse_full_credits)\nThis parse method is responsible for starting from the page of the movie and navigating to the Full Cast & Crew page. Let’s go through this code line by line. Our response would be the request of the page specified in the start url. In other words, this is the request to the Oppenheimer movie description page. If we inspect element on the page, we can see a box where there is a Full Cast & Crew button. Perfect! We save that as the variable box. We then access that button through the next line and save it as button. We then save the whole thing into a link. We must note that the link that this returns is a relative link, which means we need to manually add the beginning of the address like we have done above. Once we have saved the link we can yield a scrapy.Request object. This takes in the arguments url, which is self-explanatory, and callback. We use callback to call our next function: parse_full_credits. This means that we will pass the Full Cast & Crew page into this method, so we will be looking at that one next.\n    def parse_full_credits(self, response):\n        actor_links = response.css(\"ol.people.credits\").css(\"a::attr(href)\").getall()\n        for actor_link in actor_links:\n            link = \"https://www.themoviedb.org\" + actor_link\n            yield scrapy.Request(url=link, callback=self.parse_actor_page)\nThis one doesn’t look too complicated either. For this method, we know we start from the page containing the cast. Therefore, we want to get links to all the actors on this page and pass those into the parse_actor_page method. We first start by saving the links into the variable actor_links. We make sure to use .getall() so that we get a list of all the actor links. Then, we iterate over all of the links and for each link, we call the scrapy.Request contructor and pass it into parse_actor_page. Now let’s take a look at that final method.\n    def parse_actor_page(self, response):\n        actor = response.css(\"h2.title a::text\").get()\n        movies = response.css(\"table.card.credits a.tooltip bdi::text\").getall()\n        for movie in movies:\n            yield {\n                \"actor\": actor,\n                \"movie_or_TV_name\": movie\n            }\nThere’s a little bit more that we need to look at. We need to extract the actor’s name in the first line, which we get from one of the headers. Then we find an html card which contains the names of the movies the actor has acted in, and then we extract those names and save them into movies. We then iterate over each movie, and yield a mapping, where the actor is mapped to the movie. When we yield this, we can create a .csv file which will have a column of actors and their corresponding movies. From there on we can create our recommender. Before we do that, let’s take a look at the bash command we need to run first.\nscrapy crawl tmdb_spider -o results.csv\nNote the name tmdb_spider. That’s the value of the variable name we had in the beginning! The -o means that we would open a file named results.csv. If the file doesn’t exist yet, it’ll automatically be created. Once we have our csv file, we can open it using pandas!\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = pd.read_csv(\"results.csv\")\n\n\ndf\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nCillian Murphy\nSmall Things Like These\n\n\n1\nCillian Murphy\nUntitled Peaky Blinders Film\n\n\n2\nCillian Murphy\nPeaky Blinders: A Peek Behind the Curtain\n\n\n3\nCillian Murphy\nKensuke's Kingdom\n\n\n4\nCillian Murphy\nOppenheimer\n\n\n...\n...\n...\n\n\n3922\nEmily Blunt\nKate Warne\n\n\n3923\nEmily Blunt\nPain Hustlers\n\n\n3924\nEmily Blunt\nThe English\n\n\n3925\nEmily Blunt\nDon Jon\n\n\n3926\nEmily Blunt\nYour Sister's Sister\n\n\n\n\n3927 rows × 2 columns\n\n\n\nThis is great! As you can see, we have a list of movies/TV shows with their corresponding actors. Now, we want to group them up by movies and see how many common actors there are. We’ll then sort these by number of actors in descending order. Based on this, we can recommend movies to people!\n\nmovies = df.groupby(\"movie_or_TV_name\")[\"actor\"].count().reset_index()\n\n\nmovies = movies.sort_values(by=\"actor\", ascending=False)\nmovies.head(10)\n\n\n\n\n\n\n\n\nmovie_or_TV_name\nactor\n\n\n\n\n1729\nOppenheimer\n81\n\n\n1380\nLate Night with Seth Meyers\n10\n\n\n445\nCSI: Miami\n9\n\n\n1275\nJimmy Kimmel Live!\n9\n\n\n1358\nLIVE with Kelly and Mark\n9\n\n\n3050\nWithout a Trace\n8\n\n\n2610\nThe Oscars\n8\n\n\n1386\nLaw & Order: Special Victims Unit\n8\n\n\n746\nEcce pirate\n8\n\n\n2449\nThe Graham Norton Show\n8\n\n\n\n\n\n\n\nThis is good news, and we can see that our code worked! We can tell this by looking at the top movie on our list, which is Oppenheimer! That’s no coincidence as the movie where we got all the actors from would definitely have the highest amount of actors from that movie. So the non-trivial ones would actually be the ones right below Oppenheimer, and it looks like Tenet would be the first movie to recommend.\n\nsns.set_style(\"whitegrid\")\nsns.set_context(\"talk\")\nplt.figure(figsize=(35,16))\nsns.barplot(data=movies.head(11)[1:], x=\"movie_or_TV_name\", y=\"actor\", dodge=True)\nplt.show()\n\n\n\n\nHere is a visualization of the first 10 movies/shows that we’d recommend watching if your favorite movie is Oppenheimer. Enjoy!"
  },
  {
    "objectID": "posts/web-scraping/index.html#webscraping-with-scrapy",
    "href": "posts/web-scraping/index.html#webscraping-with-scrapy",
    "title": "Webscraping with Scrapy!",
    "section": "",
    "text": "We will learn how to scrape the web using the scrapy package. We’ll explore this by figuring out how to acquire data from a movie website.\nWe will start by creating a file named “tmdb_spider.py”. From there we will add these lines:\n# to run \n# scrapy crawl tmdb_spider -o movies.csv\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    start_urls = ['https://www.themoviedb.org/movie/872585-oppenheimer']\nAs you can see from the starting url, we will be analyzing the movie Oppenheimer. What we plan to do is to make a recommender which uses information of the cast to recommend new movies. To do this, we need to create “spiders,” which will “crawl” the web and extract the information we need! We will have three methods which each do an important task. Our first class method will be the easiest:\n    def parse(self, response):\n        box = response.css(\"section.panel.top_billed.scroller\")\n        button = box.css(\"p.new_button\")\n        link = \"https://www.themoviedb.org\" + button.css(\"a::attr(href)\").get()\n        yield scrapy.Request(url=link, callback=self.parse_full_credits)\nThis parse method is responsible for starting from the page of the movie and navigating to the Full Cast & Crew page. Let’s go through this code line by line. Our response would be the request of the page specified in the start url. In other words, this is the request to the Oppenheimer movie description page. If we inspect element on the page, we can see a box where there is a Full Cast & Crew button. Perfect! We save that as the variable box. We then access that button through the next line and save it as button. We then save the whole thing into a link. We must note that the link that this returns is a relative link, which means we need to manually add the beginning of the address like we have done above. Once we have saved the link we can yield a scrapy.Request object. This takes in the arguments url, which is self-explanatory, and callback. We use callback to call our next function: parse_full_credits. This means that we will pass the Full Cast & Crew page into this method, so we will be looking at that one next.\n    def parse_full_credits(self, response):\n        actor_links = response.css(\"ol.people.credits\").css(\"a::attr(href)\").getall()\n        for actor_link in actor_links:\n            link = \"https://www.themoviedb.org\" + actor_link\n            yield scrapy.Request(url=link, callback=self.parse_actor_page)\nThis one doesn’t look too complicated either. For this method, we know we start from the page containing the cast. Therefore, we want to get links to all the actors on this page and pass those into the parse_actor_page method. We first start by saving the links into the variable actor_links. We make sure to use .getall() so that we get a list of all the actor links. Then, we iterate over all of the links and for each link, we call the scrapy.Request contructor and pass it into parse_actor_page. Now let’s take a look at that final method.\n    def parse_actor_page(self, response):\n        actor = response.css(\"h2.title a::text\").get()\n        movies = response.css(\"table.card.credits a.tooltip bdi::text\").getall()\n        for movie in movies:\n            yield {\n                \"actor\": actor,\n                \"movie_or_TV_name\": movie\n            }\nThere’s a little bit more that we need to look at. We need to extract the actor’s name in the first line, which we get from one of the headers. Then we find an html card which contains the names of the movies the actor has acted in, and then we extract those names and save them into movies. We then iterate over each movie, and yield a mapping, where the actor is mapped to the movie. When we yield this, we can create a .csv file which will have a column of actors and their corresponding movies. From there on we can create our recommender. Before we do that, let’s take a look at the bash command we need to run first.\nscrapy crawl tmdb_spider -o results.csv\nNote the name tmdb_spider. That’s the value of the variable name we had in the beginning! The -o means that we would open a file named results.csv. If the file doesn’t exist yet, it’ll automatically be created. Once we have our csv file, we can open it using pandas!\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = pd.read_csv(\"results.csv\")\n\n\ndf\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nCillian Murphy\nSmall Things Like These\n\n\n1\nCillian Murphy\nUntitled Peaky Blinders Film\n\n\n2\nCillian Murphy\nPeaky Blinders: A Peek Behind the Curtain\n\n\n3\nCillian Murphy\nKensuke's Kingdom\n\n\n4\nCillian Murphy\nOppenheimer\n\n\n...\n...\n...\n\n\n3922\nEmily Blunt\nKate Warne\n\n\n3923\nEmily Blunt\nPain Hustlers\n\n\n3924\nEmily Blunt\nThe English\n\n\n3925\nEmily Blunt\nDon Jon\n\n\n3926\nEmily Blunt\nYour Sister's Sister\n\n\n\n\n3927 rows × 2 columns\n\n\n\nThis is great! As you can see, we have a list of movies/TV shows with their corresponding actors. Now, we want to group them up by movies and see how many common actors there are. We’ll then sort these by number of actors in descending order. Based on this, we can recommend movies to people!\n\nmovies = df.groupby(\"movie_or_TV_name\")[\"actor\"].count().reset_index()\n\n\nmovies = movies.sort_values(by=\"actor\", ascending=False)\nmovies.head(10)\n\n\n\n\n\n\n\n\nmovie_or_TV_name\nactor\n\n\n\n\n1729\nOppenheimer\n81\n\n\n1380\nLate Night with Seth Meyers\n10\n\n\n445\nCSI: Miami\n9\n\n\n1275\nJimmy Kimmel Live!\n9\n\n\n1358\nLIVE with Kelly and Mark\n9\n\n\n3050\nWithout a Trace\n8\n\n\n2610\nThe Oscars\n8\n\n\n1386\nLaw & Order: Special Victims Unit\n8\n\n\n746\nEcce pirate\n8\n\n\n2449\nThe Graham Norton Show\n8\n\n\n\n\n\n\n\nThis is good news, and we can see that our code worked! We can tell this by looking at the top movie on our list, which is Oppenheimer! That’s no coincidence as the movie where we got all the actors from would definitely have the highest amount of actors from that movie. So the non-trivial ones would actually be the ones right below Oppenheimer, and it looks like Tenet would be the first movie to recommend.\n\nsns.set_style(\"whitegrid\")\nsns.set_context(\"talk\")\nplt.figure(figsize=(35,16))\nsns.barplot(data=movies.head(11)[1:], x=\"movie_or_TV_name\", y=\"actor\", dodge=True)\nplt.show()\n\n\n\n\nHere is a visualization of the first 10 movies/shows that we’d recommend watching if your favorite movie is Oppenheimer. Enjoy!"
  },
  {
    "objectID": "posts/heat-diffusion/index.html",
    "href": "posts/heat-diffusion/index.html",
    "title": "Heat Diffusion Modeling",
    "section": "",
    "text": "We will be conducting a simulation of two-dimensional heat diffusion in a multitude of ways. We will first set the initial parameters.\n\nN = 101\nepsilon = 0.2\n\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n# construct initial condition: 1 unit of heat at midpoint\nu0 = np.zeros((N,N))\nu0[int(N/2), int(N/2)] = 1.0\nplt.imshow(u0)\nplt.show()\n\n\n\n\nAs we can see, we have a point in the very middle of the space with heat. From this point, the heat will dissipate outwards according to the diffusion equation in a circular fashion. Now let’s create some functions that will simulate this.\n\n#initialize A\nA = np.zeros((N**2,N**2))\n\nfor k in range (N**2):\n    #create main diagonal\n    A[k,k]=-4\n    \n    #create 1st lower diagonal (placing zeros every Nth entry)\n    if k-1&gt;=0:\n        if k%N==0:\n            A[k,k-1]=0\n        else:\n            A[k,k-1]=1\n        \n    #create 1st upper diagnonal (placing zeros every Nth entry)\n    if k+1&lt;=N**2-1:\n        if (k+1)%N==0:\n            A[k,k+1]=0\n        else:\n            A[k,k+1]=1\n    \n    #create Nth upper and lower diagonals\n    if N+k&lt;=N**2-1:\n        A[k,N+k]=1\n        A[N+k,k]=1\nA\n\narray([[-4.,  1.,  0., ...,  0.,  0.,  0.],\n       [ 1., -4.,  1., ...,  0.,  0.,  0.],\n       [ 0.,  1., -4., ...,  0.,  0.,  0.],\n       ...,\n       [ 0.,  0.,  0., ..., -4.,  1.,  0.],\n       [ 0.,  0.,  0., ...,  1., -4.,  1.],\n       [ 0.,  0.,  0., ...,  0.,  1., -4.]])\n\n\n\ndef graph(sols_arr, row_max, col_max, interval):\n    '''\n    plots solutions gathered throughout the call\n    '''\n    #create fig and initialize variables\n    fig,axarr = plt.subplots(row_max,col_max,sharex=True,sharey=True)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    col = 0\n    row = 0\n    i=0\n    \n    #plot each updated heatmap in correct grid spot\n    for u in sols_arr:\n        i+=1\n        axarr[row,col].imshow(u)\n        axarr[row,col].set_xlabel(f'Iteration {interval*i}')\n        if col == col_max-1:\n            col=0\n            row+=1\n        else:\n            col+=1\n\n\ndef heat_matmul(A, epsilon, N, start_mat, iterations, interval):\n    '''\n    Runs a heat diffusion simulation using matrix mulitplication and plots it\n    '''\n    #initialize values needed\n    sols=[]\n    u = start_mat\n    \n    #update the matrix of heat values over and over\n    for i in range(1,1+iterations):\n        u = u + epsilon * (A @ u.flatten()).reshape((N, N))\n        \n        #store solutions every so often for plotting\n        if i % interval == 0:\n            sols.append(u)\n            \n    return u, sols\n\n\n%%time\nu1, sols1 = heat_matmul(A, epsilon, N, u0, 2700, 300)\n\nCPU times: user 4min 19s, sys: 4.71 s, total: 4min 24s\nWall time: 1min 51s\n\n\nThis took a whole 4 minutes to run! Let’s see if we can speed up the process.\n\ngraph(sols1, 3,3, 300)\n\n\n\n\nLuckily, we can significantly cut the time using sparse matrices. We know that A is a pentadiagonal matrix, so we can use dia matrix to significantly speed up the process. We run the code for 2700 iterations again, and we’ll compare the times.\n\nfrom scipy.sparse import dia_matrix\nA_dia_matrix = dia_matrix(A)\n\n\n%%time\nu2, sols2 = heat_matmul(A_dia_matrix, epsilon, N, u0, 2700, 300)\n\nCPU times: user 130 ms, sys: 1.61 ms, total: 132 ms\nWall time: 131 ms\n\n\nThis is impressive! Before, the total time was 4 minutes, and now the time to complete the simulation was only 132 milliseconds! We can see that the time complexity of the matrix multiplications was significantly reduced, allowing us to achieve this.\n\ngraph(sols2, 3, 3, 300)\n\n\n\n\nNow, we will use the python package numba to solve this. Instead of using matrix multiplication, we will use a for loop, since numba is best at optimizing these. Now let’s see how this method does compared to the previous two.\n\nfrom numba import jit\n\n\n@jit(nopython=True) #&lt;--- THE BIG DIFFERENCE\ndef heat_explicit(A, epsilon, N, start_mat, iterations, interval):\n    '''\n    Runs a heat diffusion simulation using by hand matrix mulitplication and plots it\n    '''\n    #initialize values needed\n    sols=[]\n    u = start_mat\n    \n    #update the matrix of heat values over and over\n    for k in range(1,1+iterations):\n       \n        #manually calculate matrix multiplication result (heat equation)\n        u_new = np.zeros((N,N))\n        for i in range(N):\n            for j in range(N):\n                tot=-4*u[i,j]\n                if i&gt;0:\n                    tot+=u[i-1,j]\n                if i+1&lt;N:\n                    tot+=u[i+1,j]\n                if j&gt;0:\n                    tot+=u[i,j-1]\n                if j+1&lt;N:\n                    tot+=u[i,j+1]\n                u_new[i,j] = u[i,j] + epsilon * (tot)\n       \n        #store solutions every so often for plotting\n        if k % interval == 0:\n            sols.append(u_new)\n            \n        #update u and repeat\n        u = u_new\n            \n    return u, sols\n\nWe are just going to do a quick test run to make sure that the function is compiled before we actually run this. We do this because the compilation part takes a significant portion of time, and when we are comparing the times of our methods we wouldn’t want that to play a factor in our analysis when we’re just trying to look at the methods.\n\nu3, sols3 = heat_explicit(A, epsilon, N, u0, 10, 10)\n\n\n%%time\nu4, sols4 = heat_explicit(A, epsilon, N, u0, 2700, 300)\n\nCPU times: user 45.4 ms, sys: 484 µs, total: 45.9 ms\nWall time: 45.8 ms\n\n\nThis was less than 50 milliseconds! This is significantly faster than doing the computations using a sparse matrix, and the power of the numba module is truly put on display here. Graphed below is the visualization of the heat diffusion every 300 iterations.\n\ngraph(sols4, 3, 3, 300)"
  },
  {
    "objectID": "posts/heat-diffusion/index.html#heat-diffusion",
    "href": "posts/heat-diffusion/index.html#heat-diffusion",
    "title": "Heat Diffusion Modeling",
    "section": "",
    "text": "We will be conducting a simulation of two-dimensional heat diffusion in a multitude of ways. We will first set the initial parameters.\n\nN = 101\nepsilon = 0.2\n\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n# construct initial condition: 1 unit of heat at midpoint\nu0 = np.zeros((N,N))\nu0[int(N/2), int(N/2)] = 1.0\nplt.imshow(u0)\nplt.show()\n\n\n\n\nAs we can see, we have a point in the very middle of the space with heat. From this point, the heat will dissipate outwards according to the diffusion equation in a circular fashion. Now let’s create some functions that will simulate this.\n\n#initialize A\nA = np.zeros((N**2,N**2))\n\nfor k in range (N**2):\n    #create main diagonal\n    A[k,k]=-4\n    \n    #create 1st lower diagonal (placing zeros every Nth entry)\n    if k-1&gt;=0:\n        if k%N==0:\n            A[k,k-1]=0\n        else:\n            A[k,k-1]=1\n        \n    #create 1st upper diagnonal (placing zeros every Nth entry)\n    if k+1&lt;=N**2-1:\n        if (k+1)%N==0:\n            A[k,k+1]=0\n        else:\n            A[k,k+1]=1\n    \n    #create Nth upper and lower diagonals\n    if N+k&lt;=N**2-1:\n        A[k,N+k]=1\n        A[N+k,k]=1\nA\n\narray([[-4.,  1.,  0., ...,  0.,  0.,  0.],\n       [ 1., -4.,  1., ...,  0.,  0.,  0.],\n       [ 0.,  1., -4., ...,  0.,  0.,  0.],\n       ...,\n       [ 0.,  0.,  0., ..., -4.,  1.,  0.],\n       [ 0.,  0.,  0., ...,  1., -4.,  1.],\n       [ 0.,  0.,  0., ...,  0.,  1., -4.]])\n\n\n\ndef graph(sols_arr, row_max, col_max, interval):\n    '''\n    plots solutions gathered throughout the call\n    '''\n    #create fig and initialize variables\n    fig,axarr = plt.subplots(row_max,col_max,sharex=True,sharey=True)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    col = 0\n    row = 0\n    i=0\n    \n    #plot each updated heatmap in correct grid spot\n    for u in sols_arr:\n        i+=1\n        axarr[row,col].imshow(u)\n        axarr[row,col].set_xlabel(f'Iteration {interval*i}')\n        if col == col_max-1:\n            col=0\n            row+=1\n        else:\n            col+=1\n\n\ndef heat_matmul(A, epsilon, N, start_mat, iterations, interval):\n    '''\n    Runs a heat diffusion simulation using matrix mulitplication and plots it\n    '''\n    #initialize values needed\n    sols=[]\n    u = start_mat\n    \n    #update the matrix of heat values over and over\n    for i in range(1,1+iterations):\n        u = u + epsilon * (A @ u.flatten()).reshape((N, N))\n        \n        #store solutions every so often for plotting\n        if i % interval == 0:\n            sols.append(u)\n            \n    return u, sols\n\n\n%%time\nu1, sols1 = heat_matmul(A, epsilon, N, u0, 2700, 300)\n\nCPU times: user 4min 19s, sys: 4.71 s, total: 4min 24s\nWall time: 1min 51s\n\n\nThis took a whole 4 minutes to run! Let’s see if we can speed up the process.\n\ngraph(sols1, 3,3, 300)\n\n\n\n\nLuckily, we can significantly cut the time using sparse matrices. We know that A is a pentadiagonal matrix, so we can use dia matrix to significantly speed up the process. We run the code for 2700 iterations again, and we’ll compare the times.\n\nfrom scipy.sparse import dia_matrix\nA_dia_matrix = dia_matrix(A)\n\n\n%%time\nu2, sols2 = heat_matmul(A_dia_matrix, epsilon, N, u0, 2700, 300)\n\nCPU times: user 130 ms, sys: 1.61 ms, total: 132 ms\nWall time: 131 ms\n\n\nThis is impressive! Before, the total time was 4 minutes, and now the time to complete the simulation was only 132 milliseconds! We can see that the time complexity of the matrix multiplications was significantly reduced, allowing us to achieve this.\n\ngraph(sols2, 3, 3, 300)\n\n\n\n\nNow, we will use the python package numba to solve this. Instead of using matrix multiplication, we will use a for loop, since numba is best at optimizing these. Now let’s see how this method does compared to the previous two.\n\nfrom numba import jit\n\n\n@jit(nopython=True) #&lt;--- THE BIG DIFFERENCE\ndef heat_explicit(A, epsilon, N, start_mat, iterations, interval):\n    '''\n    Runs a heat diffusion simulation using by hand matrix mulitplication and plots it\n    '''\n    #initialize values needed\n    sols=[]\n    u = start_mat\n    \n    #update the matrix of heat values over and over\n    for k in range(1,1+iterations):\n       \n        #manually calculate matrix multiplication result (heat equation)\n        u_new = np.zeros((N,N))\n        for i in range(N):\n            for j in range(N):\n                tot=-4*u[i,j]\n                if i&gt;0:\n                    tot+=u[i-1,j]\n                if i+1&lt;N:\n                    tot+=u[i+1,j]\n                if j&gt;0:\n                    tot+=u[i,j-1]\n                if j+1&lt;N:\n                    tot+=u[i,j+1]\n                u_new[i,j] = u[i,j] + epsilon * (tot)\n       \n        #store solutions every so often for plotting\n        if k % interval == 0:\n            sols.append(u_new)\n            \n        #update u and repeat\n        u = u_new\n            \n    return u, sols\n\nWe are just going to do a quick test run to make sure that the function is compiled before we actually run this. We do this because the compilation part takes a significant portion of time, and when we are comparing the times of our methods we wouldn’t want that to play a factor in our analysis when we’re just trying to look at the methods.\n\nu3, sols3 = heat_explicit(A, epsilon, N, u0, 10, 10)\n\n\n%%time\nu4, sols4 = heat_explicit(A, epsilon, N, u0, 2700, 300)\n\nCPU times: user 45.4 ms, sys: 484 µs, total: 45.9 ms\nWall time: 45.8 ms\n\n\nThis was less than 50 milliseconds! This is significantly faster than doing the computations using a sparse matrix, and the power of the numba module is truly put on display here. Graphed below is the visualization of the heat diffusion every 300 iterations.\n\ngraph(sols4, 3, 3, 300)"
  },
  {
    "objectID": "posts/fake-news-classification/index.html",
    "href": "posts/fake-news-classification/index.html",
    "title": "Fake News Classification",
    "section": "",
    "text": "In this blog post we will be developing and using a fake news classifier using Tensorflow.\nWe will first import all the packages we’ll need.\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport re\nimport string\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import losses\nfrom tensorflow.keras import utils\n\nfrom tensorflow.keras.layers import TextVectorization\nfrom tensorflow.keras.layers import StringLookup\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nimport nltk\nfrom nltk.corpus import stopwords\n\n# for embedding viz\nimport plotly.express as px\nimport plotly.io as pio\npio.templates.default = \"plotly_white\"\npio.renderers.default=\"iframe\"\nimport matplotlib.pyplot as plt\n\n\ntrain_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true\"\ndf = pd.read_csv(train_url)\n\nLet’s see what our dataset looks like:\n\ndf.head()\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\ntitle\ntext\nfake\n\n\n\n\n0\n17366\nMerkel: Strong result for Austria's FPO 'big c...\nGerman Chancellor Angela Merkel said on Monday...\n0\n\n\n1\n5634\nTrump says Pence will lead voter fraud panel\nWEST PALM BEACH, Fla.President Donald Trump sa...\n0\n\n\n2\n17487\nJUST IN: SUSPECTED LEAKER and “Close Confidant...\nOn December 5, 2017, Circa s Sara Carter warne...\n1\n\n\n3\n12217\nThyssenkrupp has offered help to Argentina ove...\nGermany s Thyssenkrupp, has offered assistance...\n0\n\n\n4\n5535\nTrump say appeals court decision on travel ban...\nPresident Donald Trump on Thursday called the ...\n0\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nLooks interesting! We have the title of the article, the text inside of it, and the label of whether or not it’s fake. Now let’s create a function that will remove stopwords from the title and text columns, and return a tensorflow dataset from the pandas dataframe. We’ll first download the stopwords that we need.\n\nnltk.download(\"stopwords\")\n\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue\n\n\n\nstop = stopwords.words(\"english\")\n\n\nprint(stop[:10]) # visualizing some of the stop words\n\n['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n\n\n\ndef make_dataset(df):\n  stop = stopwords.words(\"english\")\n  # we need to make sure we lowercase the words before checking if its in the stopwords list\n  df[\"title\"] = df[\"title\"].apply(lambda x: \" \".join([word for word in x.split() if word.lower() not in (stop)]))\n  df[\"text\"] = df[\"text\"].apply(lambda x: \" \".join([word for word in x.split() if word.lower() not in (stop)]))\n  features = { \"title\": df[[\"title\"]], \"text\": df[[\"text\"]] }\n  target = { \"fake\": df[[\"fake\"]] }\n  dataset = tf.data.Dataset.from_tensor_slices((features, target))\n\n  return dataset.batch(100)\n\n\ndataset = make_dataset(df)\n\nGreat! We’ve made the dataset. Now let’s do an 80-20 split for validation.\n\ntrain_size = int(0.8*len(dataset))\nval_size = int(0.2*len(dataset))\n\ntrain_data = dataset.take(train_size)\nval_data = dataset.skip(train_size)\n\n\ntrain_data.shuffle(buffer_size = train_size, reshuffle_each_iteration=False)\n\n&lt;_ShuffleDataset element_spec=({'title': TensorSpec(shape=(None, 1), dtype=tf.string, name=None), 'text': TensorSpec(shape=(None, 1), dtype=tf.string, name=None)}, {'fake': TensorSpec(shape=(None, 1), dtype=tf.int64, name=None)})&gt;\n\n\n\nlen(train_data), len(val_data) # sanity check\n\n(180, 45)\n\n\nLet’s think about the base case for a bit. In this case, our base case could just be us being trigger happy and labeling all news as fake news. Lets take a quick look at the distribution of our labels and see how accurate this kind of classifier would be.\n\nlabels = []\nfor i in train_data:\n  labels.extend(i[1][\"fake\"].numpy().reshape(1,-1)[0])\n\n\nlen(labels)\n\n18000\n\n\n\nfake_count = labels.count(1)\n\n\nprint(f\"Proportion of fake labels: {fake_count/len(labels)}\")\n\nProportion of fake labels: 0.5220555555555556\n\n\nThis means that our base classifier would have around a 52.2% accuracy. Let’s see if we can do any better. Let’s start with doing some text vectorization. The code below will make all the strings lowercase and remove all punctuation. We’ll also limit the vocabulary to avoid overcomplicating things and also to ignore words that almost never show up.\n\n#preparing a text vectorization layer for tf model\nsize_vocabulary = 2000\n\ndef standardization(input_data):\n    lowercase = tf.strings.lower(input_data)\n    no_punctuation = tf.strings.regex_replace(lowercase,\n                                  '[%s]' % re.escape(string.punctuation),'')\n    return no_punctuation\n\ntitle_vectorize_layer = TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary, # only consider this many words\n    output_mode='int',\n    output_sequence_length=500)\n\ntext_vectorize_layer = TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary, # only consider this many words\n    output_mode='int',\n    output_sequence_length=500)\n\ntitle_vectorize_layer.adapt(train_data.map(lambda x, y: x[\"title\"]))\ntext_vectorize_layer.adapt(train_data.map(lambda x, y: x[\"text\"]))\n\n\nembedding_title = layers.Embedding(size_vocabulary, 2, name=\"embedding_title\")\nembedding_text = layers.Embedding(size_vocabulary, 2, name=\"embedding_text\")\n\nNow, let’s answer this interesting question: When detecting fake news, is it most effective to focus only on the title of the article, the full text, or both? Let’s start by training only on the title.\n\ntitle_inputs = keras.Input(shape=(1,), name=\"title\", dtype=\"string\")\ntitle_hidden = title_vectorize_layer(title_inputs)\ntitle_hidden = embedding_title(title_hidden)\ntitle_hidden = layers.GlobalAveragePooling1D()(title_hidden)\ntitle_hidden = layers.Dense(80, activation=\"relu\")(title_hidden)\ntitle_outputs = layers.Dense(2, name=\"fake\")(title_hidden)\n\ntitle_model = keras.Model(\n    inputs = title_inputs,\n    outputs = title_outputs\n)\n\n\ntitle_model.compile(\n    optimizer = \"adam\",\n    loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics = [\"accuracy\"]\n)\n\n\ntitle_model.summary()\n\nModel: \"model_4\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n title (InputLayer)          [(None, 1)]               0         \n                                                                 \n text_vectorization_3 (Text  (None, 500)               0         \n Vectorization)                                                  \n                                                                 \n embedding_title (Embedding  (None, 500, 2)            4000      \n )                                                               \n                                                                 \n global_average_pooling1d_3  (None, 2)                 0         \n  (GlobalAveragePooling1D)                                       \n                                                                 \n dense_3 (Dense)             (None, 80)                240       \n                                                                 \n fake (Dense)                (None, 2)                 162       \n                                                                 \n=================================================================\nTotal params: 4402 (17.20 KB)\nTrainable params: 4402 (17.20 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\nhistory_title = title_model.fit(train_data, validation_data=val_data, epochs=30, verbose=True)\n\nEpoch 1/30\n180/180 [==============================] - 7s 36ms/step - loss: 0.6925 - accuracy: 0.5178 - val_loss: 0.6917 - val_accuracy: 0.5266\nEpoch 2/30\n180/180 [==============================] - 1s 5ms/step - loss: 0.6920 - accuracy: 0.5221 - val_loss: 0.6909 - val_accuracy: 0.5266\nEpoch 3/30\n180/180 [==============================] - 1s 5ms/step - loss: 0.6870 - accuracy: 0.5392 - val_loss: 0.6759 - val_accuracy: 0.5278\nEpoch 4/30\n180/180 [==============================] - 1s 8ms/step - loss: 0.6417 - accuracy: 0.7271 - val_loss: 0.5900 - val_accuracy: 0.8517\nEpoch 5/30\n180/180 [==============================] - 1s 6ms/step - loss: 0.5195 - accuracy: 0.8608 - val_loss: 0.4527 - val_accuracy: 0.8721\nEpoch 6/30\n180/180 [==============================] - 1s 6ms/step - loss: 0.3935 - accuracy: 0.8834 - val_loss: 0.3547 - val_accuracy: 0.8809\nEpoch 7/30\n180/180 [==============================] - 1s 6ms/step - loss: 0.3152 - accuracy: 0.8975 - val_loss: 0.2976 - val_accuracy: 0.8896\nEpoch 8/30\n180/180 [==============================] - 1s 5ms/step - loss: 0.2668 - accuracy: 0.9084 - val_loss: 0.2591 - val_accuracy: 0.8986\nEpoch 9/30\n180/180 [==============================] - 1s 6ms/step - loss: 0.2333 - accuracy: 0.9171 - val_loss: 0.2324 - val_accuracy: 0.9078\nEpoch 10/30\n180/180 [==============================] - 1s 6ms/step - loss: 0.2095 - accuracy: 0.9239 - val_loss: 0.2132 - val_accuracy: 0.9135\nEpoch 11/30\n180/180 [==============================] - 1s 6ms/step - loss: 0.1919 - accuracy: 0.9283 - val_loss: 0.1990 - val_accuracy: 0.9164\nEpoch 12/30\n180/180 [==============================] - 1s 6ms/step - loss: 0.1783 - accuracy: 0.9329 - val_loss: 0.1882 - val_accuracy: 0.9211\nEpoch 13/30\n180/180 [==============================] - 1s 6ms/step - loss: 0.1675 - accuracy: 0.9362 - val_loss: 0.1798 - val_accuracy: 0.9252\nEpoch 14/30\n180/180 [==============================] - 1s 7ms/step - loss: 0.1587 - accuracy: 0.9393 - val_loss: 0.1730 - val_accuracy: 0.9285\nEpoch 15/30\n180/180 [==============================] - 1s 7ms/step - loss: 0.1514 - accuracy: 0.9417 - val_loss: 0.1675 - val_accuracy: 0.9308\nEpoch 16/30\n180/180 [==============================] - 1s 5ms/step - loss: 0.1451 - accuracy: 0.9436 - val_loss: 0.1631 - val_accuracy: 0.9321\nEpoch 17/30\n180/180 [==============================] - 1s 6ms/step - loss: 0.1397 - accuracy: 0.9456 - val_loss: 0.1594 - val_accuracy: 0.9339\nEpoch 18/30\n180/180 [==============================] - 1s 5ms/step - loss: 0.1350 - accuracy: 0.9476 - val_loss: 0.1564 - val_accuracy: 0.9362\nEpoch 19/30\n180/180 [==============================] - 1s 6ms/step - loss: 0.1308 - accuracy: 0.9488 - val_loss: 0.1539 - val_accuracy: 0.9380\nEpoch 20/30\n180/180 [==============================] - 1s 5ms/step - loss: 0.1271 - accuracy: 0.9506 - val_loss: 0.1519 - val_accuracy: 0.9384\nEpoch 21/30\n180/180 [==============================] - 1s 7ms/step - loss: 0.1238 - accuracy: 0.9518 - val_loss: 0.1502 - val_accuracy: 0.9389\nEpoch 22/30\n180/180 [==============================] - 1s 6ms/step - loss: 0.1208 - accuracy: 0.9531 - val_loss: 0.1489 - val_accuracy: 0.9398\nEpoch 23/30\n180/180 [==============================] - 1s 6ms/step - loss: 0.1180 - accuracy: 0.9543 - val_loss: 0.1479 - val_accuracy: 0.9395\nEpoch 24/30\n180/180 [==============================] - 1s 8ms/step - loss: 0.1155 - accuracy: 0.9554 - val_loss: 0.1471 - val_accuracy: 0.9398\nEpoch 25/30\n180/180 [==============================] - 1s 8ms/step - loss: 0.1132 - accuracy: 0.9566 - val_loss: 0.1465 - val_accuracy: 0.9395\nEpoch 26/30\n180/180 [==============================] - 1s 5ms/step - loss: 0.1111 - accuracy: 0.9574 - val_loss: 0.1462 - val_accuracy: 0.9391\nEpoch 27/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.1091 - accuracy: 0.9584 - val_loss: 0.1460 - val_accuracy: 0.9391\nEpoch 28/30\n180/180 [==============================] - 1s 8ms/step - loss: 0.1073 - accuracy: 0.9594 - val_loss: 0.1459 - val_accuracy: 0.9391\nEpoch 29/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.1055 - accuracy: 0.9601 - val_loss: 0.1460 - val_accuracy: 0.9382\nEpoch 30/30\n180/180 [==============================] - 2s 12ms/step - loss: 0.1039 - accuracy: 0.9607 - val_loss: 0.1462 - val_accuracy: 0.9384\n\n\n/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py:642: UserWarning:\n\nInput dict contained keys ['text'] which did not match any model input. They will be ignored by the model.\n\n\n\n\nplt.plot(history_title.history[\"accuracy\"], label = \"training accuracy\")\nplt.plot(history_title.history[\"val_accuracy\"], label = \"validation accuracy\")\nplt.legend()\nplt.show()\n\n\n\n\nWe have pretty good performance here! There’s a bit of overfitting in the later epochs, but overall this is way better than our base case model.\n\ntext_inputs = keras.Input(shape=(1,), name=\"text\", dtype=\"string\")\ntext_hidden = text_vectorize_layer(text_inputs)\ntext_hidden = embedding_text(text_hidden)\ntext_hidden = layers.GlobalAveragePooling1D()(text_hidden)\ntext_hidden = layers.Dense(80, activation=\"relu\")(text_hidden)\ntext_outputs = layers.Dense(2, name=\"fake\")(text_hidden)\n\ntext_model = keras.Model(\n    inputs = text_inputs,\n    outputs = text_outputs\n)\n\n\ntext_model.compile(\n    optimizer = \"adam\",\n    loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics = [\"accuracy\"]\n)\n\n\ntext_model.summary()\n\nModel: \"model_5\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n text (InputLayer)           [(None, 1)]               0         \n                                                                 \n text_vectorization_4 (Text  (None, 500)               0         \n Vectorization)                                                  \n                                                                 \n embedding_text (Embedding)  (None, 500, 2)            4000      \n                                                                 \n global_average_pooling1d_4  (None, 2)                 0         \n  (GlobalAveragePooling1D)                                       \n                                                                 \n dense_4 (Dense)             (None, 80)                240       \n                                                                 \n fake (Dense)                (None, 2)                 162       \n                                                                 \n=================================================================\nTotal params: 4402 (17.20 KB)\nTrainable params: 4402 (17.20 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\nhistory_text = text_model.fit(train_data, validation_data=val_data, epochs=30, verbose=True)\n\nEpoch 1/30\n180/180 [==============================] - 10s 53ms/step - loss: 0.6719 - accuracy: 0.6003 - val_loss: 0.6014 - val_accuracy: 0.8505\nEpoch 2/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.4348 - accuracy: 0.9137 - val_loss: 0.2991 - val_accuracy: 0.9418\nEpoch 3/30\n180/180 [==============================] - 2s 13ms/step - loss: 0.2396 - accuracy: 0.9447 - val_loss: 0.1987 - val_accuracy: 0.9566\nEpoch 4/30\n180/180 [==============================] - 3s 16ms/step - loss: 0.1736 - accuracy: 0.9601 - val_loss: 0.1561 - val_accuracy: 0.9652\nEpoch 5/30\n180/180 [==============================] - 2s 13ms/step - loss: 0.1400 - accuracy: 0.9682 - val_loss: 0.1318 - val_accuracy: 0.9688\nEpoch 6/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.1186 - accuracy: 0.9732 - val_loss: 0.1162 - val_accuracy: 0.9701\nEpoch 7/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.1034 - accuracy: 0.9754 - val_loss: 0.1056 - val_accuracy: 0.9721\nEpoch 8/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.0918 - accuracy: 0.9782 - val_loss: 0.0980 - val_accuracy: 0.9737\nEpoch 9/30\n180/180 [==============================] - 4s 25ms/step - loss: 0.0824 - accuracy: 0.9802 - val_loss: 0.0924 - val_accuracy: 0.9757\nEpoch 10/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.0746 - accuracy: 0.9820 - val_loss: 0.0882 - val_accuracy: 0.9766\nEpoch 11/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.0678 - accuracy: 0.9838 - val_loss: 0.0849 - val_accuracy: 0.9780\nEpoch 12/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.0619 - accuracy: 0.9856 - val_loss: 0.0823 - val_accuracy: 0.9784\nEpoch 13/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.0566 - accuracy: 0.9870 - val_loss: 0.0802 - val_accuracy: 0.9793\nEpoch 14/30\n180/180 [==============================] - 3s 15ms/step - loss: 0.0519 - accuracy: 0.9881 - val_loss: 0.0785 - val_accuracy: 0.9798\nEpoch 15/30\n180/180 [==============================] - 3s 15ms/step - loss: 0.0476 - accuracy: 0.9892 - val_loss: 0.0770 - val_accuracy: 0.9800\nEpoch 16/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.0437 - accuracy: 0.9899 - val_loss: 0.0757 - val_accuracy: 0.9804\nEpoch 17/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.0401 - accuracy: 0.9909 - val_loss: 0.0746 - val_accuracy: 0.9804\nEpoch 18/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.0368 - accuracy: 0.9917 - val_loss: 0.0737 - val_accuracy: 0.9800\nEpoch 19/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.0339 - accuracy: 0.9925 - val_loss: 0.0730 - val_accuracy: 0.9804\nEpoch 20/30\n180/180 [==============================] - 3s 15ms/step - loss: 0.0311 - accuracy: 0.9933 - val_loss: 0.0726 - val_accuracy: 0.9811\nEpoch 21/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.0286 - accuracy: 0.9940 - val_loss: 0.0726 - val_accuracy: 0.9816\nEpoch 22/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.0263 - accuracy: 0.9946 - val_loss: 0.0727 - val_accuracy: 0.9818\nEpoch 23/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.0242 - accuracy: 0.9952 - val_loss: 0.0731 - val_accuracy: 0.9822\nEpoch 24/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.0223 - accuracy: 0.9957 - val_loss: 0.0737 - val_accuracy: 0.9820\nEpoch 25/30\n180/180 [==============================] - 3s 15ms/step - loss: 0.0205 - accuracy: 0.9962 - val_loss: 0.0744 - val_accuracy: 0.9822\nEpoch 26/30\n180/180 [==============================] - 3s 16ms/step - loss: 0.0188 - accuracy: 0.9967 - val_loss: 0.0753 - val_accuracy: 0.9822\nEpoch 27/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.0173 - accuracy: 0.9972 - val_loss: 0.0762 - val_accuracy: 0.9825\nEpoch 28/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.0159 - accuracy: 0.9974 - val_loss: 0.0772 - val_accuracy: 0.9822\nEpoch 29/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.0147 - accuracy: 0.9976 - val_loss: 0.0783 - val_accuracy: 0.9820\nEpoch 30/30\n180/180 [==============================] - 3s 16ms/step - loss: 0.0135 - accuracy: 0.9977 - val_loss: 0.0796 - val_accuracy: 0.9825\n\n\n/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py:642: UserWarning:\n\nInput dict contained keys ['title'] which did not match any model input. They will be ignored by the model.\n\n\n\n\nplt.plot(history_text.history[\"accuracy\"], label = \"training accuracy\")\nplt.plot(history_text.history[\"val_accuracy\"], label = \"validation accuracy\")\nplt.legend()\nplt.show()\n\n\n\n\nThis performed even better! Our validation accuracy stabilizes above 95%, which is very impressive. Overfitting does not seem to be nearly as present here.\nWe got really great results with both models! Now let’s try combining the inputs.\n\ninputs = [title_inputs, text_inputs]\noutputs = layers.concatenate([title_hidden, text_hidden])\noutputs = layers.Dense(2, name=\"fake\")(outputs)\n\ncombined_model = keras.Model(\n    inputs = inputs,\n    outputs = outputs\n)\n\n\ncombined_model.compile(\n    optimizer = \"adam\",\n    loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics = [\"accuracy\"]\n)\n\n\ncombined_model.summary()\n\nModel: \"model_6\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n title (InputLayer)          [(None, 1)]                  0         []                            \n                                                                                                  \n text (InputLayer)           [(None, 1)]                  0         []                            \n                                                                                                  \n text_vectorization_3 (Text  (None, 500)                  0         ['title[0][0]']               \n Vectorization)                                                                                   \n                                                                                                  \n text_vectorization_4 (Text  (None, 500)                  0         ['text[0][0]']                \n Vectorization)                                                                                   \n                                                                                                  \n embedding_title (Embedding  (None, 500, 2)               4000      ['text_vectorization_3[0][0]']\n )                                                                                                \n                                                                                                  \n embedding_text (Embedding)  (None, 500, 2)               4000      ['text_vectorization_4[0][0]']\n                                                                                                  \n global_average_pooling1d_3  (None, 2)                    0         ['embedding_title[0][0]']     \n  (GlobalAveragePooling1D)                                                                        \n                                                                                                  \n global_average_pooling1d_4  (None, 2)                    0         ['embedding_text[0][0]']      \n  (GlobalAveragePooling1D)                                                                        \n                                                                                                  \n dense_3 (Dense)             (None, 80)                   240       ['global_average_pooling1d_3[0\n                                                                    ][0]']                        \n                                                                                                  \n dense_4 (Dense)             (None, 80)                   240       ['global_average_pooling1d_4[0\n                                                                    ][0]']                        \n                                                                                                  \n concatenate_1 (Concatenate  (None, 160)                  0         ['dense_3[0][0]',             \n )                                                                   'dense_4[0][0]']             \n                                                                                                  \n fake (Dense)                (None, 2)                    322       ['concatenate_1[0][0]']       \n                                                                                                  \n==================================================================================================\nTotal params: 8802 (34.38 KB)\nTrainable params: 8802 (34.38 KB)\nNon-trainable params: 0 (0.00 Byte)\n__________________________________________________________________________________________________\n\n\nLet’s visualize our final model and see how the separate inputs get combined.\n\nfrom tensorflow.keras import utils\nutils.plot_model(combined_model)\n\n\n\n\n\nhistory_combined = combined_model.fit(train_data, validation_data=val_data, epochs=30, verbose=True)\n\nEpoch 1/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.3586 - accuracy: 0.9603 - val_loss: 0.1796 - val_accuracy: 0.9876\nEpoch 2/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.1152 - accuracy: 0.9928 - val_loss: 0.0887 - val_accuracy: 0.9881\nEpoch 3/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0641 - accuracy: 0.9927 - val_loss: 0.0652 - val_accuracy: 0.9876\nEpoch 4/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0462 - accuracy: 0.9938 - val_loss: 0.0550 - val_accuracy: 0.9881\nEpoch 5/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0367 - accuracy: 0.9941 - val_loss: 0.0490 - val_accuracy: 0.9881\nEpoch 6/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0306 - accuracy: 0.9951 - val_loss: 0.0449 - val_accuracy: 0.9885\nEpoch 7/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0262 - accuracy: 0.9957 - val_loss: 0.0419 - val_accuracy: 0.9885\nEpoch 8/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.0228 - accuracy: 0.9962 - val_loss: 0.0394 - val_accuracy: 0.9890\nEpoch 9/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.0200 - accuracy: 0.9964 - val_loss: 0.0374 - val_accuracy: 0.9892\nEpoch 10/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.0177 - accuracy: 0.9967 - val_loss: 0.0357 - val_accuracy: 0.9890\nEpoch 11/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0158 - accuracy: 0.9969 - val_loss: 0.0342 - val_accuracy: 0.9892\nEpoch 12/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0142 - accuracy: 0.9973 - val_loss: 0.0331 - val_accuracy: 0.9894\nEpoch 13/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0127 - accuracy: 0.9976 - val_loss: 0.0322 - val_accuracy: 0.9894\nEpoch 14/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.0114 - accuracy: 0.9978 - val_loss: 0.0314 - val_accuracy: 0.9894\nEpoch 15/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.0104 - accuracy: 0.9981 - val_loss: 0.0308 - val_accuracy: 0.9894\nEpoch 16/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0094 - accuracy: 0.9983 - val_loss: 0.0302 - val_accuracy: 0.9894\nEpoch 17/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0086 - accuracy: 0.9984 - val_loss: 0.0298 - val_accuracy: 0.9894\nEpoch 18/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0078 - accuracy: 0.9987 - val_loss: 0.0295 - val_accuracy: 0.9894\nEpoch 19/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0071 - accuracy: 0.9987 - val_loss: 0.0292 - val_accuracy: 0.9894\nEpoch 20/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0065 - accuracy: 0.9989 - val_loss: 0.0291 - val_accuracy: 0.9894\nEpoch 21/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0060 - accuracy: 0.9990 - val_loss: 0.0289 - val_accuracy: 0.9894\nEpoch 22/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0055 - accuracy: 0.9992 - val_loss: 0.0289 - val_accuracy: 0.9899\nEpoch 23/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0050 - accuracy: 0.9993 - val_loss: 0.0289 - val_accuracy: 0.9901\nEpoch 24/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0046 - accuracy: 0.9994 - val_loss: 0.0290 - val_accuracy: 0.9906\nEpoch 25/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0042 - accuracy: 0.9994 - val_loss: 0.0292 - val_accuracy: 0.9906\nEpoch 26/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0038 - accuracy: 0.9994 - val_loss: 0.0295 - val_accuracy: 0.9903\nEpoch 27/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0035 - accuracy: 0.9996 - val_loss: 0.0298 - val_accuracy: 0.9901\nEpoch 28/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0032 - accuracy: 0.9996 - val_loss: 0.0301 - val_accuracy: 0.9901\nEpoch 29/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0030 - accuracy: 0.9996 - val_loss: 0.0305 - val_accuracy: 0.9901\nEpoch 30/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0027 - accuracy: 0.9997 - val_loss: 0.0310 - val_accuracy: 0.9906\n\n\n\nplt.plot(history_combined.history[\"accuracy\"], label = \"training accuracy\")\nplt.plot(history_combined.history[\"val_accuracy\"], label = \"validation accuracy\")\nplt.legend()\nplt.show()\n\n\n\n\nWe got a validation accuracy of over 99%! That’s really great performance of the model! This is actually the highest validation accuracy of the model, compared to when we trained solely on the titles or solely on the text. Therefore, we should be using both the title and text to detect whether something is fake news or not, because of its amazing performance. Now let’s see how our best model, the one using both titles and text, performs on unseen test data. Let’s download it in the code cell below.\n\ntest_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true\"\ntest_df = pd.read_csv(test_url)\ntest_dataset = make_dataset(test_df)\n\n\ncombined_model.evaluate(test_dataset)\n\n  1/225 [..............................] - ETA: 2s - loss: 0.0432 - accuracy: 0.9800225/225 [==============================] - 1s 5ms/step - loss: 0.0423 - accuracy: 0.9890\n\n\n[0.04232485964894295, 0.9889972805976868]\n\n\nIf we used the above model to detect fake news, we’d be right over 98% of the time! That’s very impresive for us! Now let’s visualize the embeddings that we trained for this model.\n\nweights = np.array(combined_model.get_layer(\"embedding\").get_weights())[0]\nvocab = title_vectorize_layer.get_vocabulary() # we need to pick between the title and vocab; we arbitrarily pick title's vocabulary\n\nWe can plot this on over an x and y axis, and take a look at the embeddings that we’ve trained. Since we already had an output size of 2 for the embedding layer, we don’t have to do any additional processing and we’ll just visualize using the layer directly.\n\nweights.shape\n\n(2000, 2)\n\n\n\nweights_df = pd.DataFrame(weights, columns=[\"x\", \"y\"])\n\n\nweights_df[\"vocab\"] = vocab\n\n\npx.scatter(\n    data_frame = weights_df,\n    x = \"x\",\n    y = \"y\",\n    hover_data = \"vocab\"\n)\n\n\n\n\nThese are the embeddings that we trained! From looking at this diagram, we see words like “seek”, “talks”, “needs”, and country names like Israel and The Philippines on the left side of the graph. These words generally seem objective, and these are the types of words we’d see in news that isn’t fake. However, on the right side of the graph, we see words like “fired”, “nightmare”, “breaking”, “racist”, and more. President names like Obama and Trump are in this area as well. This language we see to the right seems a lot more charged, and these are the words that likely were found in the fake news aritcles. This is a good visualization and confirmation that we trained the embeddings correctly!"
  },
  {
    "objectID": "posts/fake-news-classification/index.html#fake-news-classification",
    "href": "posts/fake-news-classification/index.html#fake-news-classification",
    "title": "Fake News Classification",
    "section": "",
    "text": "In this blog post we will be developing and using a fake news classifier using Tensorflow.\nWe will first import all the packages we’ll need.\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport re\nimport string\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import losses\nfrom tensorflow.keras import utils\n\nfrom tensorflow.keras.layers import TextVectorization\nfrom tensorflow.keras.layers import StringLookup\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nimport nltk\nfrom nltk.corpus import stopwords\n\n# for embedding viz\nimport plotly.express as px\nimport plotly.io as pio\npio.templates.default = \"plotly_white\"\npio.renderers.default=\"iframe\"\nimport matplotlib.pyplot as plt\n\n\ntrain_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true\"\ndf = pd.read_csv(train_url)\n\nLet’s see what our dataset looks like:\n\ndf.head()\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\ntitle\ntext\nfake\n\n\n\n\n0\n17366\nMerkel: Strong result for Austria's FPO 'big c...\nGerman Chancellor Angela Merkel said on Monday...\n0\n\n\n1\n5634\nTrump says Pence will lead voter fraud panel\nWEST PALM BEACH, Fla.President Donald Trump sa...\n0\n\n\n2\n17487\nJUST IN: SUSPECTED LEAKER and “Close Confidant...\nOn December 5, 2017, Circa s Sara Carter warne...\n1\n\n\n3\n12217\nThyssenkrupp has offered help to Argentina ove...\nGermany s Thyssenkrupp, has offered assistance...\n0\n\n\n4\n5535\nTrump say appeals court decision on travel ban...\nPresident Donald Trump on Thursday called the ...\n0\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nLooks interesting! We have the title of the article, the text inside of it, and the label of whether or not it’s fake. Now let’s create a function that will remove stopwords from the title and text columns, and return a tensorflow dataset from the pandas dataframe. We’ll first download the stopwords that we need.\n\nnltk.download(\"stopwords\")\n\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue\n\n\n\nstop = stopwords.words(\"english\")\n\n\nprint(stop[:10]) # visualizing some of the stop words\n\n['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n\n\n\ndef make_dataset(df):\n  stop = stopwords.words(\"english\")\n  # we need to make sure we lowercase the words before checking if its in the stopwords list\n  df[\"title\"] = df[\"title\"].apply(lambda x: \" \".join([word for word in x.split() if word.lower() not in (stop)]))\n  df[\"text\"] = df[\"text\"].apply(lambda x: \" \".join([word for word in x.split() if word.lower() not in (stop)]))\n  features = { \"title\": df[[\"title\"]], \"text\": df[[\"text\"]] }\n  target = { \"fake\": df[[\"fake\"]] }\n  dataset = tf.data.Dataset.from_tensor_slices((features, target))\n\n  return dataset.batch(100)\n\n\ndataset = make_dataset(df)\n\nGreat! We’ve made the dataset. Now let’s do an 80-20 split for validation.\n\ntrain_size = int(0.8*len(dataset))\nval_size = int(0.2*len(dataset))\n\ntrain_data = dataset.take(train_size)\nval_data = dataset.skip(train_size)\n\n\ntrain_data.shuffle(buffer_size = train_size, reshuffle_each_iteration=False)\n\n&lt;_ShuffleDataset element_spec=({'title': TensorSpec(shape=(None, 1), dtype=tf.string, name=None), 'text': TensorSpec(shape=(None, 1), dtype=tf.string, name=None)}, {'fake': TensorSpec(shape=(None, 1), dtype=tf.int64, name=None)})&gt;\n\n\n\nlen(train_data), len(val_data) # sanity check\n\n(180, 45)\n\n\nLet’s think about the base case for a bit. In this case, our base case could just be us being trigger happy and labeling all news as fake news. Lets take a quick look at the distribution of our labels and see how accurate this kind of classifier would be.\n\nlabels = []\nfor i in train_data:\n  labels.extend(i[1][\"fake\"].numpy().reshape(1,-1)[0])\n\n\nlen(labels)\n\n18000\n\n\n\nfake_count = labels.count(1)\n\n\nprint(f\"Proportion of fake labels: {fake_count/len(labels)}\")\n\nProportion of fake labels: 0.5220555555555556\n\n\nThis means that our base classifier would have around a 52.2% accuracy. Let’s see if we can do any better. Let’s start with doing some text vectorization. The code below will make all the strings lowercase and remove all punctuation. We’ll also limit the vocabulary to avoid overcomplicating things and also to ignore words that almost never show up.\n\n#preparing a text vectorization layer for tf model\nsize_vocabulary = 2000\n\ndef standardization(input_data):\n    lowercase = tf.strings.lower(input_data)\n    no_punctuation = tf.strings.regex_replace(lowercase,\n                                  '[%s]' % re.escape(string.punctuation),'')\n    return no_punctuation\n\ntitle_vectorize_layer = TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary, # only consider this many words\n    output_mode='int',\n    output_sequence_length=500)\n\ntext_vectorize_layer = TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary, # only consider this many words\n    output_mode='int',\n    output_sequence_length=500)\n\ntitle_vectorize_layer.adapt(train_data.map(lambda x, y: x[\"title\"]))\ntext_vectorize_layer.adapt(train_data.map(lambda x, y: x[\"text\"]))\n\n\nembedding_title = layers.Embedding(size_vocabulary, 2, name=\"embedding_title\")\nembedding_text = layers.Embedding(size_vocabulary, 2, name=\"embedding_text\")\n\nNow, let’s answer this interesting question: When detecting fake news, is it most effective to focus only on the title of the article, the full text, or both? Let’s start by training only on the title.\n\ntitle_inputs = keras.Input(shape=(1,), name=\"title\", dtype=\"string\")\ntitle_hidden = title_vectorize_layer(title_inputs)\ntitle_hidden = embedding_title(title_hidden)\ntitle_hidden = layers.GlobalAveragePooling1D()(title_hidden)\ntitle_hidden = layers.Dense(80, activation=\"relu\")(title_hidden)\ntitle_outputs = layers.Dense(2, name=\"fake\")(title_hidden)\n\ntitle_model = keras.Model(\n    inputs = title_inputs,\n    outputs = title_outputs\n)\n\n\ntitle_model.compile(\n    optimizer = \"adam\",\n    loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics = [\"accuracy\"]\n)\n\n\ntitle_model.summary()\n\nModel: \"model_4\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n title (InputLayer)          [(None, 1)]               0         \n                                                                 \n text_vectorization_3 (Text  (None, 500)               0         \n Vectorization)                                                  \n                                                                 \n embedding_title (Embedding  (None, 500, 2)            4000      \n )                                                               \n                                                                 \n global_average_pooling1d_3  (None, 2)                 0         \n  (GlobalAveragePooling1D)                                       \n                                                                 \n dense_3 (Dense)             (None, 80)                240       \n                                                                 \n fake (Dense)                (None, 2)                 162       \n                                                                 \n=================================================================\nTotal params: 4402 (17.20 KB)\nTrainable params: 4402 (17.20 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\nhistory_title = title_model.fit(train_data, validation_data=val_data, epochs=30, verbose=True)\n\nEpoch 1/30\n180/180 [==============================] - 7s 36ms/step - loss: 0.6925 - accuracy: 0.5178 - val_loss: 0.6917 - val_accuracy: 0.5266\nEpoch 2/30\n180/180 [==============================] - 1s 5ms/step - loss: 0.6920 - accuracy: 0.5221 - val_loss: 0.6909 - val_accuracy: 0.5266\nEpoch 3/30\n180/180 [==============================] - 1s 5ms/step - loss: 0.6870 - accuracy: 0.5392 - val_loss: 0.6759 - val_accuracy: 0.5278\nEpoch 4/30\n180/180 [==============================] - 1s 8ms/step - loss: 0.6417 - accuracy: 0.7271 - val_loss: 0.5900 - val_accuracy: 0.8517\nEpoch 5/30\n180/180 [==============================] - 1s 6ms/step - loss: 0.5195 - accuracy: 0.8608 - val_loss: 0.4527 - val_accuracy: 0.8721\nEpoch 6/30\n180/180 [==============================] - 1s 6ms/step - loss: 0.3935 - accuracy: 0.8834 - val_loss: 0.3547 - val_accuracy: 0.8809\nEpoch 7/30\n180/180 [==============================] - 1s 6ms/step - loss: 0.3152 - accuracy: 0.8975 - val_loss: 0.2976 - val_accuracy: 0.8896\nEpoch 8/30\n180/180 [==============================] - 1s 5ms/step - loss: 0.2668 - accuracy: 0.9084 - val_loss: 0.2591 - val_accuracy: 0.8986\nEpoch 9/30\n180/180 [==============================] - 1s 6ms/step - loss: 0.2333 - accuracy: 0.9171 - val_loss: 0.2324 - val_accuracy: 0.9078\nEpoch 10/30\n180/180 [==============================] - 1s 6ms/step - loss: 0.2095 - accuracy: 0.9239 - val_loss: 0.2132 - val_accuracy: 0.9135\nEpoch 11/30\n180/180 [==============================] - 1s 6ms/step - loss: 0.1919 - accuracy: 0.9283 - val_loss: 0.1990 - val_accuracy: 0.9164\nEpoch 12/30\n180/180 [==============================] - 1s 6ms/step - loss: 0.1783 - accuracy: 0.9329 - val_loss: 0.1882 - val_accuracy: 0.9211\nEpoch 13/30\n180/180 [==============================] - 1s 6ms/step - loss: 0.1675 - accuracy: 0.9362 - val_loss: 0.1798 - val_accuracy: 0.9252\nEpoch 14/30\n180/180 [==============================] - 1s 7ms/step - loss: 0.1587 - accuracy: 0.9393 - val_loss: 0.1730 - val_accuracy: 0.9285\nEpoch 15/30\n180/180 [==============================] - 1s 7ms/step - loss: 0.1514 - accuracy: 0.9417 - val_loss: 0.1675 - val_accuracy: 0.9308\nEpoch 16/30\n180/180 [==============================] - 1s 5ms/step - loss: 0.1451 - accuracy: 0.9436 - val_loss: 0.1631 - val_accuracy: 0.9321\nEpoch 17/30\n180/180 [==============================] - 1s 6ms/step - loss: 0.1397 - accuracy: 0.9456 - val_loss: 0.1594 - val_accuracy: 0.9339\nEpoch 18/30\n180/180 [==============================] - 1s 5ms/step - loss: 0.1350 - accuracy: 0.9476 - val_loss: 0.1564 - val_accuracy: 0.9362\nEpoch 19/30\n180/180 [==============================] - 1s 6ms/step - loss: 0.1308 - accuracy: 0.9488 - val_loss: 0.1539 - val_accuracy: 0.9380\nEpoch 20/30\n180/180 [==============================] - 1s 5ms/step - loss: 0.1271 - accuracy: 0.9506 - val_loss: 0.1519 - val_accuracy: 0.9384\nEpoch 21/30\n180/180 [==============================] - 1s 7ms/step - loss: 0.1238 - accuracy: 0.9518 - val_loss: 0.1502 - val_accuracy: 0.9389\nEpoch 22/30\n180/180 [==============================] - 1s 6ms/step - loss: 0.1208 - accuracy: 0.9531 - val_loss: 0.1489 - val_accuracy: 0.9398\nEpoch 23/30\n180/180 [==============================] - 1s 6ms/step - loss: 0.1180 - accuracy: 0.9543 - val_loss: 0.1479 - val_accuracy: 0.9395\nEpoch 24/30\n180/180 [==============================] - 1s 8ms/step - loss: 0.1155 - accuracy: 0.9554 - val_loss: 0.1471 - val_accuracy: 0.9398\nEpoch 25/30\n180/180 [==============================] - 1s 8ms/step - loss: 0.1132 - accuracy: 0.9566 - val_loss: 0.1465 - val_accuracy: 0.9395\nEpoch 26/30\n180/180 [==============================] - 1s 5ms/step - loss: 0.1111 - accuracy: 0.9574 - val_loss: 0.1462 - val_accuracy: 0.9391\nEpoch 27/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.1091 - accuracy: 0.9584 - val_loss: 0.1460 - val_accuracy: 0.9391\nEpoch 28/30\n180/180 [==============================] - 1s 8ms/step - loss: 0.1073 - accuracy: 0.9594 - val_loss: 0.1459 - val_accuracy: 0.9391\nEpoch 29/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.1055 - accuracy: 0.9601 - val_loss: 0.1460 - val_accuracy: 0.9382\nEpoch 30/30\n180/180 [==============================] - 2s 12ms/step - loss: 0.1039 - accuracy: 0.9607 - val_loss: 0.1462 - val_accuracy: 0.9384\n\n\n/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py:642: UserWarning:\n\nInput dict contained keys ['text'] which did not match any model input. They will be ignored by the model.\n\n\n\n\nplt.plot(history_title.history[\"accuracy\"], label = \"training accuracy\")\nplt.plot(history_title.history[\"val_accuracy\"], label = \"validation accuracy\")\nplt.legend()\nplt.show()\n\n\n\n\nWe have pretty good performance here! There’s a bit of overfitting in the later epochs, but overall this is way better than our base case model.\n\ntext_inputs = keras.Input(shape=(1,), name=\"text\", dtype=\"string\")\ntext_hidden = text_vectorize_layer(text_inputs)\ntext_hidden = embedding_text(text_hidden)\ntext_hidden = layers.GlobalAveragePooling1D()(text_hidden)\ntext_hidden = layers.Dense(80, activation=\"relu\")(text_hidden)\ntext_outputs = layers.Dense(2, name=\"fake\")(text_hidden)\n\ntext_model = keras.Model(\n    inputs = text_inputs,\n    outputs = text_outputs\n)\n\n\ntext_model.compile(\n    optimizer = \"adam\",\n    loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics = [\"accuracy\"]\n)\n\n\ntext_model.summary()\n\nModel: \"model_5\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n text (InputLayer)           [(None, 1)]               0         \n                                                                 \n text_vectorization_4 (Text  (None, 500)               0         \n Vectorization)                                                  \n                                                                 \n embedding_text (Embedding)  (None, 500, 2)            4000      \n                                                                 \n global_average_pooling1d_4  (None, 2)                 0         \n  (GlobalAveragePooling1D)                                       \n                                                                 \n dense_4 (Dense)             (None, 80)                240       \n                                                                 \n fake (Dense)                (None, 2)                 162       \n                                                                 \n=================================================================\nTotal params: 4402 (17.20 KB)\nTrainable params: 4402 (17.20 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\nhistory_text = text_model.fit(train_data, validation_data=val_data, epochs=30, verbose=True)\n\nEpoch 1/30\n180/180 [==============================] - 10s 53ms/step - loss: 0.6719 - accuracy: 0.6003 - val_loss: 0.6014 - val_accuracy: 0.8505\nEpoch 2/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.4348 - accuracy: 0.9137 - val_loss: 0.2991 - val_accuracy: 0.9418\nEpoch 3/30\n180/180 [==============================] - 2s 13ms/step - loss: 0.2396 - accuracy: 0.9447 - val_loss: 0.1987 - val_accuracy: 0.9566\nEpoch 4/30\n180/180 [==============================] - 3s 16ms/step - loss: 0.1736 - accuracy: 0.9601 - val_loss: 0.1561 - val_accuracy: 0.9652\nEpoch 5/30\n180/180 [==============================] - 2s 13ms/step - loss: 0.1400 - accuracy: 0.9682 - val_loss: 0.1318 - val_accuracy: 0.9688\nEpoch 6/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.1186 - accuracy: 0.9732 - val_loss: 0.1162 - val_accuracy: 0.9701\nEpoch 7/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.1034 - accuracy: 0.9754 - val_loss: 0.1056 - val_accuracy: 0.9721\nEpoch 8/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.0918 - accuracy: 0.9782 - val_loss: 0.0980 - val_accuracy: 0.9737\nEpoch 9/30\n180/180 [==============================] - 4s 25ms/step - loss: 0.0824 - accuracy: 0.9802 - val_loss: 0.0924 - val_accuracy: 0.9757\nEpoch 10/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.0746 - accuracy: 0.9820 - val_loss: 0.0882 - val_accuracy: 0.9766\nEpoch 11/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.0678 - accuracy: 0.9838 - val_loss: 0.0849 - val_accuracy: 0.9780\nEpoch 12/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.0619 - accuracy: 0.9856 - val_loss: 0.0823 - val_accuracy: 0.9784\nEpoch 13/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.0566 - accuracy: 0.9870 - val_loss: 0.0802 - val_accuracy: 0.9793\nEpoch 14/30\n180/180 [==============================] - 3s 15ms/step - loss: 0.0519 - accuracy: 0.9881 - val_loss: 0.0785 - val_accuracy: 0.9798\nEpoch 15/30\n180/180 [==============================] - 3s 15ms/step - loss: 0.0476 - accuracy: 0.9892 - val_loss: 0.0770 - val_accuracy: 0.9800\nEpoch 16/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.0437 - accuracy: 0.9899 - val_loss: 0.0757 - val_accuracy: 0.9804\nEpoch 17/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.0401 - accuracy: 0.9909 - val_loss: 0.0746 - val_accuracy: 0.9804\nEpoch 18/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.0368 - accuracy: 0.9917 - val_loss: 0.0737 - val_accuracy: 0.9800\nEpoch 19/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.0339 - accuracy: 0.9925 - val_loss: 0.0730 - val_accuracy: 0.9804\nEpoch 20/30\n180/180 [==============================] - 3s 15ms/step - loss: 0.0311 - accuracy: 0.9933 - val_loss: 0.0726 - val_accuracy: 0.9811\nEpoch 21/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.0286 - accuracy: 0.9940 - val_loss: 0.0726 - val_accuracy: 0.9816\nEpoch 22/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.0263 - accuracy: 0.9946 - val_loss: 0.0727 - val_accuracy: 0.9818\nEpoch 23/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.0242 - accuracy: 0.9952 - val_loss: 0.0731 - val_accuracy: 0.9822\nEpoch 24/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.0223 - accuracy: 0.9957 - val_loss: 0.0737 - val_accuracy: 0.9820\nEpoch 25/30\n180/180 [==============================] - 3s 15ms/step - loss: 0.0205 - accuracy: 0.9962 - val_loss: 0.0744 - val_accuracy: 0.9822\nEpoch 26/30\n180/180 [==============================] - 3s 16ms/step - loss: 0.0188 - accuracy: 0.9967 - val_loss: 0.0753 - val_accuracy: 0.9822\nEpoch 27/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.0173 - accuracy: 0.9972 - val_loss: 0.0762 - val_accuracy: 0.9825\nEpoch 28/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.0159 - accuracy: 0.9974 - val_loss: 0.0772 - val_accuracy: 0.9822\nEpoch 29/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.0147 - accuracy: 0.9976 - val_loss: 0.0783 - val_accuracy: 0.9820\nEpoch 30/30\n180/180 [==============================] - 3s 16ms/step - loss: 0.0135 - accuracy: 0.9977 - val_loss: 0.0796 - val_accuracy: 0.9825\n\n\n/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py:642: UserWarning:\n\nInput dict contained keys ['title'] which did not match any model input. They will be ignored by the model.\n\n\n\n\nplt.plot(history_text.history[\"accuracy\"], label = \"training accuracy\")\nplt.plot(history_text.history[\"val_accuracy\"], label = \"validation accuracy\")\nplt.legend()\nplt.show()\n\n\n\n\nThis performed even better! Our validation accuracy stabilizes above 95%, which is very impressive. Overfitting does not seem to be nearly as present here.\nWe got really great results with both models! Now let’s try combining the inputs.\n\ninputs = [title_inputs, text_inputs]\noutputs = layers.concatenate([title_hidden, text_hidden])\noutputs = layers.Dense(2, name=\"fake\")(outputs)\n\ncombined_model = keras.Model(\n    inputs = inputs,\n    outputs = outputs\n)\n\n\ncombined_model.compile(\n    optimizer = \"adam\",\n    loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics = [\"accuracy\"]\n)\n\n\ncombined_model.summary()\n\nModel: \"model_6\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n title (InputLayer)          [(None, 1)]                  0         []                            \n                                                                                                  \n text (InputLayer)           [(None, 1)]                  0         []                            \n                                                                                                  \n text_vectorization_3 (Text  (None, 500)                  0         ['title[0][0]']               \n Vectorization)                                                                                   \n                                                                                                  \n text_vectorization_4 (Text  (None, 500)                  0         ['text[0][0]']                \n Vectorization)                                                                                   \n                                                                                                  \n embedding_title (Embedding  (None, 500, 2)               4000      ['text_vectorization_3[0][0]']\n )                                                                                                \n                                                                                                  \n embedding_text (Embedding)  (None, 500, 2)               4000      ['text_vectorization_4[0][0]']\n                                                                                                  \n global_average_pooling1d_3  (None, 2)                    0         ['embedding_title[0][0]']     \n  (GlobalAveragePooling1D)                                                                        \n                                                                                                  \n global_average_pooling1d_4  (None, 2)                    0         ['embedding_text[0][0]']      \n  (GlobalAveragePooling1D)                                                                        \n                                                                                                  \n dense_3 (Dense)             (None, 80)                   240       ['global_average_pooling1d_3[0\n                                                                    ][0]']                        \n                                                                                                  \n dense_4 (Dense)             (None, 80)                   240       ['global_average_pooling1d_4[0\n                                                                    ][0]']                        \n                                                                                                  \n concatenate_1 (Concatenate  (None, 160)                  0         ['dense_3[0][0]',             \n )                                                                   'dense_4[0][0]']             \n                                                                                                  \n fake (Dense)                (None, 2)                    322       ['concatenate_1[0][0]']       \n                                                                                                  \n==================================================================================================\nTotal params: 8802 (34.38 KB)\nTrainable params: 8802 (34.38 KB)\nNon-trainable params: 0 (0.00 Byte)\n__________________________________________________________________________________________________\n\n\nLet’s visualize our final model and see how the separate inputs get combined.\n\nfrom tensorflow.keras import utils\nutils.plot_model(combined_model)\n\n\n\n\n\nhistory_combined = combined_model.fit(train_data, validation_data=val_data, epochs=30, verbose=True)\n\nEpoch 1/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.3586 - accuracy: 0.9603 - val_loss: 0.1796 - val_accuracy: 0.9876\nEpoch 2/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.1152 - accuracy: 0.9928 - val_loss: 0.0887 - val_accuracy: 0.9881\nEpoch 3/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0641 - accuracy: 0.9927 - val_loss: 0.0652 - val_accuracy: 0.9876\nEpoch 4/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0462 - accuracy: 0.9938 - val_loss: 0.0550 - val_accuracy: 0.9881\nEpoch 5/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0367 - accuracy: 0.9941 - val_loss: 0.0490 - val_accuracy: 0.9881\nEpoch 6/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0306 - accuracy: 0.9951 - val_loss: 0.0449 - val_accuracy: 0.9885\nEpoch 7/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0262 - accuracy: 0.9957 - val_loss: 0.0419 - val_accuracy: 0.9885\nEpoch 8/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.0228 - accuracy: 0.9962 - val_loss: 0.0394 - val_accuracy: 0.9890\nEpoch 9/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.0200 - accuracy: 0.9964 - val_loss: 0.0374 - val_accuracy: 0.9892\nEpoch 10/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.0177 - accuracy: 0.9967 - val_loss: 0.0357 - val_accuracy: 0.9890\nEpoch 11/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0158 - accuracy: 0.9969 - val_loss: 0.0342 - val_accuracy: 0.9892\nEpoch 12/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0142 - accuracy: 0.9973 - val_loss: 0.0331 - val_accuracy: 0.9894\nEpoch 13/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0127 - accuracy: 0.9976 - val_loss: 0.0322 - val_accuracy: 0.9894\nEpoch 14/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.0114 - accuracy: 0.9978 - val_loss: 0.0314 - val_accuracy: 0.9894\nEpoch 15/30\n180/180 [==============================] - 2s 11ms/step - loss: 0.0104 - accuracy: 0.9981 - val_loss: 0.0308 - val_accuracy: 0.9894\nEpoch 16/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0094 - accuracy: 0.9983 - val_loss: 0.0302 - val_accuracy: 0.9894\nEpoch 17/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0086 - accuracy: 0.9984 - val_loss: 0.0298 - val_accuracy: 0.9894\nEpoch 18/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0078 - accuracy: 0.9987 - val_loss: 0.0295 - val_accuracy: 0.9894\nEpoch 19/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0071 - accuracy: 0.9987 - val_loss: 0.0292 - val_accuracy: 0.9894\nEpoch 20/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0065 - accuracy: 0.9989 - val_loss: 0.0291 - val_accuracy: 0.9894\nEpoch 21/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0060 - accuracy: 0.9990 - val_loss: 0.0289 - val_accuracy: 0.9894\nEpoch 22/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0055 - accuracy: 0.9992 - val_loss: 0.0289 - val_accuracy: 0.9899\nEpoch 23/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0050 - accuracy: 0.9993 - val_loss: 0.0289 - val_accuracy: 0.9901\nEpoch 24/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0046 - accuracy: 0.9994 - val_loss: 0.0290 - val_accuracy: 0.9906\nEpoch 25/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0042 - accuracy: 0.9994 - val_loss: 0.0292 - val_accuracy: 0.9906\nEpoch 26/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0038 - accuracy: 0.9994 - val_loss: 0.0295 - val_accuracy: 0.9903\nEpoch 27/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0035 - accuracy: 0.9996 - val_loss: 0.0298 - val_accuracy: 0.9901\nEpoch 28/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0032 - accuracy: 0.9996 - val_loss: 0.0301 - val_accuracy: 0.9901\nEpoch 29/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0030 - accuracy: 0.9996 - val_loss: 0.0305 - val_accuracy: 0.9901\nEpoch 30/30\n180/180 [==============================] - 2s 10ms/step - loss: 0.0027 - accuracy: 0.9997 - val_loss: 0.0310 - val_accuracy: 0.9906\n\n\n\nplt.plot(history_combined.history[\"accuracy\"], label = \"training accuracy\")\nplt.plot(history_combined.history[\"val_accuracy\"], label = \"validation accuracy\")\nplt.legend()\nplt.show()\n\n\n\n\nWe got a validation accuracy of over 99%! That’s really great performance of the model! This is actually the highest validation accuracy of the model, compared to when we trained solely on the titles or solely on the text. Therefore, we should be using both the title and text to detect whether something is fake news or not, because of its amazing performance. Now let’s see how our best model, the one using both titles and text, performs on unseen test data. Let’s download it in the code cell below.\n\ntest_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true\"\ntest_df = pd.read_csv(test_url)\ntest_dataset = make_dataset(test_df)\n\n\ncombined_model.evaluate(test_dataset)\n\n  1/225 [..............................] - ETA: 2s - loss: 0.0432 - accuracy: 0.9800225/225 [==============================] - 1s 5ms/step - loss: 0.0423 - accuracy: 0.9890\n\n\n[0.04232485964894295, 0.9889972805976868]\n\n\nIf we used the above model to detect fake news, we’d be right over 98% of the time! That’s very impresive for us! Now let’s visualize the embeddings that we trained for this model.\n\nweights = np.array(combined_model.get_layer(\"embedding\").get_weights())[0]\nvocab = title_vectorize_layer.get_vocabulary() # we need to pick between the title and vocab; we arbitrarily pick title's vocabulary\n\nWe can plot this on over an x and y axis, and take a look at the embeddings that we’ve trained. Since we already had an output size of 2 for the embedding layer, we don’t have to do any additional processing and we’ll just visualize using the layer directly.\n\nweights.shape\n\n(2000, 2)\n\n\n\nweights_df = pd.DataFrame(weights, columns=[\"x\", \"y\"])\n\n\nweights_df[\"vocab\"] = vocab\n\n\npx.scatter(\n    data_frame = weights_df,\n    x = \"x\",\n    y = \"y\",\n    hover_data = \"vocab\"\n)\n\n\n\n\nThese are the embeddings that we trained! From looking at this diagram, we see words like “seek”, “talks”, “needs”, and country names like Israel and The Philippines on the left side of the graph. These words generally seem objective, and these are the types of words we’d see in news that isn’t fake. However, on the right side of the graph, we see words like “fired”, “nightmare”, “breaking”, “racist”, and more. President names like Obama and Trump are in this area as well. This language we see to the right seems a lot more charged, and these are the words that likely were found in the fake news aritcles. This is a good visualization and confirmation that we trained the embeddings correctly!"
  },
  {
    "objectID": "posts/hand-gesture-tutorial/index.html",
    "href": "posts/hand-gesture-tutorial/index.html",
    "title": "Hand Gesture Media Player",
    "section": "",
    "text": "In this project walkthrough, we’ll go through how we created a program that reads in gestures from the computer’s webcam and controls your media player for you! This includes functions like changing the volume, skipping tracks, and even scrubbing forwards/backwards on media! We’ll talk about how Google’s MediaPipe works, how we trained the model, how we got the computer to control the user’s media player, and go into the GUI as well. For reference, the link to the github repository is here: https://github.com/ReetinavDas/Hand-Gesture-Media-Player.\n\n\n\nQuick demo of some of the gestures\n\n\n\n\n\nOpenCV (Open Source Computer Vision) is an open-source library of programming functions mainly aimed at real-time computer vision. It provides tools and utilities for computer vision tasks, such as image and video processing. OpenCV is widely used in fields like robotics, machine learning, image and video analysis, and computer vision applications. Here’s a simple example to see how this works:\nimport cv2\n\n# Read an image from file\nimage = cv2.imread('path/to/your/image.jpg')\n\n# Display the image\ncv2.imshow('Image', image)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\nIn this code, it is reading in an image in the user’s filepath, and this image can be showed in a new window by using the cv2.imshow('Image', image) function. The next two lines will continuously check if a key is pressed, and if so, it will close the windows currently displaying images.\nBut what if we want to use this to read input from our computer/laptop webcam? We’d especially want to know this since the focus of the project is detecting hand gestures from the computer webcam. Luckily this isn’t too complicated, and the code below is all we need to get the webcam up and running:\ncap = cv2.VideoCapture(0)\nwhile True:\n    ret, frame = cap.read()\n    cv2.imshow(\"frame\", frame)\n    if cv2.waitKey(1) == ord(\"q\"):\n        break\ncap.release()\ncv2.destroyAllWindows()\nThe function cap.read() returns the frame (the image itself), and ret tells us whether or not the capture worked properly. Once we store the output in frame we can send that over to the MediaPipe program to convert that image into hand landmarks. Before we go into that we’ll take a closer look into MediaPipe.\n\n\n\nMediaPipe is a package by Google which simplifies a lot of common machine learning tasks, allowing one to skip the strenous process of training millions of samples. They have many different “solutions” to common challenges, like audio, natural language, etc. One of these solutions are their Hand Landmarks Detection program, which takes in pictures of hands and translates them into numerical coordinates. We will use this package to teach the model how to detect a hand gesture. The main benefit of this is that we do not need to directly train on the images of hands using convolutional neural networks, which typically require a very large amount of data and a lot of time to train. Instead, we just train on the coordinates which is a lot simpler and faster to compute. We can see the landmarks that the program extracts from the hand images below. This simplifies the image down to 21 coordinates of the hand, with 4 points for each finger, and an additional one for the wrist.\n\n\n\nVisualization of Hand Tracking Landmarks\n\n\nLet’s put this all together and see if we can visualize these keypoints in real time! We first import the necessary packages:\nimport numpy as np\nimport cv2\nimport time\nimport mediapipe as mp\nfrom mediapipe.tasks import python\nfrom mediapipe.tasks.python import vision\nWe will then create a function that will take use the image read in by OpenCV and annotate it with the keypoints. The function definition is as below:\ndef draw_landmarks_on_image(rgb_image, detection_result):\n  hand_landmarks_list = detection_result.hand_landmarks\n  handedness_list = detection_result.handedness\n  annotated_image = np.copy(rgb_image)\n\n  # Loop through the detected hands to visualize.\n  for idx in range(len(hand_landmarks_list)):\n    hand_landmarks = hand_landmarks_list[idx]\n    handedness = handedness_list[idx]\n\n    # Draw the hand landmarks.\n    hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n    hand_landmarks_proto.landmark.extend([\n      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks\n    ])\n    solutions.drawing_utils.draw_landmarks(\n      annotated_image,\n      hand_landmarks_proto,\n      solutions.hands.HAND_CONNECTIONS,\n      solutions.drawing_styles.get_default_hand_landmarks_style(),\n      solutions.drawing_styles.get_default_hand_connections_style())\n\n    # Get the top left corner of the detected hand's bounding box.\n    height, width, _ = annotated_image.shape\n    x_coordinates = [landmark.x for landmark in hand_landmarks]\n    y_coordinates = [landmark.y for landmark in hand_landmarks]\n    text_x = int(min(x_coordinates) * width)\n    text_y = int(min(y_coordinates) * height) - MARGIN\n\n    # Draw handedness (left or right hand) on the image.\n    cv2.putText(annotated_image, f\"{handedness[0].category_name}\",\n                (text_x, text_y), cv2.FONT_HERSHEY_DUPLEX,\n                FONT_SIZE, HANDEDNESS_TEXT_COLOR, FONT_THICKNESS, cv2.LINE_AA)\n\n  return annotated_image\nNow we will bring this together. We will set up some constants that will handle the processing of hand frames and we’ll use the draw function to display the landmarks.\nmphands = mp.solutions.hands\nhands = mphands.Hands()\nmp_drawing = mp.solutions.drawing_utils\n\ncap = cv2.VideoCapture(0)\nwhile True:\n    ret, frame = cap.read() #returns the frame (the image itself), ret tells us whether the capture worked properly\n    framergb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    result = hands.process(framergb)\n\n    hand_landmarks = result.multi_hand_landmarks\n    if hand_landmarks:\n        for handLMs in hand_landmarks:\n            mp_drawing.draw_landmarks(frame, handLMs, mphands.HAND_CONNECTIONS)\n\n    cv2.imshow(\"frame\", frame)\n    if cv2.waitKey(1) == ord(\"q\"):\n        break\ncap.release()\ncv2.destroyAllWindows()\nSome things to note are that the actual processing is handled in the function call result = hands.process(framergb). Another major thing we should note is that when we create the framergb variable, we are using a modified version of the frame to initialize it. We use the cv2.COLOR_BGR2RGB parameter to convert the image from BGR formatting to RGB. This is because OpenCV by default reads in images in a BGR format, but MediaPipe requires images to be in RGB format before processing. Below is an image of what the hand landmarks look like in real life.\n\n\n\nHand Landmarks Example\n\n\nWe can clearly see the landmarks on my hand in the image above (the red dots). We are going to be training on these dots!\n\n\n\nHere comes the most interesting part of the project: computer vision machine learning! Like we said in the previous sections, instead of training on the images directly, we’ll be training on the hand landmark points. Let’s first import all of the packages that we’ll need.\n\nimport mediapipe as mp\nimport cv2\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.utils as utils\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport warnings\n\nmp_drawing = mp.solutions.drawing_utils\nmp_hands = mp.solutions.hands\nhands = mp_hands.Hands(min_detection_confidence=0.3, static_image_mode=True, max_num_hands=1)\nclasses = (\"down\", \"up\", \"stop\", \"thumbright\", \"thumbleft\", \"right\", \"left\", \"background\")\n\nLet’s talk about the classes for a bit. We have 7 main classes that we want our program to recognize. We will use pointing up and down with one’s index finger to control the volume. We will use an open palm facing towards the camera to toggle between pausing and playing the active media. We will use pointing to the right and to the left with our thumb to skip forwards and move backwards respectively in our playlist. Pointing right and left with our index finger will scrub the media forwards and backwards respectively by 10 seconds. The last label, which is not part of the 7 major classes, is called Background. We need to create a background class which consists of images that do not correspond to the other labels. To make this as usable as possible, we want to prevent the program from accidentally performing actions that are undesired. For example, we wouldn’t want the program to skip a track every time we were trying to scratch our nose, right? Because of this we created a background class where if the program doesn’t detect one of the 7 gestures, it’ll do nothing.\nWe will now continue by reading in the training data. Note that though the processing outputs an x,y,z axis, we only use the x and y coordinates since the z (depth) axis is unnecessary for stationary images. We are also going to be using one-hot-encoded vectors for the labels to not make the ordering significant to the model.\n\ntrain_data = []\ntrain_labels = []\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"cv2\")\n\nfor class_index, gesture_class in enumerate(classes):\n    for i in range(175):\n        try:\n            image = cv2.imread(f\"../../../Hand-Gesture-Media-Player/training/{gesture_class}.{i}.jpg\")\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # changes from bgr to rgb since cv2 is bgr but mediapipe requires rgb\n        except:\n            continue\n        image.flags.writeable = False\n        results = hands.process(image) # this makes the actual detections\n        \n        landmarks = []\n        if results.multi_hand_landmarks:\n            for landmark in results.multi_hand_landmarks[0].landmark:\n                x, y = landmark.x, landmark.y\n                landmarks.append([x,y])\n            train_label = np.zeros([len(classes)])\n            train_label[class_index] = 1\n            train_data.append(landmarks)\n            train_labels.append(train_label)\n\nLet’s peek at what our training data looks like:\n\ntrain_data[:2]\n\n[[[0.11390243470668793, 0.38363003730773926],\n  [0.1847730278968811, 0.45156458020210266],\n  [0.21980813145637512, 0.6019423604011536],\n  [0.19830942153930664, 0.7313971519470215],\n  [0.16206201910972595, 0.7761077880859375],\n  [0.18846909701824188, 0.5634511113166809],\n  [0.18456439673900604, 0.7657402157783508],\n  [0.17698808014392853, 0.8699557781219482],\n  [0.1677616834640503, 0.952928900718689],\n  [0.12264260649681091, 0.5713459849357605],\n  [0.1421872079372406, 0.7661824226379395],\n  [0.15865662693977356, 0.7259009480476379],\n  [0.15946514904499054, 0.6688014268875122],\n  [0.07189302146434784, 0.5825060606002808],\n  [0.10543633252382278, 0.7401406168937683],\n  [0.11655972898006439, 0.6992499232292175],\n  [0.11231370270252228, 0.6501151919364929],\n  [0.03424684703350067, 0.5949092507362366],\n  [0.0696953684091568, 0.7219727635383606],\n  [0.0826505571603775, 0.6901166439056396],\n  [0.07980084419250488, 0.6466896533966064]],\n [[0.22970367968082428, 0.47063347697257996],\n  [0.2716544568538666, 0.5102194547653198],\n  [0.28997427225112915, 0.594897985458374],\n  [0.28405407071113586, 0.6771860718727112],\n  [0.2713959515094757, 0.7295496463775635],\n  [0.2887209355831146, 0.5984773635864258],\n  [0.28781017661094666, 0.7187618017196655],\n  [0.2832407057285309, 0.7820509076118469],\n  [0.2787639796733856, 0.8315849304199219],\n  [0.25375017523765564, 0.6028256416320801],\n  [0.2544407546520233, 0.7210322618484497],\n  [0.2565171718597412, 0.6930841207504272],\n  [0.25780150294303894, 0.654019832611084],\n  [0.22042502462863922, 0.603621244430542],\n  [0.22629037499427795, 0.7026846408843994],\n  [0.22974006831645966, 0.6755079627037048],\n  [0.2297360599040985, 0.6399338245391846],\n  [0.19227087497711182, 0.6022643446922302],\n  [0.20413926243782043, 0.6789812445640564],\n  [0.2096225768327713, 0.6574336886405945],\n  [0.20850571990013123, 0.6306650638580322]]]\n\n\nWe took the first two images represented as hand landmarks. As you can see, for each image there are 21 coordinates, each having an x value and a y value. This is the data we’ll be training on, but to get it ready we’ll need to convert this to tensors and load these into a custom PyTorch dataset.\n\ntrain_data = torch.tensor(train_data)\ntrain_labels = torch.tensor(train_labels)\n\n\ntrain_data.shape\n\ntorch.Size([796, 21, 2])\n\n\nAbove we can see the shape of our dataset. We have 796 samples of our data in all, with around 100 images per class. We will now create a class that inherits from PyTorch’s Dataset class. By doing this we can use useful PyTorch functions that will greatly simplify our training process.\n\nclass LandmarksDataset(utils.data.Dataset):\n    def __init__(self, X, y, transform=None):\n        self.X = X\n        self.y = y\n        self.len = len(y)\n        self.transform = transform\n    def __len__(self):\n        return self.len\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\n\ntraining_set = LandmarksDataset(train_data, train_labels)\ntraining_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True) # we set shuffle to true for faster convergence\n\nGreat! Now let’s do the same for the validation data.\n\nval_data = []\nval_labels = []\nfor class_index, gesture_class in enumerate(classes):\n    for i in range(40):\n        image = cv2.imread(f\"../../../Hand-Gesture-Media-Player/validation/{gesture_class}.{i}.jpg\")\n\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # changes from bgr to rgb since cv2 is bgr but mediapipe requires rgb\n        image.flags.writeable = False\n        results = hands.process(image) # this makes the actual detections\n        \n        landmarks = []\n        if results.multi_hand_landmarks:\n            for landmark in results.multi_hand_landmarks[0].landmark:\n                x, y = landmark.x, landmark.y\n                landmarks.append([x,y])\n            val_label = np.zeros([len(classes)])\n            val_label[class_index] = 1\n            val_data.append(landmarks)\n            val_labels.append(val_label)\n\n\nval_data = torch.tensor(val_data)\nval_labels = torch.tensor(val_labels)\n\n\nvalidation_set = LandmarksDataset(val_data, val_labels)\nvalidation_loader = torch.utils.data.DataLoader(validation_set, batch_size=4, shuffle=False)\n\n\nval_data.shape\n\ntorch.Size([313, 21, 2])\n\n\nHere comes the fun part. We’ll now construct our model for reading in the hand gesture coordinates. To do this, we’ll use a couple of fully connected linear layers. Of course, we’ll need to use an activation function between each layer. In this case, I have opted to use LeakyReLU. The output will be a vector the length of the number of classes (including background) and it will consist of the logits (scores) of each gesture class. From there we can pick the class with the highest score as the correct class, and softmax the output to find the probability scores. For background, the softmax function squishes the outputs into a range from 0 to 1.\n\nclass HandNetwork(nn.Module):\n    def __init__(self):\n        super(HandNetwork, self).__init__()\n        self.flatten = nn.Flatten()\n        self.relu = nn.LeakyReLU()\n        self.fc1 = nn.Linear(42, 120)\n        self.fc2 = nn.Linear(120, 100)\n        self.fc3 = nn.Linear(100, 100)\n        self.fc4 = nn.Linear(100, len(classes))\n    def forward(self, x):\n        x = self.flatten(x)\n        x = self.relu(self.fc1(x))\n        x = self.relu(self.fc2(x))\n        x = self.relu(self.fc3(x))\n        x = self.fc4(x)\n        return x\n\n\nmodel = HandNetwork()\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.1)\n\n\ndef train_one_epoch(curr_model):\n    last_loss = 0\n\n    for i, data in enumerate(training_loader):\n        inputs, labels = data\n        optimizer.zero_grad()\n        outputs = curr_model(inputs)\n        loss = loss_fn(outputs, labels)\n        loss.backward() # calculate the gradients\n        optimizer.step() # update the params\n\n    return last_loss\n\n\ntimestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n\n#This is doing some logging that we don't need to worry about right now.\nepoch_number = 0\nEPOCHS = 300\nbest_vloss = 1_000_000.\nval_history = []\nbest_model = model\n\nfor epoch in range(EPOCHS):\n    \n    model.train(True)\n    \n    avg_loss = train_one_epoch(curr_model=model)\n\n    # We don't need gradients on to do reporting\n    model.train(False)\n\n    running_vloss = 0.0\n    for i, vdata in enumerate(validation_loader):\n        vinputs, vlabels = vdata\n        voutputs = model(vinputs)\n        vloss = loss_fn(voutputs, vlabels)\n        running_vloss += vloss\n\n    avg_vloss = running_vloss / (i + 1)\n    val_history.append(avg_vloss.detach().numpy())\n\n    if (epoch_number+1) % 50 == 0:\n        print('EPOCH {}:'.format(epoch_number + 1))\n        print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n\n    \n    # Track best performance, and save the model's state\n    if avg_vloss &lt; best_vloss:\n        best_vloss = avg_vloss\n        torch.save(model, \"model.pth\")\n    epoch_number += 1\n\nplt.plot(range(EPOCHS), val_history)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Validation Loss\")\n\nEPOCH 50:\nLOSS train 0 valid 0.8300335395468187\nEPOCH 100:\nLOSS train 0 valid 0.3033125001335825\nEPOCH 150:\nLOSS train 0 valid 0.26129591822483017\nEPOCH 200:\nLOSS train 0 valid 0.2604941233988927\nEPOCH 250:\nLOSS train 0 valid 0.24089700083625049\nEPOCH 300:\nLOSS train 0 valid 0.24952827380335454\n\n\nText(0, 0.5, 'Validation Loss')\n\n\n\n\n\n\nbest_vloss\n\ntensor(0.2044, dtype=torch.float64, grad_fn=&lt;DivBackward0&gt;)\n\n\nOur best loss was 0.2044. Not bad at all!\n\nimport torch.nn.functional as F\nfrom sklearn.metrics import f1_score\n\n\nmodel = torch.load(\"model.pth\")\nmodel.eval()\ncorrect_count = 0\ntotal = len(validation_set)\nwith torch.no_grad():\n    for i, vdata in enumerate(validation_loader):\n        inputs, labels = vdata\n        outputs = torch.argmax(model(inputs), dim=1)\n        labels = torch.argmax(labels, dim=1)\n        correct_count += (outputs==labels).sum().item()\n\n\nprint(\"Accuracy:\", correct_count/total)\n\nAccuracy: 0.9616613418530351\n\n\n\nprint(\"F1 Score:\", f1_score(np.argmax(model(val_data).detach().numpy(), axis=1), np.argmax(val_labels.detach().numpy(), axis=1), average='macro'))\n\nF1 Score: 0.960922512702576\n\n\nWe got a great accuracy and F1 Score! We can now use this model to predict hand gestures.\n\n\n\nTime for the fun part. We’ll explore how to control the computer using Python, and the functions we create will be linked to the gesture recognition process. We will be using a module named PyAutoGUI to simulate keypresses. We do this because modern laptops typically have media control buttons, like volume up/down, pause/play, and more! The neat thing is, even when a computer doesn’t have a specific button, using PyAutoGUI to simulate that button actually still works! We should first import the necessary packages:\nimport time\nimport pyautogui\nimport platform\nimport subprocess\nThe reason we are using the time package is because when we have our gestures activating commands, we need to be careful with how often the command will be executed. The idea is that the camera will be continuously checking the gesture in the frame, and controlling the camera. But for example, what if you wanted to skip the track and put your thumb pointing to the right for 5 frames? (Less than half a second) Then the program would skip 5 songs when you just wanted to skip one! To prevent this from happening, we use the time.sleep(n) function to stop the code for n seconds. This way, we won’t have issues where we skip way too far ahead in a song or switch tracks too many times.\nAs for the platform module, we import this to account for the differences between Mac and Windows. (Sorry Linux, we couldn’t figure you out.) We had to use different PyAutoGUI commands for Mac and Windows since Mac had a couple of issues we had to fix. As for subprocess, we will be using it to implement the scrub features for currently playing media, though it only works for Macs. :( We will walk through some of the gesture functions that were implemented.\nos = platform.system()\n\ndef increase_volume():\n    \"\"\"\n    The increase_volume() function allows Python to increase the volume of\n    your system.\n    \"\"\"\n    # MacOS Execute\n    if os == \"Darwin\":\n        pyautogui.press(u'KEYTYPE_SOUND_UP') \n    # Windows Execute\n    else:\n        pyautogui.press(\"volumeup\")\nIn this function, we first check whether the system is using MacOS or not using the platform module. For some reason, the name for Mac is “Darwin” (for some reason) and the name for Windows is “Windows”. Note that the PyAutoGUI commands are slightly different even though they perform the exact same thing. Here’s another implementation which in this case involves the time.sleep(n) method.\ndef play_pause():\n    \"\"\"\n    The play_pause() function allows Python to pause or play any media on\n    your system. \n    \"\"\"\n    # MacOS Execute\n    if os == \"Darwin\":\n        pyautogui.press(u'KEYTYPE_PLAY')\n    # Windows Execute\n    else:\n        pyautogui.press(\"playpause\")\n    time.sleep(1)\nSince we want to toggle between playing/pausing the media with a gesture, we use time.sleep(1) to ensure that it isn’t triggered too often. This means that if we just keep our hand in front of the camera in the play/pause pose, then the program will toggle the action every second instead of every frame.\n\n\n\nThe GUI is designed to create a more user-friendly medium to interact with the program. The goal is to communicate more information to the user regarding the use of the program and include modifyable parameters for the user to adjust the sensitivity of the model. Information on enabling the camera is initialized as the first possible input. More information regarding the list of recognized gestures is included in a table. The last tab lists hotkeys for the program. The only hotkey implemented was to turn off the camera, or Q. The GUI was linked to the Camera class, with an instance of the class being initialized and ran upon clicking the “Turn on camera” button. From there, OpenCV is initialized to enable your camera. Lastly, the GUI includes an adjustable parameter for the user to modify. This allows for the modification of the model’s input sensitivity, or how confident the model must be in order to read the input. The slider input is passed through the class and divided by 100 to convert into a usable input for the Camera object. Once this object is initialized, we can start reading in the gestures.\n# Event Trigger for Camera Enable #\nif event == 'Turn on camera':\n    cap = Camera(values['-PER-']/100)\n    cap.start_capture_session()\nThere is also a “Check” button, where the console outputs the value of the given slider input. This is a troubleshooting feature.\n\n\n\nThe GUI window\n\n\n\n\n\nTo wrap up, we believe this to be a successful endeavor of creating gesture recognition software! Ideas for future direction may include adding a feature where users can record their own gesture and add it to the model, or a more extensive list of macros that the gestures can perform for the user to pick from. Ethical ramifications of our project are pretty limited, MediaPipe picks up all skin tones very well, and this program should be accessible for anyone. Some concerns however could be security, in that the program has access to the user’s camera. We would have to learn more about encryption/data protection for this. Additionally if we allow users to create their own gestures, that calls into possiblitiy inappropriate or hateful gestures. We would need to implement some sort of database of hateful gestures in order to counteraact and block these."
  },
  {
    "objectID": "posts/hand-gesture-tutorial/index.html#overview",
    "href": "posts/hand-gesture-tutorial/index.html#overview",
    "title": "Hand Gesture Media Player",
    "section": "",
    "text": "In this project walkthrough, we’ll go through how we created a program that reads in gestures from the computer’s webcam and controls your media player for you! This includes functions like changing the volume, skipping tracks, and even scrubbing forwards/backwards on media! We’ll talk about how Google’s MediaPipe works, how we trained the model, how we got the computer to control the user’s media player, and go into the GUI as well. For reference, the link to the github repository is here: https://github.com/ReetinavDas/Hand-Gesture-Media-Player.\n\n\n\nQuick demo of some of the gestures"
  },
  {
    "objectID": "posts/hand-gesture-tutorial/index.html#opencv-introduction",
    "href": "posts/hand-gesture-tutorial/index.html#opencv-introduction",
    "title": "Hand Gesture Media Player",
    "section": "",
    "text": "OpenCV (Open Source Computer Vision) is an open-source library of programming functions mainly aimed at real-time computer vision. It provides tools and utilities for computer vision tasks, such as image and video processing. OpenCV is widely used in fields like robotics, machine learning, image and video analysis, and computer vision applications. Here’s a simple example to see how this works:\nimport cv2\n\n# Read an image from file\nimage = cv2.imread('path/to/your/image.jpg')\n\n# Display the image\ncv2.imshow('Image', image)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\nIn this code, it is reading in an image in the user’s filepath, and this image can be showed in a new window by using the cv2.imshow('Image', image) function. The next two lines will continuously check if a key is pressed, and if so, it will close the windows currently displaying images.\nBut what if we want to use this to read input from our computer/laptop webcam? We’d especially want to know this since the focus of the project is detecting hand gestures from the computer webcam. Luckily this isn’t too complicated, and the code below is all we need to get the webcam up and running:\ncap = cv2.VideoCapture(0)\nwhile True:\n    ret, frame = cap.read()\n    cv2.imshow(\"frame\", frame)\n    if cv2.waitKey(1) == ord(\"q\"):\n        break\ncap.release()\ncv2.destroyAllWindows()\nThe function cap.read() returns the frame (the image itself), and ret tells us whether or not the capture worked properly. Once we store the output in frame we can send that over to the MediaPipe program to convert that image into hand landmarks. Before we go into that we’ll take a closer look into MediaPipe."
  },
  {
    "objectID": "posts/hand-gesture-tutorial/index.html#mediapipe-introduction",
    "href": "posts/hand-gesture-tutorial/index.html#mediapipe-introduction",
    "title": "Hand Gesture Media Player",
    "section": "",
    "text": "MediaPipe is a package by Google which simplifies a lot of common machine learning tasks, allowing one to skip the strenous process of training millions of samples. They have many different “solutions” to common challenges, like audio, natural language, etc. One of these solutions are their Hand Landmarks Detection program, which takes in pictures of hands and translates them into numerical coordinates. We will use this package to teach the model how to detect a hand gesture. The main benefit of this is that we do not need to directly train on the images of hands using convolutional neural networks, which typically require a very large amount of data and a lot of time to train. Instead, we just train on the coordinates which is a lot simpler and faster to compute. We can see the landmarks that the program extracts from the hand images below. This simplifies the image down to 21 coordinates of the hand, with 4 points for each finger, and an additional one for the wrist.\n\n\n\nVisualization of Hand Tracking Landmarks\n\n\nLet’s put this all together and see if we can visualize these keypoints in real time! We first import the necessary packages:\nimport numpy as np\nimport cv2\nimport time\nimport mediapipe as mp\nfrom mediapipe.tasks import python\nfrom mediapipe.tasks.python import vision\nWe will then create a function that will take use the image read in by OpenCV and annotate it with the keypoints. The function definition is as below:\ndef draw_landmarks_on_image(rgb_image, detection_result):\n  hand_landmarks_list = detection_result.hand_landmarks\n  handedness_list = detection_result.handedness\n  annotated_image = np.copy(rgb_image)\n\n  # Loop through the detected hands to visualize.\n  for idx in range(len(hand_landmarks_list)):\n    hand_landmarks = hand_landmarks_list[idx]\n    handedness = handedness_list[idx]\n\n    # Draw the hand landmarks.\n    hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n    hand_landmarks_proto.landmark.extend([\n      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks\n    ])\n    solutions.drawing_utils.draw_landmarks(\n      annotated_image,\n      hand_landmarks_proto,\n      solutions.hands.HAND_CONNECTIONS,\n      solutions.drawing_styles.get_default_hand_landmarks_style(),\n      solutions.drawing_styles.get_default_hand_connections_style())\n\n    # Get the top left corner of the detected hand's bounding box.\n    height, width, _ = annotated_image.shape\n    x_coordinates = [landmark.x for landmark in hand_landmarks]\n    y_coordinates = [landmark.y for landmark in hand_landmarks]\n    text_x = int(min(x_coordinates) * width)\n    text_y = int(min(y_coordinates) * height) - MARGIN\n\n    # Draw handedness (left or right hand) on the image.\n    cv2.putText(annotated_image, f\"{handedness[0].category_name}\",\n                (text_x, text_y), cv2.FONT_HERSHEY_DUPLEX,\n                FONT_SIZE, HANDEDNESS_TEXT_COLOR, FONT_THICKNESS, cv2.LINE_AA)\n\n  return annotated_image\nNow we will bring this together. We will set up some constants that will handle the processing of hand frames and we’ll use the draw function to display the landmarks.\nmphands = mp.solutions.hands\nhands = mphands.Hands()\nmp_drawing = mp.solutions.drawing_utils\n\ncap = cv2.VideoCapture(0)\nwhile True:\n    ret, frame = cap.read() #returns the frame (the image itself), ret tells us whether the capture worked properly\n    framergb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    result = hands.process(framergb)\n\n    hand_landmarks = result.multi_hand_landmarks\n    if hand_landmarks:\n        for handLMs in hand_landmarks:\n            mp_drawing.draw_landmarks(frame, handLMs, mphands.HAND_CONNECTIONS)\n\n    cv2.imshow(\"frame\", frame)\n    if cv2.waitKey(1) == ord(\"q\"):\n        break\ncap.release()\ncv2.destroyAllWindows()\nSome things to note are that the actual processing is handled in the function call result = hands.process(framergb). Another major thing we should note is that when we create the framergb variable, we are using a modified version of the frame to initialize it. We use the cv2.COLOR_BGR2RGB parameter to convert the image from BGR formatting to RGB. This is because OpenCV by default reads in images in a BGR format, but MediaPipe requires images to be in RGB format before processing. Below is an image of what the hand landmarks look like in real life.\n\n\n\nHand Landmarks Example\n\n\nWe can clearly see the landmarks on my hand in the image above (the red dots). We are going to be training on these dots!"
  },
  {
    "objectID": "posts/hand-gesture-tutorial/index.html#pytorch-and-training-the-model",
    "href": "posts/hand-gesture-tutorial/index.html#pytorch-and-training-the-model",
    "title": "Hand Gesture Media Player",
    "section": "",
    "text": "Here comes the most interesting part of the project: computer vision machine learning! Like we said in the previous sections, instead of training on the images directly, we’ll be training on the hand landmark points. Let’s first import all of the packages that we’ll need.\n\nimport mediapipe as mp\nimport cv2\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.utils as utils\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport warnings\n\nmp_drawing = mp.solutions.drawing_utils\nmp_hands = mp.solutions.hands\nhands = mp_hands.Hands(min_detection_confidence=0.3, static_image_mode=True, max_num_hands=1)\nclasses = (\"down\", \"up\", \"stop\", \"thumbright\", \"thumbleft\", \"right\", \"left\", \"background\")\n\nLet’s talk about the classes for a bit. We have 7 main classes that we want our program to recognize. We will use pointing up and down with one’s index finger to control the volume. We will use an open palm facing towards the camera to toggle between pausing and playing the active media. We will use pointing to the right and to the left with our thumb to skip forwards and move backwards respectively in our playlist. Pointing right and left with our index finger will scrub the media forwards and backwards respectively by 10 seconds. The last label, which is not part of the 7 major classes, is called Background. We need to create a background class which consists of images that do not correspond to the other labels. To make this as usable as possible, we want to prevent the program from accidentally performing actions that are undesired. For example, we wouldn’t want the program to skip a track every time we were trying to scratch our nose, right? Because of this we created a background class where if the program doesn’t detect one of the 7 gestures, it’ll do nothing.\nWe will now continue by reading in the training data. Note that though the processing outputs an x,y,z axis, we only use the x and y coordinates since the z (depth) axis is unnecessary for stationary images. We are also going to be using one-hot-encoded vectors for the labels to not make the ordering significant to the model.\n\ntrain_data = []\ntrain_labels = []\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"cv2\")\n\nfor class_index, gesture_class in enumerate(classes):\n    for i in range(175):\n        try:\n            image = cv2.imread(f\"../../../Hand-Gesture-Media-Player/training/{gesture_class}.{i}.jpg\")\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # changes from bgr to rgb since cv2 is bgr but mediapipe requires rgb\n        except:\n            continue\n        image.flags.writeable = False\n        results = hands.process(image) # this makes the actual detections\n        \n        landmarks = []\n        if results.multi_hand_landmarks:\n            for landmark in results.multi_hand_landmarks[0].landmark:\n                x, y = landmark.x, landmark.y\n                landmarks.append([x,y])\n            train_label = np.zeros([len(classes)])\n            train_label[class_index] = 1\n            train_data.append(landmarks)\n            train_labels.append(train_label)\n\nLet’s peek at what our training data looks like:\n\ntrain_data[:2]\n\n[[[0.11390243470668793, 0.38363003730773926],\n  [0.1847730278968811, 0.45156458020210266],\n  [0.21980813145637512, 0.6019423604011536],\n  [0.19830942153930664, 0.7313971519470215],\n  [0.16206201910972595, 0.7761077880859375],\n  [0.18846909701824188, 0.5634511113166809],\n  [0.18456439673900604, 0.7657402157783508],\n  [0.17698808014392853, 0.8699557781219482],\n  [0.1677616834640503, 0.952928900718689],\n  [0.12264260649681091, 0.5713459849357605],\n  [0.1421872079372406, 0.7661824226379395],\n  [0.15865662693977356, 0.7259009480476379],\n  [0.15946514904499054, 0.6688014268875122],\n  [0.07189302146434784, 0.5825060606002808],\n  [0.10543633252382278, 0.7401406168937683],\n  [0.11655972898006439, 0.6992499232292175],\n  [0.11231370270252228, 0.6501151919364929],\n  [0.03424684703350067, 0.5949092507362366],\n  [0.0696953684091568, 0.7219727635383606],\n  [0.0826505571603775, 0.6901166439056396],\n  [0.07980084419250488, 0.6466896533966064]],\n [[0.22970367968082428, 0.47063347697257996],\n  [0.2716544568538666, 0.5102194547653198],\n  [0.28997427225112915, 0.594897985458374],\n  [0.28405407071113586, 0.6771860718727112],\n  [0.2713959515094757, 0.7295496463775635],\n  [0.2887209355831146, 0.5984773635864258],\n  [0.28781017661094666, 0.7187618017196655],\n  [0.2832407057285309, 0.7820509076118469],\n  [0.2787639796733856, 0.8315849304199219],\n  [0.25375017523765564, 0.6028256416320801],\n  [0.2544407546520233, 0.7210322618484497],\n  [0.2565171718597412, 0.6930841207504272],\n  [0.25780150294303894, 0.654019832611084],\n  [0.22042502462863922, 0.603621244430542],\n  [0.22629037499427795, 0.7026846408843994],\n  [0.22974006831645966, 0.6755079627037048],\n  [0.2297360599040985, 0.6399338245391846],\n  [0.19227087497711182, 0.6022643446922302],\n  [0.20413926243782043, 0.6789812445640564],\n  [0.2096225768327713, 0.6574336886405945],\n  [0.20850571990013123, 0.6306650638580322]]]\n\n\nWe took the first two images represented as hand landmarks. As you can see, for each image there are 21 coordinates, each having an x value and a y value. This is the data we’ll be training on, but to get it ready we’ll need to convert this to tensors and load these into a custom PyTorch dataset.\n\ntrain_data = torch.tensor(train_data)\ntrain_labels = torch.tensor(train_labels)\n\n\ntrain_data.shape\n\ntorch.Size([796, 21, 2])\n\n\nAbove we can see the shape of our dataset. We have 796 samples of our data in all, with around 100 images per class. We will now create a class that inherits from PyTorch’s Dataset class. By doing this we can use useful PyTorch functions that will greatly simplify our training process.\n\nclass LandmarksDataset(utils.data.Dataset):\n    def __init__(self, X, y, transform=None):\n        self.X = X\n        self.y = y\n        self.len = len(y)\n        self.transform = transform\n    def __len__(self):\n        return self.len\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\n\ntraining_set = LandmarksDataset(train_data, train_labels)\ntraining_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True) # we set shuffle to true for faster convergence\n\nGreat! Now let’s do the same for the validation data.\n\nval_data = []\nval_labels = []\nfor class_index, gesture_class in enumerate(classes):\n    for i in range(40):\n        image = cv2.imread(f\"../../../Hand-Gesture-Media-Player/validation/{gesture_class}.{i}.jpg\")\n\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # changes from bgr to rgb since cv2 is bgr but mediapipe requires rgb\n        image.flags.writeable = False\n        results = hands.process(image) # this makes the actual detections\n        \n        landmarks = []\n        if results.multi_hand_landmarks:\n            for landmark in results.multi_hand_landmarks[0].landmark:\n                x, y = landmark.x, landmark.y\n                landmarks.append([x,y])\n            val_label = np.zeros([len(classes)])\n            val_label[class_index] = 1\n            val_data.append(landmarks)\n            val_labels.append(val_label)\n\n\nval_data = torch.tensor(val_data)\nval_labels = torch.tensor(val_labels)\n\n\nvalidation_set = LandmarksDataset(val_data, val_labels)\nvalidation_loader = torch.utils.data.DataLoader(validation_set, batch_size=4, shuffle=False)\n\n\nval_data.shape\n\ntorch.Size([313, 21, 2])\n\n\nHere comes the fun part. We’ll now construct our model for reading in the hand gesture coordinates. To do this, we’ll use a couple of fully connected linear layers. Of course, we’ll need to use an activation function between each layer. In this case, I have opted to use LeakyReLU. The output will be a vector the length of the number of classes (including background) and it will consist of the logits (scores) of each gesture class. From there we can pick the class with the highest score as the correct class, and softmax the output to find the probability scores. For background, the softmax function squishes the outputs into a range from 0 to 1.\n\nclass HandNetwork(nn.Module):\n    def __init__(self):\n        super(HandNetwork, self).__init__()\n        self.flatten = nn.Flatten()\n        self.relu = nn.LeakyReLU()\n        self.fc1 = nn.Linear(42, 120)\n        self.fc2 = nn.Linear(120, 100)\n        self.fc3 = nn.Linear(100, 100)\n        self.fc4 = nn.Linear(100, len(classes))\n    def forward(self, x):\n        x = self.flatten(x)\n        x = self.relu(self.fc1(x))\n        x = self.relu(self.fc2(x))\n        x = self.relu(self.fc3(x))\n        x = self.fc4(x)\n        return x\n\n\nmodel = HandNetwork()\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.1)\n\n\ndef train_one_epoch(curr_model):\n    last_loss = 0\n\n    for i, data in enumerate(training_loader):\n        inputs, labels = data\n        optimizer.zero_grad()\n        outputs = curr_model(inputs)\n        loss = loss_fn(outputs, labels)\n        loss.backward() # calculate the gradients\n        optimizer.step() # update the params\n\n    return last_loss\n\n\ntimestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n\n#This is doing some logging that we don't need to worry about right now.\nepoch_number = 0\nEPOCHS = 300\nbest_vloss = 1_000_000.\nval_history = []\nbest_model = model\n\nfor epoch in range(EPOCHS):\n    \n    model.train(True)\n    \n    avg_loss = train_one_epoch(curr_model=model)\n\n    # We don't need gradients on to do reporting\n    model.train(False)\n\n    running_vloss = 0.0\n    for i, vdata in enumerate(validation_loader):\n        vinputs, vlabels = vdata\n        voutputs = model(vinputs)\n        vloss = loss_fn(voutputs, vlabels)\n        running_vloss += vloss\n\n    avg_vloss = running_vloss / (i + 1)\n    val_history.append(avg_vloss.detach().numpy())\n\n    if (epoch_number+1) % 50 == 0:\n        print('EPOCH {}:'.format(epoch_number + 1))\n        print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n\n    \n    # Track best performance, and save the model's state\n    if avg_vloss &lt; best_vloss:\n        best_vloss = avg_vloss\n        torch.save(model, \"model.pth\")\n    epoch_number += 1\n\nplt.plot(range(EPOCHS), val_history)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Validation Loss\")\n\nEPOCH 50:\nLOSS train 0 valid 0.8300335395468187\nEPOCH 100:\nLOSS train 0 valid 0.3033125001335825\nEPOCH 150:\nLOSS train 0 valid 0.26129591822483017\nEPOCH 200:\nLOSS train 0 valid 0.2604941233988927\nEPOCH 250:\nLOSS train 0 valid 0.24089700083625049\nEPOCH 300:\nLOSS train 0 valid 0.24952827380335454\n\n\nText(0, 0.5, 'Validation Loss')\n\n\n\n\n\n\nbest_vloss\n\ntensor(0.2044, dtype=torch.float64, grad_fn=&lt;DivBackward0&gt;)\n\n\nOur best loss was 0.2044. Not bad at all!\n\nimport torch.nn.functional as F\nfrom sklearn.metrics import f1_score\n\n\nmodel = torch.load(\"model.pth\")\nmodel.eval()\ncorrect_count = 0\ntotal = len(validation_set)\nwith torch.no_grad():\n    for i, vdata in enumerate(validation_loader):\n        inputs, labels = vdata\n        outputs = torch.argmax(model(inputs), dim=1)\n        labels = torch.argmax(labels, dim=1)\n        correct_count += (outputs==labels).sum().item()\n\n\nprint(\"Accuracy:\", correct_count/total)\n\nAccuracy: 0.9616613418530351\n\n\n\nprint(\"F1 Score:\", f1_score(np.argmax(model(val_data).detach().numpy(), axis=1), np.argmax(val_labels.detach().numpy(), axis=1), average='macro'))\n\nF1 Score: 0.960922512702576\n\n\nWe got a great accuracy and F1 Score! We can now use this model to predict hand gestures."
  },
  {
    "objectID": "posts/hand-gesture-tutorial/index.html#controlling-media-with-gestures",
    "href": "posts/hand-gesture-tutorial/index.html#controlling-media-with-gestures",
    "title": "Hand Gesture Media Player",
    "section": "",
    "text": "Time for the fun part. We’ll explore how to control the computer using Python, and the functions we create will be linked to the gesture recognition process. We will be using a module named PyAutoGUI to simulate keypresses. We do this because modern laptops typically have media control buttons, like volume up/down, pause/play, and more! The neat thing is, even when a computer doesn’t have a specific button, using PyAutoGUI to simulate that button actually still works! We should first import the necessary packages:\nimport time\nimport pyautogui\nimport platform\nimport subprocess\nThe reason we are using the time package is because when we have our gestures activating commands, we need to be careful with how often the command will be executed. The idea is that the camera will be continuously checking the gesture in the frame, and controlling the camera. But for example, what if you wanted to skip the track and put your thumb pointing to the right for 5 frames? (Less than half a second) Then the program would skip 5 songs when you just wanted to skip one! To prevent this from happening, we use the time.sleep(n) function to stop the code for n seconds. This way, we won’t have issues where we skip way too far ahead in a song or switch tracks too many times.\nAs for the platform module, we import this to account for the differences between Mac and Windows. (Sorry Linux, we couldn’t figure you out.) We had to use different PyAutoGUI commands for Mac and Windows since Mac had a couple of issues we had to fix. As for subprocess, we will be using it to implement the scrub features for currently playing media, though it only works for Macs. :( We will walk through some of the gesture functions that were implemented.\nos = platform.system()\n\ndef increase_volume():\n    \"\"\"\n    The increase_volume() function allows Python to increase the volume of\n    your system.\n    \"\"\"\n    # MacOS Execute\n    if os == \"Darwin\":\n        pyautogui.press(u'KEYTYPE_SOUND_UP') \n    # Windows Execute\n    else:\n        pyautogui.press(\"volumeup\")\nIn this function, we first check whether the system is using MacOS or not using the platform module. For some reason, the name for Mac is “Darwin” (for some reason) and the name for Windows is “Windows”. Note that the PyAutoGUI commands are slightly different even though they perform the exact same thing. Here’s another implementation which in this case involves the time.sleep(n) method.\ndef play_pause():\n    \"\"\"\n    The play_pause() function allows Python to pause or play any media on\n    your system. \n    \"\"\"\n    # MacOS Execute\n    if os == \"Darwin\":\n        pyautogui.press(u'KEYTYPE_PLAY')\n    # Windows Execute\n    else:\n        pyautogui.press(\"playpause\")\n    time.sleep(1)\nSince we want to toggle between playing/pausing the media with a gesture, we use time.sleep(1) to ensure that it isn’t triggered too often. This means that if we just keep our hand in front of the camera in the play/pause pose, then the program will toggle the action every second instead of every frame."
  },
  {
    "objectID": "posts/hand-gesture-tutorial/index.html#gui-makin-things-pretty",
    "href": "posts/hand-gesture-tutorial/index.html#gui-makin-things-pretty",
    "title": "Hand Gesture Media Player",
    "section": "",
    "text": "The GUI is designed to create a more user-friendly medium to interact with the program. The goal is to communicate more information to the user regarding the use of the program and include modifyable parameters for the user to adjust the sensitivity of the model. Information on enabling the camera is initialized as the first possible input. More information regarding the list of recognized gestures is included in a table. The last tab lists hotkeys for the program. The only hotkey implemented was to turn off the camera, or Q. The GUI was linked to the Camera class, with an instance of the class being initialized and ran upon clicking the “Turn on camera” button. From there, OpenCV is initialized to enable your camera. Lastly, the GUI includes an adjustable parameter for the user to modify. This allows for the modification of the model’s input sensitivity, or how confident the model must be in order to read the input. The slider input is passed through the class and divided by 100 to convert into a usable input for the Camera object. Once this object is initialized, we can start reading in the gestures.\n# Event Trigger for Camera Enable #\nif event == 'Turn on camera':\n    cap = Camera(values['-PER-']/100)\n    cap.start_capture_session()\nThere is also a “Check” button, where the console outputs the value of the given slider input. This is a troubleshooting feature.\n\n\n\nThe GUI window"
  },
  {
    "objectID": "posts/hand-gesture-tutorial/index.html#conclusion",
    "href": "posts/hand-gesture-tutorial/index.html#conclusion",
    "title": "Hand Gesture Media Player",
    "section": "",
    "text": "To wrap up, we believe this to be a successful endeavor of creating gesture recognition software! Ideas for future direction may include adding a feature where users can record their own gesture and add it to the model, or a more extensive list of macros that the gestures can perform for the user to pick from. Ethical ramifications of our project are pretty limited, MediaPipe picks up all skin tones very well, and this program should be accessible for anyone. Some concerns however could be security, in that the program has access to the user’s camera. We would have to learn more about encryption/data protection for this. Additionally if we allow users to create their own gestures, that calls into possiblitiy inappropriate or hateful gestures. We would need to implement some sort of database of hateful gestures in order to counteraact and block these."
  },
  {
    "objectID": "posts/penguins/index.html",
    "href": "posts/penguins/index.html",
    "title": "Plentiful Palmer Penguins",
    "section": "",
    "text": "We will be taking a look at the statistics of the Palmer Penguins Dataset.\nWe will set things up by importing necessary packages and taking a look at the first few rows of the dataset.\n\nimport pandas as pd\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\nurl = \"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\npenguins.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n\n\n\n\n\nThis is great info! Let’s take a look and see the different kinds of species that exist for the penguins.\n\nprint(penguins[\"Species\"].unique())\n\n['Adelie Penguin (Pygoscelis adeliae)'\n 'Chinstrap penguin (Pygoscelis antarctica)'\n 'Gentoo penguin (Pygoscelis papua)']\n\n\nWe are interested in the differences between the three species. We will explore these differences by looking at traits like Culmen Length, Flipper Length, and more! First we will drop rows where these values are NaN, or in other words, not available.\n\npenguins = penguins.dropna(subset=[\"Culmen Length (mm)\", \n                                   \"Culmen Depth (mm)\", \n                                   \"Flipper Length (mm)\", \n                                   \"Body Mass (g)\"])\n\nWe will first take a look at the culmen length and depth of the penguins using plotly. We will set the x axis to be culmen length and the y axis to be culmen depth. To make this even more useful, we’ll color code the points by species so we can see how these statistics differ.\n\nfrom plotly import express as px\nfig = px.scatter(data_frame=penguins, \n                 x=\"Culmen Length (mm)\", \n                 y=\"Culmen Depth (mm)\", \n                 color=\"Species\", \n                 hover_name=\"Species\", \n                 hover_data=[\"Island\",\"Sex\"], \n                 width=700, \n                 height=300)\n#reduce whitespace\nfig.update_layout(margin={\"r\":0, \"t\":0, \"l\":0, \"b\":0})\nfig.show()\n\n\n\n\nWe can see here that culmen length and culmen depth can be pretty good predictors for what species a penguin could be! From this plot we see that Adelie penguins tend to have lower culmen depth than the other two species, whereas Gentoo penguins have lower culmen length than the other two. Chinstrap penguins tend to have both higher culmen depth and length.\nLet’s go ahead and take a look at the other two attributes: Flipper Length and Body Mass. We will do a similar plot and see the distribution and see if there are any interesting facts.\n\nfig = px.scatter(data_frame=penguins, \n                 x=\"Body Mass (g)\", \n                 y=\"Flipper Length (mm)\", \n                 color=\"Species\", \n                 hover_name=\"Species\", \n                 hover_data=[\"Island\",\"Sex\"], \n                 width=700, \n                 height=300)\n#reduce whitespace\nfig.update_layout(margin={\"r\":0, \"t\":0, \"l\":0, \"b\":0})\nfig.show()\n\n\n\n\nIt seems like we can conclude some interesting facts based on this graph. When it comes to the Adelie and Chinstrap Penguins, there is not much of a difference between them in terms of flipper length and body mass. However, the Gentoo penguins on average have a significantly higher body mass and flipper length than the other two species."
  },
  {
    "objectID": "posts/penguins/index.html#penguins-dataset-exploration",
    "href": "posts/penguins/index.html#penguins-dataset-exploration",
    "title": "Plentiful Palmer Penguins",
    "section": "",
    "text": "We will be taking a look at the statistics of the Palmer Penguins Dataset.\nWe will set things up by importing necessary packages and taking a look at the first few rows of the dataset.\n\nimport pandas as pd\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\nurl = \"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\npenguins.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n\n\n\n\n\nThis is great info! Let’s take a look and see the different kinds of species that exist for the penguins.\n\nprint(penguins[\"Species\"].unique())\n\n['Adelie Penguin (Pygoscelis adeliae)'\n 'Chinstrap penguin (Pygoscelis antarctica)'\n 'Gentoo penguin (Pygoscelis papua)']\n\n\nWe are interested in the differences between the three species. We will explore these differences by looking at traits like Culmen Length, Flipper Length, and more! First we will drop rows where these values are NaN, or in other words, not available.\n\npenguins = penguins.dropna(subset=[\"Culmen Length (mm)\", \n                                   \"Culmen Depth (mm)\", \n                                   \"Flipper Length (mm)\", \n                                   \"Body Mass (g)\"])\n\nWe will first take a look at the culmen length and depth of the penguins using plotly. We will set the x axis to be culmen length and the y axis to be culmen depth. To make this even more useful, we’ll color code the points by species so we can see how these statistics differ.\n\nfrom plotly import express as px\nfig = px.scatter(data_frame=penguins, \n                 x=\"Culmen Length (mm)\", \n                 y=\"Culmen Depth (mm)\", \n                 color=\"Species\", \n                 hover_name=\"Species\", \n                 hover_data=[\"Island\",\"Sex\"], \n                 width=700, \n                 height=300)\n#reduce whitespace\nfig.update_layout(margin={\"r\":0, \"t\":0, \"l\":0, \"b\":0})\nfig.show()\n\n\n\n\nWe can see here that culmen length and culmen depth can be pretty good predictors for what species a penguin could be! From this plot we see that Adelie penguins tend to have lower culmen depth than the other two species, whereas Gentoo penguins have lower culmen length than the other two. Chinstrap penguins tend to have both higher culmen depth and length.\nLet’s go ahead and take a look at the other two attributes: Flipper Length and Body Mass. We will do a similar plot and see the distribution and see if there are any interesting facts.\n\nfig = px.scatter(data_frame=penguins, \n                 x=\"Body Mass (g)\", \n                 y=\"Flipper Length (mm)\", \n                 color=\"Species\", \n                 hover_name=\"Species\", \n                 hover_data=[\"Island\",\"Sex\"], \n                 width=700, \n                 height=300)\n#reduce whitespace\nfig.update_layout(margin={\"r\":0, \"t\":0, \"l\":0, \"b\":0})\nfig.show()\n\n\n\n\nIt seems like we can conclude some interesting facts based on this graph. When it comes to the Adelie and Chinstrap Penguins, there is not much of a difference between them in terms of flipper length and body mass. However, the Gentoo penguins on average have a significantly higher body mass and flipper length than the other two species."
  },
  {
    "objectID": "posts/image-classification/index.html",
    "href": "posts/image-classification/index.html",
    "title": "Image Classification: Cats and Dogs!",
    "section": "",
    "text": "Today we will be learning how to create a machine learning model based on a convolutional neural network that will be able to classify and differentiate between cats and dogs.\nFirst we’ll import what we’ll need\n\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras import utils, layers\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n# location of data\n_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n\n# download the data and extract it\npath_to_zip = utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)\n\n# construct paths\nPATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')\n\ntrain_dir = os.path.join(PATH, 'train')\nvalidation_dir = os.path.join(PATH, 'validation')\n\n# parameters for datasets\nBATCH_SIZE = 32\nIMG_SIZE = (160, 160)\n\n# construct train and validation datasets\ntrain_dataset = utils.image_dataset_from_directory(train_dir,\n                                                   shuffle=True,\n                                                   batch_size=BATCH_SIZE,\n                                                   image_size=IMG_SIZE)\n\nvalidation_dataset = utils.image_dataset_from_directory(validation_dir,\n                                                        shuffle=True,\n                                                        batch_size=BATCH_SIZE,\n                                                        image_size=IMG_SIZE)\n\n# construct the test dataset by taking every 5th observation out of the validation dataset\nval_batches = tf.data.experimental.cardinality(validation_dataset)\ntest_dataset = validation_dataset.take(val_batches // 5)\nvalidation_dataset = validation_dataset.skip(val_batches // 5)\n\nFound 2000 files belonging to 2 classes.\nFound 1000 files belonging to 2 classes.\n\n\nWe will use the code below for rapidly reading the data.\n\nAUTOTUNE = tf.data.AUTOTUNE\n\ntrain_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\nvalidation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\ntest_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)\n\n\nfor i, j in train_dataset.take(1):\n    images, labels = i, j\n\n\ncat_indexes = [i for i in range(len(labels)) if labels[i] == 0]\ndog_indexes = [i for i in range(len(labels)) if labels[i] == 1]\n\n\nprint(cat_indexes)\nprint(dog_indexes)\n\n[4, 8, 11, 12, 15, 18, 21, 26, 30, 31]\n[0, 1, 2, 3, 5, 6, 7, 9, 10, 13, 14, 16, 17, 19, 20, 22, 23, 24, 25, 27, 28, 29]\n\n\n\ncat_indexes = np.random.choice(cat_indexes, 3, replace=False)\ndog_indexes = np.random.choice(dog_indexes, 3, replace=False)\n\nLet’s see if we can get the images to display\n\nfor i in range(3):\n    plt.subplot(2,3,i+1)\n    plt.imshow((images[cat_indexes[i]]/255).numpy())\n    plt.xlabel(\"Cat\")\nfor i in range(3):\n    plt.subplot(2,3,i+4)\n    plt.imshow((images[dog_indexes[i]]/255).numpy())\n    plt.xlabel(\"Dog\")\nplt.tight_layout()\n\n\n\n\nNow let’s turn this into a function\n\ndef view_samples(images, labels, num_columns):\n    cat_indexes = [i for i in range(len(labels)) if labels[i] == 0]\n    dog_indexes = [i for i in range(len(labels)) if labels[i] == 1]\n    cat_indexes = np.random.choice(cat_indexes, num_columns, replace=False)\n    dog_indexes = np.random.choice(dog_indexes, num_columns, replace=False)\n    for i in range(num_columns):\n        plt.subplot(2,num_columns,i+1)\n        plt.imshow((images[cat_indexes[i]]/255).numpy())\n        plt.xlabel(\"Cat\")\n    for i in range(num_columns):\n        plt.subplot(2,num_columns,i+1+num_columns)\n        plt.imshow((images[dog_indexes[i]]/255).numpy())\n        plt.xlabel(\"Dog\")\n    plt.tight_layout()\n\n\nview_samples(images, labels, 3)\n\n\n\n\nOur function works! Now let’s check the label frequencies.\n\nlabels_iterator= train_dataset.unbatch().map(lambda image, label: label).as_numpy_iterator()\n\n\ncat_count = 0\ndog_count = 0\nfor label in labels_iterator:\n    if label == 0:\n        cat_count += 1\n    elif label == 1:\n        dog_count += 1\n\n\ncat_count, dog_count\n\n(1000, 1000)\n\n\nWe see that the number of images of cats is the same as that of dogs. This means that the baseline model would just either randomly choose between cats and dogs, or choose one of them as the majority. Either way, we’d get 50% accuracy.\n\nmodel1 = tf.keras.Sequential([\n    layers.Conv2D(32, (3,3), activation=\"relu\", input_shape = (160, 160, 3)),\n    layers.MaxPooling2D((2,2)),\n    layers.Conv2D(64, (3,3), activation=\"relu\"),\n    layers.MaxPooling2D((2,2)),\n    layers.Flatten(),\n    layers.Dense(40, activation = \"relu\"),\n    layers.Dropout(0.2),\n    layers.Dense(2)\n])\n\n\nmodel1.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\n\nmodel1.summary()\n\nModel: \"sequential_21\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d_38 (Conv2D)          (None, 158, 158, 32)      896       \n                                                                 \n max_pooling2d_38 (MaxPooli  (None, 79, 79, 32)        0         \n ng2D)                                                           \n                                                                 \n conv2d_39 (Conv2D)          (None, 77, 77, 64)        18496     \n                                                                 \n max_pooling2d_39 (MaxPooli  (None, 38, 38, 64)        0         \n ng2D)                                                           \n                                                                 \n flatten_19 (Flatten)        (None, 92416)             0         \n                                                                 \n dense_41 (Dense)            (None, 40)                3696680   \n                                                                 \n dropout_11 (Dropout)        (None, 40)                0         \n                                                                 \n dense_42 (Dense)            (None, 2)                 82        \n                                                                 \n=================================================================\nTotal params: 3716154 (14.18 MB)\nTrainable params: 3716154 (14.18 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\nhistory1 = model1.fit(train_dataset, validation_data=validation_dataset, epochs=20)\n\nEpoch 1/20\n63/63 [==============================] - 6s 69ms/step - loss: 45.7836 - accuracy: 0.5740 - val_loss: 0.6681 - val_accuracy: 0.5903\nEpoch 2/20\n63/63 [==============================] - 3s 51ms/step - loss: 0.5724 - accuracy: 0.7025 - val_loss: 0.7661 - val_accuracy: 0.6139\nEpoch 3/20\n63/63 [==============================] - 3s 51ms/step - loss: 0.4076 - accuracy: 0.8125 - val_loss: 0.9572 - val_accuracy: 0.6176\nEpoch 4/20\n63/63 [==============================] - 4s 67ms/step - loss: 0.3079 - accuracy: 0.8725 - val_loss: 1.1746 - val_accuracy: 0.5990\nEpoch 5/20\n63/63 [==============================] - 4s 63ms/step - loss: 0.1961 - accuracy: 0.9245 - val_loss: 1.2200 - val_accuracy: 0.6287\nEpoch 6/20\n63/63 [==============================] - 3s 52ms/step - loss: 0.1460 - accuracy: 0.9480 - val_loss: 1.2713 - val_accuracy: 0.6163\nEpoch 7/20\n63/63 [==============================] - 4s 53ms/step - loss: 0.1249 - accuracy: 0.9605 - val_loss: 1.1999 - val_accuracy: 0.5879\nEpoch 8/20\n63/63 [==============================] - 5s 79ms/step - loss: 0.1208 - accuracy: 0.9540 - val_loss: 1.2705 - val_accuracy: 0.6250\nEpoch 9/20\n63/63 [==============================] - 3s 50ms/step - loss: 0.0891 - accuracy: 0.9740 - val_loss: 1.7460 - val_accuracy: 0.6040\nEpoch 10/20\n63/63 [==============================] - 3s 51ms/step - loss: 0.0682 - accuracy: 0.9765 - val_loss: 1.5511 - val_accuracy: 0.6089\nEpoch 11/20\n63/63 [==============================] - 4s 61ms/step - loss: 0.0941 - accuracy: 0.9675 - val_loss: 1.2757 - val_accuracy: 0.5879\nEpoch 12/20\n63/63 [==============================] - 4s 52ms/step - loss: 0.0685 - accuracy: 0.9785 - val_loss: 2.0207 - val_accuracy: 0.6002\nEpoch 13/20\n63/63 [==============================] - 3s 50ms/step - loss: 0.0771 - accuracy: 0.9770 - val_loss: 1.3295 - val_accuracy: 0.6312\nEpoch 14/20\n63/63 [==============================] - 5s 80ms/step - loss: 0.0585 - accuracy: 0.9815 - val_loss: 1.5487 - val_accuracy: 0.6386\nEpoch 15/20\n63/63 [==============================] - 3s 50ms/step - loss: 0.0478 - accuracy: 0.9870 - val_loss: 1.8362 - val_accuracy: 0.6064\nEpoch 16/20\n63/63 [==============================] - 3s 52ms/step - loss: 0.0360 - accuracy: 0.9880 - val_loss: 1.9736 - val_accuracy: 0.6188\nEpoch 17/20\n63/63 [==============================] - 6s 95ms/step - loss: 0.0627 - accuracy: 0.9855 - val_loss: 1.7215 - val_accuracy: 0.6027\nEpoch 18/20\n63/63 [==============================] - 3s 50ms/step - loss: 0.0526 - accuracy: 0.9830 - val_loss: 1.6026 - val_accuracy: 0.5879\nEpoch 19/20\n63/63 [==============================] - 5s 73ms/step - loss: 0.0526 - accuracy: 0.9850 - val_loss: 2.0568 - val_accuracy: 0.6114\nEpoch 20/20\n63/63 [==============================] - 5s 74ms/step - loss: 0.0400 - accuracy: 0.9855 - val_loss: 1.7724 - val_accuracy: 0.6040\n\n\n\nplt.plot(history1.history[\"accuracy\"], label = \"training accuracy\")\nplt.plot(history1.history[\"val_accuracy\"], label = \"validation accuracy\")\nplt.legend()\nplt.show()\n\n\n\n\nLooking at the graph, we can see that the validation accuracy stabilized at around 60% during training. However, when we look at the training accuracy, it goes all the way up to around 1.0. Because of this large disparity, we see that there’s massive overfitting. Now let’s see if we can fix this by artificially increasing the size of the data. We will do this by using data augmentation layers. To increase our data size, we will perform different alterations to the images by doing this like flipping the pictures, turning them upside down, etc.\nFirst of all, let’s visualize how the RandomFlip layer works. This will randomly choose between flipping over the x and y axis. In this case, we can see that the picture is being flipped vertically.\n\nplt.imshow((images[1]/255).numpy())\nplt.show()\n\n\n\n\n\nfliplayer = layers.RandomFlip()\nflipped_image = fliplayer(images[1])\nplt.imshow((flipped_image/255).numpy())\nplt.show()\n\n\n\n\nNow let’s visualize the RandomRotation. We want to rotate by 90 degrees, so we’ll use a factor of 0.25. This is because in the documentation it says that the factor is multipled by 2π, which would create a range of (-π/2, π/2). We see below that the image got rotated.\n\nrotatelayer = layers.RandomRotation(factor=0.25)\nrotated_image = rotatelayer(images[1])\nplt.imshow((rotated_image/255).numpy())\nplt.show()\n\n\n\n\n\nmodel2 = tf.keras.Sequential([\n    layers.RandomRotation(factor=0.25,input_shape=(160, 160, 3)),\n    layers.RandomFlip(),\n    layers.Conv2D(32, (3,3), activation=\"relu\"),\n    layers.MaxPooling2D((2,2)),\n    layers.Conv2D(64, (3,3), activation=\"relu\"),\n    layers.MaxPooling2D((2,2)),\n    layers.Flatten(),\n    layers.Dense(40, activation = \"relu\"),\n    layers.Dropout(0.2),\n    layers.Dense(2)\n])\n\n\nmodel2.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\n\nmodel2.summary()\n\nModel: \"sequential_12\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n random_rotation_18 (Random  (None, 160, 160, 3)       0         \n Rotation)                                                       \n                                                                 \n random_flip_12 (RandomFlip  (None, 160, 160, 3)       0         \n )                                                               \n                                                                 \n conv2d_20 (Conv2D)          (None, 158, 158, 32)      896       \n                                                                 \n max_pooling2d_20 (MaxPooli  (None, 79, 79, 32)        0         \n ng2D)                                                           \n                                                                 \n conv2d_21 (Conv2D)          (None, 77, 77, 64)        18496     \n                                                                 \n max_pooling2d_21 (MaxPooli  (None, 38, 38, 64)        0         \n ng2D)                                                           \n                                                                 \n flatten_10 (Flatten)        (None, 92416)             0         \n                                                                 \n dense_23 (Dense)            (None, 40)                3696680   \n                                                                 \n dropout_3 (Dropout)         (None, 40)                0         \n                                                                 \n dense_24 (Dense)            (None, 2)                 82        \n                                                                 \n=================================================================\nTotal params: 3716154 (14.18 MB)\nTrainable params: 3716154 (14.18 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\nhistory2 = model2.fit(train_dataset, validation_data=validation_dataset, epochs=20)\n\nEpoch 1/20\n63/63 [==============================] - 5s 51ms/step - loss: 61.7950 - accuracy: 0.4925 - val_loss: 0.6943 - val_accuracy: 0.5062\nEpoch 2/20\n63/63 [==============================] - 6s 88ms/step - loss: 0.6926 - accuracy: 0.5150 - val_loss: 0.6910 - val_accuracy: 0.5891\nEpoch 3/20\n63/63 [==============================] - 4s 52ms/step - loss: 0.6897 - accuracy: 0.5320 - val_loss: 0.6904 - val_accuracy: 0.5866\nEpoch 4/20\n63/63 [==============================] - 3s 51ms/step - loss: 0.6838 - accuracy: 0.5695 - val_loss: 0.6719 - val_accuracy: 0.6077\nEpoch 5/20\n63/63 [==============================] - 5s 77ms/step - loss: 0.6724 - accuracy: 0.5630 - val_loss: 0.6719 - val_accuracy: 0.6015\nEpoch 6/20\n63/63 [==============================] - 4s 51ms/step - loss: 0.6672 - accuracy: 0.5705 - val_loss: 0.6509 - val_accuracy: 0.6287\nEpoch 7/20\n63/63 [==============================] - 4s 52ms/step - loss: 0.6578 - accuracy: 0.5840 - val_loss: 0.6449 - val_accuracy: 0.6361\nEpoch 8/20\n63/63 [==============================] - 5s 72ms/step - loss: 0.6497 - accuracy: 0.6030 - val_loss: 0.6508 - val_accuracy: 0.6547\nEpoch 9/20\n63/63 [==============================] - 4s 51ms/step - loss: 0.6565 - accuracy: 0.5995 - val_loss: 0.6228 - val_accuracy: 0.6287\nEpoch 10/20\n63/63 [==============================] - 3s 52ms/step - loss: 0.6421 - accuracy: 0.6120 - val_loss: 0.6256 - val_accuracy: 0.6436\nEpoch 11/20\n63/63 [==============================] - 6s 90ms/step - loss: 0.6590 - accuracy: 0.6090 - val_loss: 0.6232 - val_accuracy: 0.6386\nEpoch 12/20\n63/63 [==============================] - 3s 52ms/step - loss: 0.6414 - accuracy: 0.6155 - val_loss: 0.6249 - val_accuracy: 0.6262\nEpoch 13/20\n63/63 [==============================] - 3s 52ms/step - loss: 0.6405 - accuracy: 0.6240 - val_loss: 0.6187 - val_accuracy: 0.6448\nEpoch 14/20\n63/63 [==============================] - 5s 80ms/step - loss: 0.6354 - accuracy: 0.6200 - val_loss: 0.6141 - val_accuracy: 0.6584\nEpoch 15/20\n63/63 [==============================] - 6s 90ms/step - loss: 0.6373 - accuracy: 0.6165 - val_loss: 0.6264 - val_accuracy: 0.6238\nEpoch 16/20\n63/63 [==============================] - 4s 63ms/step - loss: 0.6423 - accuracy: 0.6125 - val_loss: 0.6256 - val_accuracy: 0.6411\nEpoch 17/20\n63/63 [==============================] - 3s 51ms/step - loss: 0.6159 - accuracy: 0.6400 - val_loss: 0.6367 - val_accuracy: 0.6423\nEpoch 18/20\n63/63 [==============================] - 3s 52ms/step - loss: 0.6318 - accuracy: 0.6260 - val_loss: 0.6216 - val_accuracy: 0.6411\nEpoch 19/20\n63/63 [==============================] - 6s 93ms/step - loss: 0.6224 - accuracy: 0.6335 - val_loss: 0.6307 - val_accuracy: 0.6324\nEpoch 20/20\n63/63 [==============================] - 3s 51ms/step - loss: 0.6214 - accuracy: 0.6355 - val_loss: 0.6326 - val_accuracy: 0.6572\n\n\n\nplt.plot(history2.history[\"accuracy\"], label = \"training accuracy\")\nplt.plot(history2.history[\"val_accuracy\"], label = \"validation accuracy\")\nplt.legend()\nplt.show()\n\n\n\n\nOur validation accuracy ends up converging to the range of 62.5% - 65%, which is a little bit better than our first model. We can see that the difference between training and validation accuracy is much lower than before, and there is very little overfitting compared to the first model.\n\ni = tf.keras.Input(shape=(160, 160, 3))\nx = tf.keras.applications.mobilenet_v2.preprocess_input(i)\npreprocessor = tf.keras.Model(inputs = [i], outputs = [x])\n\n\nmodel3 = tf.keras.Sequential([\n    preprocessor,\n    layers.RandomRotation(factor=0.25),\n    layers.RandomFlip(),\n    layers.Conv2D(32, (3,3), activation=\"relu\"),\n    layers.MaxPooling2D((2,2)),\n    layers.Conv2D(64, (3,3), activation=\"relu\"),\n    layers.MaxPooling2D((2,2)),\n    layers.Flatten(),\n    layers.Dense(40, activation = \"relu\"),\n    layers.Dropout(0.2),\n    layers.Dense(2)\n])\n\n\nmodel3.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\n\nmodel3.summary()\n\nModel: \"sequential_18\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n model_4 (Functional)        (None, 160, 160, 3)       0         \n                                                                 \n random_rotation_24 (Random  (None, 160, 160, 3)       0         \n Rotation)                                                       \n                                                                 \n random_flip_18 (RandomFlip  (None, 160, 160, 3)       0         \n )                                                               \n                                                                 \n conv2d_32 (Conv2D)          (None, 158, 158, 32)      896       \n                                                                 \n max_pooling2d_32 (MaxPooli  (None, 79, 79, 32)        0         \n ng2D)                                                           \n                                                                 \n conv2d_33 (Conv2D)          (None, 77, 77, 64)        18496     \n                                                                 \n max_pooling2d_33 (MaxPooli  (None, 38, 38, 64)        0         \n ng2D)                                                           \n                                                                 \n flatten_16 (Flatten)        (None, 92416)             0         \n                                                                 \n dense_35 (Dense)            (None, 40)                3696680   \n                                                                 \n dropout_9 (Dropout)         (None, 40)                0         \n                                                                 \n dense_36 (Dense)            (None, 2)                 82        \n                                                                 \n=================================================================\nTotal params: 3716154 (14.18 MB)\nTrainable params: 3716154 (14.18 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\nhistory3 = model3.fit(train_dataset, validation_data=validation_dataset, epochs=20)\n\nEpoch 1/20\n63/63 [==============================] - 5s 58ms/step - loss: 0.8717 - accuracy: 0.5420 - val_loss: 0.6557 - val_accuracy: 0.5965\nEpoch 2/20\n63/63 [==============================] - 4s 57ms/step - loss: 0.6655 - accuracy: 0.5885 - val_loss: 0.6454 - val_accuracy: 0.6052\nEpoch 3/20\n63/63 [==============================] - 5s 74ms/step - loss: 0.6444 - accuracy: 0.5940 - val_loss: 0.6425 - val_accuracy: 0.6077\nEpoch 4/20\n63/63 [==============================] - 3s 52ms/step - loss: 0.6344 - accuracy: 0.6380 - val_loss: 0.6038 - val_accuracy: 0.6547\nEpoch 5/20\n63/63 [==============================] - 4s 53ms/step - loss: 0.6170 - accuracy: 0.6525 - val_loss: 0.5927 - val_accuracy: 0.6856\nEpoch 6/20\n63/63 [==============================] - 5s 70ms/step - loss: 0.6051 - accuracy: 0.6540 - val_loss: 0.6616 - val_accuracy: 0.6621\nEpoch 7/20\n63/63 [==============================] - 4s 52ms/step - loss: 0.6011 - accuracy: 0.6745 - val_loss: 0.6238 - val_accuracy: 0.6658\nEpoch 8/20\n63/63 [==============================] - 3s 52ms/step - loss: 0.6097 - accuracy: 0.6610 - val_loss: 0.6003 - val_accuracy: 0.6559\nEpoch 9/20\n63/63 [==============================] - 4s 55ms/step - loss: 0.5834 - accuracy: 0.6920 - val_loss: 0.5905 - val_accuracy: 0.6807\nEpoch 10/20\n63/63 [==============================] - 4s 53ms/step - loss: 0.5892 - accuracy: 0.6935 - val_loss: 0.5769 - val_accuracy: 0.6906\nEpoch 11/20\n63/63 [==============================] - 4s 53ms/step - loss: 0.5660 - accuracy: 0.6965 - val_loss: 0.5648 - val_accuracy: 0.6931\nEpoch 12/20\n63/63 [==============================] - 3s 53ms/step - loss: 0.5672 - accuracy: 0.7020 - val_loss: 0.5511 - val_accuracy: 0.7017\nEpoch 13/20\n63/63 [==============================] - 4s 60ms/step - loss: 0.5651 - accuracy: 0.7010 - val_loss: 0.5726 - val_accuracy: 0.6980\nEpoch 14/20\n63/63 [==============================] - 3s 51ms/step - loss: 0.5715 - accuracy: 0.7045 - val_loss: 0.5490 - val_accuracy: 0.7129\nEpoch 15/20\n63/63 [==============================] - 6s 89ms/step - loss: 0.5455 - accuracy: 0.7140 - val_loss: 0.6160 - val_accuracy: 0.6931\nEpoch 16/20\n63/63 [==============================] - 3s 51ms/step - loss: 0.5393 - accuracy: 0.7215 - val_loss: 0.5626 - val_accuracy: 0.7141\nEpoch 17/20\n63/63 [==============================] - 3s 51ms/step - loss: 0.5478 - accuracy: 0.7205 - val_loss: 0.5428 - val_accuracy: 0.7401\nEpoch 18/20\n63/63 [==============================] - 6s 94ms/step - loss: 0.5389 - accuracy: 0.7245 - val_loss: 0.5884 - val_accuracy: 0.6918\nEpoch 19/20\n63/63 [==============================] - 4s 53ms/step - loss: 0.5524 - accuracy: 0.7230 - val_loss: 0.5342 - val_accuracy: 0.7203\nEpoch 20/20\n63/63 [==============================] - 5s 74ms/step - loss: 0.5343 - accuracy: 0.7320 - val_loss: 0.5543 - val_accuracy: 0.7178\n\n\n\nplt.plot(history3.history[\"accuracy\"], label = \"training accuracy\")\nplt.plot(history3.history[\"val_accuracy\"], label = \"validation accuracy\")\nplt.legend()\nplt.show()\n\n\n\n\nA much better performance! Looks like the model stabilized at around 70% validation accuracy. There’s even less overfitting in this model than the last one, which is really good.\nWe will now try using transfer learning, which is where we take an existing model and use it for our task. We’ll be using the MobileNetV2 model and use it to train our model. For background the MobileNetV2 model is a convolutional neural network that is 53 layers deep. That’s a lot of layers, especially considering ours is less than 5! This model is trained on more than a million images and can classify many object categories. If we can harness the power of this model, we can surely achieve great results.\n\nIMG_SHAPE = IMG_SIZE + (3,)\nbase_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')\nbase_model.trainable = False\n\ni = tf.keras.Input(shape=IMG_SHAPE)\nx = base_model(i, training = False)\nbase_model_layer = tf.keras.Model(inputs = [i], outputs = [x])\n\n\nmodel4 = tf.keras.Sequential([\n    preprocessor,\n    layers.RandomRotation(factor=0.25),\n    layers.RandomFlip(),\n    base_model_layer,\n    layers.Flatten(),\n    layers.Dropout(0.2),\n    layers.Dense(2)\n])\n\n\nmodel4.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\n\nmodel4.summary()\n\nModel: \"sequential_27\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n model_4 (Functional)        (None, 160, 160, 3)       0         \n                                                                 \n random_rotation_30 (Random  (None, 160, 160, 3)       0         \n Rotation)                                                       \n                                                                 \n random_flip_24 (RandomFlip  (None, 160, 160, 3)       0         \n )                                                               \n                                                                 \n model_5 (Functional)        (None, 5, 5, 1280)        2257984   \n                                                                 \n flatten_21 (Flatten)        (None, 32000)             0         \n                                                                 \n dropout_16 (Dropout)        (None, 32000)             0         \n                                                                 \n dense_49 (Dense)            (None, 2)                 64002     \n                                                                 \n=================================================================\nTotal params: 2321986 (8.86 MB)\nTrainable params: 64002 (250.01 KB)\nNon-trainable params: 2257984 (8.61 MB)\n_________________________________________________________________\n\n\n\nhistory4 = model4.fit(train_dataset, validation_data=(validation_dataset), epochs=20)\n\nEpoch 1/20\n63/63 [==============================] - 9s 96ms/step - loss: 0.8361 - accuracy: 0.8565 - val_loss: 0.0813 - val_accuracy: 0.9814\nEpoch 2/20\n63/63 [==============================] - 4s 58ms/step - loss: 0.6559 - accuracy: 0.9110 - val_loss: 0.5218 - val_accuracy: 0.9431\nEpoch 3/20\n63/63 [==============================] - 4s 58ms/step - loss: 0.6952 - accuracy: 0.9125 - val_loss: 0.2417 - val_accuracy: 0.9678\nEpoch 4/20\n63/63 [==============================] - 6s 90ms/step - loss: 0.7077 - accuracy: 0.9145 - val_loss: 0.0961 - val_accuracy: 0.9802\nEpoch 5/20\n63/63 [==============================] - 5s 74ms/step - loss: 0.6416 - accuracy: 0.9230 - val_loss: 0.1971 - val_accuracy: 0.9715\nEpoch 6/20\n63/63 [==============================] - 4s 57ms/step - loss: 0.7348 - accuracy: 0.9225 - val_loss: 0.1911 - val_accuracy: 0.9666\nEpoch 7/20\n63/63 [==============================] - 6s 86ms/step - loss: 0.6548 - accuracy: 0.9235 - val_loss: 0.2055 - val_accuracy: 0.9616\nEpoch 8/20\n63/63 [==============================] - 4s 57ms/step - loss: 0.5608 - accuracy: 0.9310 - val_loss: 0.3155 - val_accuracy: 0.9666\nEpoch 9/20\n63/63 [==============================] - 4s 59ms/step - loss: 0.6772 - accuracy: 0.9270 - val_loss: 0.3506 - val_accuracy: 0.9604\nEpoch 10/20\n63/63 [==============================] - 5s 67ms/step - loss: 0.5516 - accuracy: 0.9385 - val_loss: 0.3042 - val_accuracy: 0.9666\nEpoch 11/20\n63/63 [==============================] - 5s 81ms/step - loss: 0.6044 - accuracy: 0.9380 - val_loss: 0.3164 - val_accuracy: 0.9765\nEpoch 12/20\n63/63 [==============================] - 4s 56ms/step - loss: 0.5933 - accuracy: 0.9320 - val_loss: 0.2754 - val_accuracy: 0.9629\nEpoch 13/20\n63/63 [==============================] - 4s 58ms/step - loss: 0.5933 - accuracy: 0.9405 - val_loss: 0.3501 - val_accuracy: 0.9554\nEpoch 14/20\n63/63 [==============================] - 5s 78ms/step - loss: 0.6720 - accuracy: 0.9375 - val_loss: 0.2814 - val_accuracy: 0.9715\nEpoch 15/20\n63/63 [==============================] - 4s 59ms/step - loss: 0.5990 - accuracy: 0.9440 - val_loss: 0.4147 - val_accuracy: 0.9678\nEpoch 16/20\n63/63 [==============================] - 4s 57ms/step - loss: 0.5712 - accuracy: 0.9415 - val_loss: 0.4248 - val_accuracy: 0.9715\nEpoch 17/20\n63/63 [==============================] - 5s 80ms/step - loss: 0.6546 - accuracy: 0.9430 - val_loss: 0.5180 - val_accuracy: 0.9653\nEpoch 18/20\n63/63 [==============================] - 4s 58ms/step - loss: 0.5932 - accuracy: 0.9500 - val_loss: 0.3976 - val_accuracy: 0.9629\nEpoch 19/20\n63/63 [==============================] - 4s 66ms/step - loss: 0.6165 - accuracy: 0.9395 - val_loss: 0.4452 - val_accuracy: 0.9554\nEpoch 20/20\n63/63 [==============================] - 5s 79ms/step - loss: 0.6156 - accuracy: 0.9380 - val_loss: 0.3415 - val_accuracy: 0.9728\n\n\n\nplt.plot(history4.history[\"accuracy\"], label = \"training accuracy\")\nplt.plot(history4.history[\"val_accuracy\"], label = \"validation accuracy\")\nplt.legend()\nplt.show()\n\n\n\n\nThis is amazing performance! We can see that the validation accuracy is consistently higher than 95% in the later epochs, and using transfer learning was by far the best method to use. We see that the validation is actually higher than the training accuracy, so there is no evidence of overfitting here which is great news!\nTo truly check how well our best model is doing, we will test this against unseen test data. We will use the evaluate method on the dataset to see how well the transfer learning model did.\n\nmodel4.evaluate(test_dataset)\n\n6/6 [==============================] - 1s 60ms/step - loss: 0.5277 - accuracy: 0.9479\n\n\n[0.5276554226875305, 0.9479166865348816]\n\n\nWe got a 94.8%! That’s a really great score and we should be very happy with the performance of this model."
  },
  {
    "objectID": "posts/image-classification/index.html#image-classification-cats-and-dogs",
    "href": "posts/image-classification/index.html#image-classification-cats-and-dogs",
    "title": "Image Classification: Cats and Dogs!",
    "section": "",
    "text": "Today we will be learning how to create a machine learning model based on a convolutional neural network that will be able to classify and differentiate between cats and dogs.\nFirst we’ll import what we’ll need\n\nimport os\nimport tensorflow as tf\nfrom tensorflow.keras import utils, layers\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n# location of data\n_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n\n# download the data and extract it\npath_to_zip = utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)\n\n# construct paths\nPATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')\n\ntrain_dir = os.path.join(PATH, 'train')\nvalidation_dir = os.path.join(PATH, 'validation')\n\n# parameters for datasets\nBATCH_SIZE = 32\nIMG_SIZE = (160, 160)\n\n# construct train and validation datasets\ntrain_dataset = utils.image_dataset_from_directory(train_dir,\n                                                   shuffle=True,\n                                                   batch_size=BATCH_SIZE,\n                                                   image_size=IMG_SIZE)\n\nvalidation_dataset = utils.image_dataset_from_directory(validation_dir,\n                                                        shuffle=True,\n                                                        batch_size=BATCH_SIZE,\n                                                        image_size=IMG_SIZE)\n\n# construct the test dataset by taking every 5th observation out of the validation dataset\nval_batches = tf.data.experimental.cardinality(validation_dataset)\ntest_dataset = validation_dataset.take(val_batches // 5)\nvalidation_dataset = validation_dataset.skip(val_batches // 5)\n\nFound 2000 files belonging to 2 classes.\nFound 1000 files belonging to 2 classes.\n\n\nWe will use the code below for rapidly reading the data.\n\nAUTOTUNE = tf.data.AUTOTUNE\n\ntrain_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\nvalidation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\ntest_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)\n\n\nfor i, j in train_dataset.take(1):\n    images, labels = i, j\n\n\ncat_indexes = [i for i in range(len(labels)) if labels[i] == 0]\ndog_indexes = [i for i in range(len(labels)) if labels[i] == 1]\n\n\nprint(cat_indexes)\nprint(dog_indexes)\n\n[4, 8, 11, 12, 15, 18, 21, 26, 30, 31]\n[0, 1, 2, 3, 5, 6, 7, 9, 10, 13, 14, 16, 17, 19, 20, 22, 23, 24, 25, 27, 28, 29]\n\n\n\ncat_indexes = np.random.choice(cat_indexes, 3, replace=False)\ndog_indexes = np.random.choice(dog_indexes, 3, replace=False)\n\nLet’s see if we can get the images to display\n\nfor i in range(3):\n    plt.subplot(2,3,i+1)\n    plt.imshow((images[cat_indexes[i]]/255).numpy())\n    plt.xlabel(\"Cat\")\nfor i in range(3):\n    plt.subplot(2,3,i+4)\n    plt.imshow((images[dog_indexes[i]]/255).numpy())\n    plt.xlabel(\"Dog\")\nplt.tight_layout()\n\n\n\n\nNow let’s turn this into a function\n\ndef view_samples(images, labels, num_columns):\n    cat_indexes = [i for i in range(len(labels)) if labels[i] == 0]\n    dog_indexes = [i for i in range(len(labels)) if labels[i] == 1]\n    cat_indexes = np.random.choice(cat_indexes, num_columns, replace=False)\n    dog_indexes = np.random.choice(dog_indexes, num_columns, replace=False)\n    for i in range(num_columns):\n        plt.subplot(2,num_columns,i+1)\n        plt.imshow((images[cat_indexes[i]]/255).numpy())\n        plt.xlabel(\"Cat\")\n    for i in range(num_columns):\n        plt.subplot(2,num_columns,i+1+num_columns)\n        plt.imshow((images[dog_indexes[i]]/255).numpy())\n        plt.xlabel(\"Dog\")\n    plt.tight_layout()\n\n\nview_samples(images, labels, 3)\n\n\n\n\nOur function works! Now let’s check the label frequencies.\n\nlabels_iterator= train_dataset.unbatch().map(lambda image, label: label).as_numpy_iterator()\n\n\ncat_count = 0\ndog_count = 0\nfor label in labels_iterator:\n    if label == 0:\n        cat_count += 1\n    elif label == 1:\n        dog_count += 1\n\n\ncat_count, dog_count\n\n(1000, 1000)\n\n\nWe see that the number of images of cats is the same as that of dogs. This means that the baseline model would just either randomly choose between cats and dogs, or choose one of them as the majority. Either way, we’d get 50% accuracy.\n\nmodel1 = tf.keras.Sequential([\n    layers.Conv2D(32, (3,3), activation=\"relu\", input_shape = (160, 160, 3)),\n    layers.MaxPooling2D((2,2)),\n    layers.Conv2D(64, (3,3), activation=\"relu\"),\n    layers.MaxPooling2D((2,2)),\n    layers.Flatten(),\n    layers.Dense(40, activation = \"relu\"),\n    layers.Dropout(0.2),\n    layers.Dense(2)\n])\n\n\nmodel1.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\n\nmodel1.summary()\n\nModel: \"sequential_21\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d_38 (Conv2D)          (None, 158, 158, 32)      896       \n                                                                 \n max_pooling2d_38 (MaxPooli  (None, 79, 79, 32)        0         \n ng2D)                                                           \n                                                                 \n conv2d_39 (Conv2D)          (None, 77, 77, 64)        18496     \n                                                                 \n max_pooling2d_39 (MaxPooli  (None, 38, 38, 64)        0         \n ng2D)                                                           \n                                                                 \n flatten_19 (Flatten)        (None, 92416)             0         \n                                                                 \n dense_41 (Dense)            (None, 40)                3696680   \n                                                                 \n dropout_11 (Dropout)        (None, 40)                0         \n                                                                 \n dense_42 (Dense)            (None, 2)                 82        \n                                                                 \n=================================================================\nTotal params: 3716154 (14.18 MB)\nTrainable params: 3716154 (14.18 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\nhistory1 = model1.fit(train_dataset, validation_data=validation_dataset, epochs=20)\n\nEpoch 1/20\n63/63 [==============================] - 6s 69ms/step - loss: 45.7836 - accuracy: 0.5740 - val_loss: 0.6681 - val_accuracy: 0.5903\nEpoch 2/20\n63/63 [==============================] - 3s 51ms/step - loss: 0.5724 - accuracy: 0.7025 - val_loss: 0.7661 - val_accuracy: 0.6139\nEpoch 3/20\n63/63 [==============================] - 3s 51ms/step - loss: 0.4076 - accuracy: 0.8125 - val_loss: 0.9572 - val_accuracy: 0.6176\nEpoch 4/20\n63/63 [==============================] - 4s 67ms/step - loss: 0.3079 - accuracy: 0.8725 - val_loss: 1.1746 - val_accuracy: 0.5990\nEpoch 5/20\n63/63 [==============================] - 4s 63ms/step - loss: 0.1961 - accuracy: 0.9245 - val_loss: 1.2200 - val_accuracy: 0.6287\nEpoch 6/20\n63/63 [==============================] - 3s 52ms/step - loss: 0.1460 - accuracy: 0.9480 - val_loss: 1.2713 - val_accuracy: 0.6163\nEpoch 7/20\n63/63 [==============================] - 4s 53ms/step - loss: 0.1249 - accuracy: 0.9605 - val_loss: 1.1999 - val_accuracy: 0.5879\nEpoch 8/20\n63/63 [==============================] - 5s 79ms/step - loss: 0.1208 - accuracy: 0.9540 - val_loss: 1.2705 - val_accuracy: 0.6250\nEpoch 9/20\n63/63 [==============================] - 3s 50ms/step - loss: 0.0891 - accuracy: 0.9740 - val_loss: 1.7460 - val_accuracy: 0.6040\nEpoch 10/20\n63/63 [==============================] - 3s 51ms/step - loss: 0.0682 - accuracy: 0.9765 - val_loss: 1.5511 - val_accuracy: 0.6089\nEpoch 11/20\n63/63 [==============================] - 4s 61ms/step - loss: 0.0941 - accuracy: 0.9675 - val_loss: 1.2757 - val_accuracy: 0.5879\nEpoch 12/20\n63/63 [==============================] - 4s 52ms/step - loss: 0.0685 - accuracy: 0.9785 - val_loss: 2.0207 - val_accuracy: 0.6002\nEpoch 13/20\n63/63 [==============================] - 3s 50ms/step - loss: 0.0771 - accuracy: 0.9770 - val_loss: 1.3295 - val_accuracy: 0.6312\nEpoch 14/20\n63/63 [==============================] - 5s 80ms/step - loss: 0.0585 - accuracy: 0.9815 - val_loss: 1.5487 - val_accuracy: 0.6386\nEpoch 15/20\n63/63 [==============================] - 3s 50ms/step - loss: 0.0478 - accuracy: 0.9870 - val_loss: 1.8362 - val_accuracy: 0.6064\nEpoch 16/20\n63/63 [==============================] - 3s 52ms/step - loss: 0.0360 - accuracy: 0.9880 - val_loss: 1.9736 - val_accuracy: 0.6188\nEpoch 17/20\n63/63 [==============================] - 6s 95ms/step - loss: 0.0627 - accuracy: 0.9855 - val_loss: 1.7215 - val_accuracy: 0.6027\nEpoch 18/20\n63/63 [==============================] - 3s 50ms/step - loss: 0.0526 - accuracy: 0.9830 - val_loss: 1.6026 - val_accuracy: 0.5879\nEpoch 19/20\n63/63 [==============================] - 5s 73ms/step - loss: 0.0526 - accuracy: 0.9850 - val_loss: 2.0568 - val_accuracy: 0.6114\nEpoch 20/20\n63/63 [==============================] - 5s 74ms/step - loss: 0.0400 - accuracy: 0.9855 - val_loss: 1.7724 - val_accuracy: 0.6040\n\n\n\nplt.plot(history1.history[\"accuracy\"], label = \"training accuracy\")\nplt.plot(history1.history[\"val_accuracy\"], label = \"validation accuracy\")\nplt.legend()\nplt.show()\n\n\n\n\nLooking at the graph, we can see that the validation accuracy stabilized at around 60% during training. However, when we look at the training accuracy, it goes all the way up to around 1.0. Because of this large disparity, we see that there’s massive overfitting. Now let’s see if we can fix this by artificially increasing the size of the data. We will do this by using data augmentation layers. To increase our data size, we will perform different alterations to the images by doing this like flipping the pictures, turning them upside down, etc.\nFirst of all, let’s visualize how the RandomFlip layer works. This will randomly choose between flipping over the x and y axis. In this case, we can see that the picture is being flipped vertically.\n\nplt.imshow((images[1]/255).numpy())\nplt.show()\n\n\n\n\n\nfliplayer = layers.RandomFlip()\nflipped_image = fliplayer(images[1])\nplt.imshow((flipped_image/255).numpy())\nplt.show()\n\n\n\n\nNow let’s visualize the RandomRotation. We want to rotate by 90 degrees, so we’ll use a factor of 0.25. This is because in the documentation it says that the factor is multipled by 2π, which would create a range of (-π/2, π/2). We see below that the image got rotated.\n\nrotatelayer = layers.RandomRotation(factor=0.25)\nrotated_image = rotatelayer(images[1])\nplt.imshow((rotated_image/255).numpy())\nplt.show()\n\n\n\n\n\nmodel2 = tf.keras.Sequential([\n    layers.RandomRotation(factor=0.25,input_shape=(160, 160, 3)),\n    layers.RandomFlip(),\n    layers.Conv2D(32, (3,3), activation=\"relu\"),\n    layers.MaxPooling2D((2,2)),\n    layers.Conv2D(64, (3,3), activation=\"relu\"),\n    layers.MaxPooling2D((2,2)),\n    layers.Flatten(),\n    layers.Dense(40, activation = \"relu\"),\n    layers.Dropout(0.2),\n    layers.Dense(2)\n])\n\n\nmodel2.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\n\nmodel2.summary()\n\nModel: \"sequential_12\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n random_rotation_18 (Random  (None, 160, 160, 3)       0         \n Rotation)                                                       \n                                                                 \n random_flip_12 (RandomFlip  (None, 160, 160, 3)       0         \n )                                                               \n                                                                 \n conv2d_20 (Conv2D)          (None, 158, 158, 32)      896       \n                                                                 \n max_pooling2d_20 (MaxPooli  (None, 79, 79, 32)        0         \n ng2D)                                                           \n                                                                 \n conv2d_21 (Conv2D)          (None, 77, 77, 64)        18496     \n                                                                 \n max_pooling2d_21 (MaxPooli  (None, 38, 38, 64)        0         \n ng2D)                                                           \n                                                                 \n flatten_10 (Flatten)        (None, 92416)             0         \n                                                                 \n dense_23 (Dense)            (None, 40)                3696680   \n                                                                 \n dropout_3 (Dropout)         (None, 40)                0         \n                                                                 \n dense_24 (Dense)            (None, 2)                 82        \n                                                                 \n=================================================================\nTotal params: 3716154 (14.18 MB)\nTrainable params: 3716154 (14.18 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\nhistory2 = model2.fit(train_dataset, validation_data=validation_dataset, epochs=20)\n\nEpoch 1/20\n63/63 [==============================] - 5s 51ms/step - loss: 61.7950 - accuracy: 0.4925 - val_loss: 0.6943 - val_accuracy: 0.5062\nEpoch 2/20\n63/63 [==============================] - 6s 88ms/step - loss: 0.6926 - accuracy: 0.5150 - val_loss: 0.6910 - val_accuracy: 0.5891\nEpoch 3/20\n63/63 [==============================] - 4s 52ms/step - loss: 0.6897 - accuracy: 0.5320 - val_loss: 0.6904 - val_accuracy: 0.5866\nEpoch 4/20\n63/63 [==============================] - 3s 51ms/step - loss: 0.6838 - accuracy: 0.5695 - val_loss: 0.6719 - val_accuracy: 0.6077\nEpoch 5/20\n63/63 [==============================] - 5s 77ms/step - loss: 0.6724 - accuracy: 0.5630 - val_loss: 0.6719 - val_accuracy: 0.6015\nEpoch 6/20\n63/63 [==============================] - 4s 51ms/step - loss: 0.6672 - accuracy: 0.5705 - val_loss: 0.6509 - val_accuracy: 0.6287\nEpoch 7/20\n63/63 [==============================] - 4s 52ms/step - loss: 0.6578 - accuracy: 0.5840 - val_loss: 0.6449 - val_accuracy: 0.6361\nEpoch 8/20\n63/63 [==============================] - 5s 72ms/step - loss: 0.6497 - accuracy: 0.6030 - val_loss: 0.6508 - val_accuracy: 0.6547\nEpoch 9/20\n63/63 [==============================] - 4s 51ms/step - loss: 0.6565 - accuracy: 0.5995 - val_loss: 0.6228 - val_accuracy: 0.6287\nEpoch 10/20\n63/63 [==============================] - 3s 52ms/step - loss: 0.6421 - accuracy: 0.6120 - val_loss: 0.6256 - val_accuracy: 0.6436\nEpoch 11/20\n63/63 [==============================] - 6s 90ms/step - loss: 0.6590 - accuracy: 0.6090 - val_loss: 0.6232 - val_accuracy: 0.6386\nEpoch 12/20\n63/63 [==============================] - 3s 52ms/step - loss: 0.6414 - accuracy: 0.6155 - val_loss: 0.6249 - val_accuracy: 0.6262\nEpoch 13/20\n63/63 [==============================] - 3s 52ms/step - loss: 0.6405 - accuracy: 0.6240 - val_loss: 0.6187 - val_accuracy: 0.6448\nEpoch 14/20\n63/63 [==============================] - 5s 80ms/step - loss: 0.6354 - accuracy: 0.6200 - val_loss: 0.6141 - val_accuracy: 0.6584\nEpoch 15/20\n63/63 [==============================] - 6s 90ms/step - loss: 0.6373 - accuracy: 0.6165 - val_loss: 0.6264 - val_accuracy: 0.6238\nEpoch 16/20\n63/63 [==============================] - 4s 63ms/step - loss: 0.6423 - accuracy: 0.6125 - val_loss: 0.6256 - val_accuracy: 0.6411\nEpoch 17/20\n63/63 [==============================] - 3s 51ms/step - loss: 0.6159 - accuracy: 0.6400 - val_loss: 0.6367 - val_accuracy: 0.6423\nEpoch 18/20\n63/63 [==============================] - 3s 52ms/step - loss: 0.6318 - accuracy: 0.6260 - val_loss: 0.6216 - val_accuracy: 0.6411\nEpoch 19/20\n63/63 [==============================] - 6s 93ms/step - loss: 0.6224 - accuracy: 0.6335 - val_loss: 0.6307 - val_accuracy: 0.6324\nEpoch 20/20\n63/63 [==============================] - 3s 51ms/step - loss: 0.6214 - accuracy: 0.6355 - val_loss: 0.6326 - val_accuracy: 0.6572\n\n\n\nplt.plot(history2.history[\"accuracy\"], label = \"training accuracy\")\nplt.plot(history2.history[\"val_accuracy\"], label = \"validation accuracy\")\nplt.legend()\nplt.show()\n\n\n\n\nOur validation accuracy ends up converging to the range of 62.5% - 65%, which is a little bit better than our first model. We can see that the difference between training and validation accuracy is much lower than before, and there is very little overfitting compared to the first model.\n\ni = tf.keras.Input(shape=(160, 160, 3))\nx = tf.keras.applications.mobilenet_v2.preprocess_input(i)\npreprocessor = tf.keras.Model(inputs = [i], outputs = [x])\n\n\nmodel3 = tf.keras.Sequential([\n    preprocessor,\n    layers.RandomRotation(factor=0.25),\n    layers.RandomFlip(),\n    layers.Conv2D(32, (3,3), activation=\"relu\"),\n    layers.MaxPooling2D((2,2)),\n    layers.Conv2D(64, (3,3), activation=\"relu\"),\n    layers.MaxPooling2D((2,2)),\n    layers.Flatten(),\n    layers.Dense(40, activation = \"relu\"),\n    layers.Dropout(0.2),\n    layers.Dense(2)\n])\n\n\nmodel3.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\n\nmodel3.summary()\n\nModel: \"sequential_18\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n model_4 (Functional)        (None, 160, 160, 3)       0         \n                                                                 \n random_rotation_24 (Random  (None, 160, 160, 3)       0         \n Rotation)                                                       \n                                                                 \n random_flip_18 (RandomFlip  (None, 160, 160, 3)       0         \n )                                                               \n                                                                 \n conv2d_32 (Conv2D)          (None, 158, 158, 32)      896       \n                                                                 \n max_pooling2d_32 (MaxPooli  (None, 79, 79, 32)        0         \n ng2D)                                                           \n                                                                 \n conv2d_33 (Conv2D)          (None, 77, 77, 64)        18496     \n                                                                 \n max_pooling2d_33 (MaxPooli  (None, 38, 38, 64)        0         \n ng2D)                                                           \n                                                                 \n flatten_16 (Flatten)        (None, 92416)             0         \n                                                                 \n dense_35 (Dense)            (None, 40)                3696680   \n                                                                 \n dropout_9 (Dropout)         (None, 40)                0         \n                                                                 \n dense_36 (Dense)            (None, 2)                 82        \n                                                                 \n=================================================================\nTotal params: 3716154 (14.18 MB)\nTrainable params: 3716154 (14.18 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\nhistory3 = model3.fit(train_dataset, validation_data=validation_dataset, epochs=20)\n\nEpoch 1/20\n63/63 [==============================] - 5s 58ms/step - loss: 0.8717 - accuracy: 0.5420 - val_loss: 0.6557 - val_accuracy: 0.5965\nEpoch 2/20\n63/63 [==============================] - 4s 57ms/step - loss: 0.6655 - accuracy: 0.5885 - val_loss: 0.6454 - val_accuracy: 0.6052\nEpoch 3/20\n63/63 [==============================] - 5s 74ms/step - loss: 0.6444 - accuracy: 0.5940 - val_loss: 0.6425 - val_accuracy: 0.6077\nEpoch 4/20\n63/63 [==============================] - 3s 52ms/step - loss: 0.6344 - accuracy: 0.6380 - val_loss: 0.6038 - val_accuracy: 0.6547\nEpoch 5/20\n63/63 [==============================] - 4s 53ms/step - loss: 0.6170 - accuracy: 0.6525 - val_loss: 0.5927 - val_accuracy: 0.6856\nEpoch 6/20\n63/63 [==============================] - 5s 70ms/step - loss: 0.6051 - accuracy: 0.6540 - val_loss: 0.6616 - val_accuracy: 0.6621\nEpoch 7/20\n63/63 [==============================] - 4s 52ms/step - loss: 0.6011 - accuracy: 0.6745 - val_loss: 0.6238 - val_accuracy: 0.6658\nEpoch 8/20\n63/63 [==============================] - 3s 52ms/step - loss: 0.6097 - accuracy: 0.6610 - val_loss: 0.6003 - val_accuracy: 0.6559\nEpoch 9/20\n63/63 [==============================] - 4s 55ms/step - loss: 0.5834 - accuracy: 0.6920 - val_loss: 0.5905 - val_accuracy: 0.6807\nEpoch 10/20\n63/63 [==============================] - 4s 53ms/step - loss: 0.5892 - accuracy: 0.6935 - val_loss: 0.5769 - val_accuracy: 0.6906\nEpoch 11/20\n63/63 [==============================] - 4s 53ms/step - loss: 0.5660 - accuracy: 0.6965 - val_loss: 0.5648 - val_accuracy: 0.6931\nEpoch 12/20\n63/63 [==============================] - 3s 53ms/step - loss: 0.5672 - accuracy: 0.7020 - val_loss: 0.5511 - val_accuracy: 0.7017\nEpoch 13/20\n63/63 [==============================] - 4s 60ms/step - loss: 0.5651 - accuracy: 0.7010 - val_loss: 0.5726 - val_accuracy: 0.6980\nEpoch 14/20\n63/63 [==============================] - 3s 51ms/step - loss: 0.5715 - accuracy: 0.7045 - val_loss: 0.5490 - val_accuracy: 0.7129\nEpoch 15/20\n63/63 [==============================] - 6s 89ms/step - loss: 0.5455 - accuracy: 0.7140 - val_loss: 0.6160 - val_accuracy: 0.6931\nEpoch 16/20\n63/63 [==============================] - 3s 51ms/step - loss: 0.5393 - accuracy: 0.7215 - val_loss: 0.5626 - val_accuracy: 0.7141\nEpoch 17/20\n63/63 [==============================] - 3s 51ms/step - loss: 0.5478 - accuracy: 0.7205 - val_loss: 0.5428 - val_accuracy: 0.7401\nEpoch 18/20\n63/63 [==============================] - 6s 94ms/step - loss: 0.5389 - accuracy: 0.7245 - val_loss: 0.5884 - val_accuracy: 0.6918\nEpoch 19/20\n63/63 [==============================] - 4s 53ms/step - loss: 0.5524 - accuracy: 0.7230 - val_loss: 0.5342 - val_accuracy: 0.7203\nEpoch 20/20\n63/63 [==============================] - 5s 74ms/step - loss: 0.5343 - accuracy: 0.7320 - val_loss: 0.5543 - val_accuracy: 0.7178\n\n\n\nplt.plot(history3.history[\"accuracy\"], label = \"training accuracy\")\nplt.plot(history3.history[\"val_accuracy\"], label = \"validation accuracy\")\nplt.legend()\nplt.show()\n\n\n\n\nA much better performance! Looks like the model stabilized at around 70% validation accuracy. There’s even less overfitting in this model than the last one, which is really good.\nWe will now try using transfer learning, which is where we take an existing model and use it for our task. We’ll be using the MobileNetV2 model and use it to train our model. For background the MobileNetV2 model is a convolutional neural network that is 53 layers deep. That’s a lot of layers, especially considering ours is less than 5! This model is trained on more than a million images and can classify many object categories. If we can harness the power of this model, we can surely achieve great results.\n\nIMG_SHAPE = IMG_SIZE + (3,)\nbase_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')\nbase_model.trainable = False\n\ni = tf.keras.Input(shape=IMG_SHAPE)\nx = base_model(i, training = False)\nbase_model_layer = tf.keras.Model(inputs = [i], outputs = [x])\n\n\nmodel4 = tf.keras.Sequential([\n    preprocessor,\n    layers.RandomRotation(factor=0.25),\n    layers.RandomFlip(),\n    base_model_layer,\n    layers.Flatten(),\n    layers.Dropout(0.2),\n    layers.Dense(2)\n])\n\n\nmodel4.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\n\nmodel4.summary()\n\nModel: \"sequential_27\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n model_4 (Functional)        (None, 160, 160, 3)       0         \n                                                                 \n random_rotation_30 (Random  (None, 160, 160, 3)       0         \n Rotation)                                                       \n                                                                 \n random_flip_24 (RandomFlip  (None, 160, 160, 3)       0         \n )                                                               \n                                                                 \n model_5 (Functional)        (None, 5, 5, 1280)        2257984   \n                                                                 \n flatten_21 (Flatten)        (None, 32000)             0         \n                                                                 \n dropout_16 (Dropout)        (None, 32000)             0         \n                                                                 \n dense_49 (Dense)            (None, 2)                 64002     \n                                                                 \n=================================================================\nTotal params: 2321986 (8.86 MB)\nTrainable params: 64002 (250.01 KB)\nNon-trainable params: 2257984 (8.61 MB)\n_________________________________________________________________\n\n\n\nhistory4 = model4.fit(train_dataset, validation_data=(validation_dataset), epochs=20)\n\nEpoch 1/20\n63/63 [==============================] - 9s 96ms/step - loss: 0.8361 - accuracy: 0.8565 - val_loss: 0.0813 - val_accuracy: 0.9814\nEpoch 2/20\n63/63 [==============================] - 4s 58ms/step - loss: 0.6559 - accuracy: 0.9110 - val_loss: 0.5218 - val_accuracy: 0.9431\nEpoch 3/20\n63/63 [==============================] - 4s 58ms/step - loss: 0.6952 - accuracy: 0.9125 - val_loss: 0.2417 - val_accuracy: 0.9678\nEpoch 4/20\n63/63 [==============================] - 6s 90ms/step - loss: 0.7077 - accuracy: 0.9145 - val_loss: 0.0961 - val_accuracy: 0.9802\nEpoch 5/20\n63/63 [==============================] - 5s 74ms/step - loss: 0.6416 - accuracy: 0.9230 - val_loss: 0.1971 - val_accuracy: 0.9715\nEpoch 6/20\n63/63 [==============================] - 4s 57ms/step - loss: 0.7348 - accuracy: 0.9225 - val_loss: 0.1911 - val_accuracy: 0.9666\nEpoch 7/20\n63/63 [==============================] - 6s 86ms/step - loss: 0.6548 - accuracy: 0.9235 - val_loss: 0.2055 - val_accuracy: 0.9616\nEpoch 8/20\n63/63 [==============================] - 4s 57ms/step - loss: 0.5608 - accuracy: 0.9310 - val_loss: 0.3155 - val_accuracy: 0.9666\nEpoch 9/20\n63/63 [==============================] - 4s 59ms/step - loss: 0.6772 - accuracy: 0.9270 - val_loss: 0.3506 - val_accuracy: 0.9604\nEpoch 10/20\n63/63 [==============================] - 5s 67ms/step - loss: 0.5516 - accuracy: 0.9385 - val_loss: 0.3042 - val_accuracy: 0.9666\nEpoch 11/20\n63/63 [==============================] - 5s 81ms/step - loss: 0.6044 - accuracy: 0.9380 - val_loss: 0.3164 - val_accuracy: 0.9765\nEpoch 12/20\n63/63 [==============================] - 4s 56ms/step - loss: 0.5933 - accuracy: 0.9320 - val_loss: 0.2754 - val_accuracy: 0.9629\nEpoch 13/20\n63/63 [==============================] - 4s 58ms/step - loss: 0.5933 - accuracy: 0.9405 - val_loss: 0.3501 - val_accuracy: 0.9554\nEpoch 14/20\n63/63 [==============================] - 5s 78ms/step - loss: 0.6720 - accuracy: 0.9375 - val_loss: 0.2814 - val_accuracy: 0.9715\nEpoch 15/20\n63/63 [==============================] - 4s 59ms/step - loss: 0.5990 - accuracy: 0.9440 - val_loss: 0.4147 - val_accuracy: 0.9678\nEpoch 16/20\n63/63 [==============================] - 4s 57ms/step - loss: 0.5712 - accuracy: 0.9415 - val_loss: 0.4248 - val_accuracy: 0.9715\nEpoch 17/20\n63/63 [==============================] - 5s 80ms/step - loss: 0.6546 - accuracy: 0.9430 - val_loss: 0.5180 - val_accuracy: 0.9653\nEpoch 18/20\n63/63 [==============================] - 4s 58ms/step - loss: 0.5932 - accuracy: 0.9500 - val_loss: 0.3976 - val_accuracy: 0.9629\nEpoch 19/20\n63/63 [==============================] - 4s 66ms/step - loss: 0.6165 - accuracy: 0.9395 - val_loss: 0.4452 - val_accuracy: 0.9554\nEpoch 20/20\n63/63 [==============================] - 5s 79ms/step - loss: 0.6156 - accuracy: 0.9380 - val_loss: 0.3415 - val_accuracy: 0.9728\n\n\n\nplt.plot(history4.history[\"accuracy\"], label = \"training accuracy\")\nplt.plot(history4.history[\"val_accuracy\"], label = \"validation accuracy\")\nplt.legend()\nplt.show()\n\n\n\n\nThis is amazing performance! We can see that the validation accuracy is consistently higher than 95% in the later epochs, and using transfer learning was by far the best method to use. We see that the validation is actually higher than the training accuracy, so there is no evidence of overfitting here which is great news!\nTo truly check how well our best model is doing, we will test this against unseen test data. We will use the evaluate method on the dataset to see how well the transfer learning model did.\n\nmodel4.evaluate(test_dataset)\n\n6/6 [==============================] - 1s 60ms/step - loss: 0.5277 - accuracy: 0.9479\n\n\n[0.5276554226875305, 0.9479166865348816]\n\n\nWe got a 94.8%! That’s a really great score and we should be very happy with the performance of this model."
  },
  {
    "objectID": "posts/climate/climate.html",
    "href": "posts/climate/climate.html",
    "title": "NOAA Climate Data Analysis",
    "section": "",
    "text": "Let’s study how extreme climate change is by using databases and Pandas!\nWe will start by importing all of the necessary packages we’ll need.\n\nimport pandas as pd\nimport sqlite3\nimport plotly.io as pio\nfrom plotly import express as px\npio.renderers.default=\"iframe\"\n\nWe will be creating a database with three tables: temperatures, stations, and countries. We first will transform the temperature data to make it more clean.\n\ndef prepare_df(df):\n    df = df.set_index(keys=[\"ID\", \"Year\"])\n    df = df.stack()\n    df = df.reset_index()\n    df = df.rename(columns = {\"level_2\": \"Month\", 0: \"Temp\"})\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"] = df[\"Temp\"]/100\n    df[\"FIPS 10-4\"] = df[\"ID\"].str[:2]\n    return df\n\nWe will now create a database with the filename climate.db\n\nconn = sqlite3.connect(\"climate.db\") #creates database in curr directory called climate.db\n\nAnd now we have a file in our current directory called climate.db! We will now use the iterator ability of pandas to write the dataframe into the database\n\ndf_iter = pd.read_csv(\"temps.csv\", chunksize=100000)\nfor df in df_iter:\n    df = prepare_df(df)\n    df.to_sql(\"temperatures\", conn, if_exists=\"append\", index=False)\n\nNow we will add tables for the stations and countries. Since these aren’t nearly as large, we won’t be reading them in by chunks.\n\nstations = pd.read_csv(\"station-metadata.csv\")\nstations.to_sql(\"stations\", conn, if_exists=\"replace\", index=False)\n\n27585\n\n\n\ncountries = pd.read_csv(\"countries.csv\")\ncountries.to_sql(\"countries\", conn, if_exists=\"replace\", index=False)\n\n279\n\n\nNow that we’ve entered these as tables into our databases, we’ll look at the data and figure out how to query a climate database.\n\ndf.head() #part of the temperatures data\n\n\n\n\n\n\n\n\nID\nYear\nMonth\nTemp\nFIPS 10-4\n\n\n\n\n0\nUSW00014924\n2016\n1\n-13.69\nUS\n\n\n1\nUSW00014924\n2016\n2\n-8.40\nUS\n\n\n2\nUSW00014924\n2016\n3\n-0.20\nUS\n\n\n3\nUSW00014924\n2016\n4\n3.21\nUS\n\n\n4\nUSW00014924\n2016\n5\n13.85\nUS\n\n\n\n\n\n\n\n\nstations.head()\n\n\n\n\n\n\n\n\nID\nLATITUDE\nLONGITUDE\nSTNELEV\nNAME\n\n\n\n\n0\nACW00011604\n57.7667\n11.8667\n18.0\nSAVE\n\n\n1\nAE000041196\n25.3330\n55.5170\n34.0\nSHARJAH_INTER_AIRP\n\n\n2\nAEM00041184\n25.6170\n55.9330\n31.0\nRAS_AL_KHAIMAH_INTE\n\n\n3\nAEM00041194\n25.2550\n55.3640\n10.4\nDUBAI_INTL\n\n\n4\nAEM00041216\n24.4300\n54.4700\n3.0\nABU_DHABI_BATEEN_AIR\n\n\n\n\n\n\n\n\ncountries.head()\n\n\n\n\n\n\n\n\nFIPS 10-4\nISO 3166\nName\n\n\n\n\n0\nAF\nAF\nAfghanistan\n\n\n1\nAX\n-\nAkrotiri\n\n\n2\nAL\nAL\nAlbania\n\n\n3\nAG\nDZ\nAlgeria\n\n\n4\nAQ\nAS\nAmerican Samoa\n\n\n\n\n\n\n\nLooks good! We will now set up our cursor for doing SQL commands from the tables, and check that we named them correctly\n\ncursor = conn.cursor()\ncursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\nprint(cursor.fetchall())\n\n[('temperatures',), ('stations',), ('countries',)]\n\n\nNow we will make sure that we correctly populated our tables.\n\ncursor.execute(\"SELECT sql FROM sqlite_master WHERE type='table';\")\nfor result in cursor.fetchall():\n    print(result[0])\n\nCREATE TABLE \"temperatures\" (\n\"ID\" TEXT,\n  \"Year\" INTEGER,\n  \"Month\" INTEGER,\n  \"Temp\" REAL,\n  \"FIPS 10-4\" TEXT\n)\nCREATE TABLE \"stations\" (\n\"ID\" TEXT,\n  \"LATITUDE\" REAL,\n  \"LONGITUDE\" REAL,\n  \"STNELEV\" REAL,\n  \"NAME\" TEXT\n)\nCREATE TABLE \"countries\" (\n\"FIPS 10-4\" TEXT,\n  \"ISO 3166\" TEXT,\n  \"Name\" TEXT\n)\n\n\nWe will test out if we can reproduce the sample output provided in the directions of the homework.\n\ncmd = \\\n\"\"\"\nSELECT S.name, S.latitude, S.longitude, T.year, T.month, T.temp\nFROM temperatures T\nLEFT JOIN stations S ON T.id = S.id\nLEFT JOIN countries C ON T.\"FIPS 10-4\" = C.\"FIPS 10-4\"\nWHERE T.year &gt;= 1980 AND T.year &lt;= 2020 AND T.month = 1 AND C.name = \"India\";\n\"\"\"\ndf = pd.read_sql_query(cmd, conn)\ndf\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\n1980\n1\n23.48\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\n1981\n1\n24.57\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\n1982\n1\n24.19\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\n1983\n1\n23.51\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\n1984\n1\n24.81\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nDARJEELING\n27.050\n88.270\n1983\n1\n5.10\n\n\n3148\nDARJEELING\n27.050\n88.270\n1986\n1\n6.90\n\n\n3149\nDARJEELING\n27.050\n88.270\n1994\n1\n8.10\n\n\n3150\nDARJEELING\n27.050\n88.270\n1995\n1\n5.60\n\n\n3151\nDARJEELING\n27.050\n88.270\n1997\n1\n5.70\n\n\n\n\n3152 rows × 6 columns\n\n\n\nWe did it! We can now proceed and create the general function for querying based on the user inputs.\n\ndef query_climate_database(country, year_begin, year_end, month):\n    cmd = \\\n    f\"\"\"\n    SELECT S.name, S.latitude, S.longitude, T.year, T.month, T.temp\n    FROM temperatures T\n    LEFT JOIN stations S ON T.id = S.id\n    LEFT JOIN countries C ON T.\"FIPS 10-4\" = C.\"FIPS 10-4\"\n    WHERE T.year &gt;= {year_begin} AND T.year &lt;= {year_end} AND T.month = {month} \n    AND C.name = \"{country}\";\n    \"\"\"\n    df = pd.read_sql_query(cmd, conn)\n    return df\n\n\nquery_climate_database(country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\n1980\n1\n23.48\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\n1981\n1\n24.57\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\n1982\n1\n24.19\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\n1983\n1\n23.51\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\n1984\n1\n24.81\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nDARJEELING\n27.050\n88.270\n1983\n1\n5.10\n\n\n3148\nDARJEELING\n27.050\n88.270\n1986\n1\n6.90\n\n\n3149\nDARJEELING\n27.050\n88.270\n1994\n1\n8.10\n\n\n3150\nDARJEELING\n27.050\n88.270\n1995\n1\n5.60\n\n\n3151\nDARJEELING\n27.050\n88.270\n1997\n1\n5.70\n\n\n\n\n3152 rows × 6 columns\n\n\n\nWe’ve done it! Now let’s test this for a different example just to make sure this is working.\n\nquery_climate_database(country = \"United States\", \n                       year_begin = 2000, \n                       year_end = 2010,\n                       month = 12)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nYear\nMonth\nTemp\n\n\n\n\n0\nADDISON\n34.2553\n-87.1814\n2001\n12\n8.43\n\n\n1\nADDISON\n34.2553\n-87.1814\n2002\n12\n4.73\n\n\n2\nADDISON\n34.2553\n-87.1814\n2005\n12\n3.65\n\n\n3\nADDISON\n34.2553\n-87.1814\n2009\n12\n5.33\n\n\n4\nADDISON\n34.2553\n-87.1814\n2010\n12\n2.00\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n75667\nLINCOLN_11_SW\n40.6953\n-96.8542\n2006\n12\n1.45\n\n\n75668\nLINCOLN_11_SW\n40.6953\n-96.8542\n2007\n12\n-4.18\n\n\n75669\nLINCOLN_11_SW\n40.6953\n-96.8542\n2008\n12\n-4.88\n\n\n75670\nLINCOLN_11_SW\n40.6953\n-96.8542\n2009\n12\n-6.97\n\n\n75671\nLINCOLN_11_SW\n40.6953\n-96.8542\n2010\n12\n-3.46\n\n\n\n\n75672 rows × 6 columns\n\n\n\n\ndf = query_climate_database(country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)\ndf\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\n1980\n1\n23.48\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\n1981\n1\n24.57\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\n1982\n1\n24.19\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\n1983\n1\n23.51\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\n1984\n1\n24.81\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nDARJEELING\n27.050\n88.270\n1983\n1\n5.10\n\n\n3148\nDARJEELING\n27.050\n88.270\n1986\n1\n6.90\n\n\n3149\nDARJEELING\n27.050\n88.270\n1994\n1\n8.10\n\n\n3150\nDARJEELING\n27.050\n88.270\n1995\n1\n5.60\n\n\n3151\nDARJEELING\n27.050\n88.270\n1997\n1\n5.70\n\n\n\n\n3152 rows × 6 columns\n\n\n\n\ndf[\"year_count\"] = df.groupby(\"NAME\")[\"Year\"].transform(\"count\")\ndf\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nYear\nMonth\nTemp\nyear_count\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\n1980\n1\n23.48\n34\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\n1981\n1\n24.57\n34\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\n1982\n1\n24.19\n34\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\n1983\n1\n23.51\n34\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\n1984\n1\n24.81\n34\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nDARJEELING\n27.050\n88.270\n1983\n1\n5.10\n7\n\n\n3148\nDARJEELING\n27.050\n88.270\n1986\n1\n6.90\n7\n\n\n3149\nDARJEELING\n27.050\n88.270\n1994\n1\n8.10\n7\n\n\n3150\nDARJEELING\n27.050\n88.270\n1995\n1\n5.60\n7\n\n\n3151\nDARJEELING\n27.050\n88.270\n1997\n1\n5.70\n7\n\n\n\n\n3152 rows × 7 columns\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\n\ndef coef(data_group):\n    x = data_group[[\"Year\"]] # 2 brackets because X should be a df\n    y = data_group[\"Temp\"]   # 1 bracket because y should be a series\n    LR = LinearRegression()\n    LR.fit(x, y)\n    return LR.coef_[0]\n\n\ncoefs = df.groupby([\"NAME\", \"Month\"]).apply(coef)\ncoefs = coefs.reset_index()\ncoefs\n\n\n\n\n\n\n\n\nNAME\nMonth\n0\n\n\n\n\n0\nAGARTALA\n1\n-0.006184\n\n\n1\nAGRA\n1\n-0.095413\n\n\n2\nAHMADABAD\n1\n0.006731\n\n\n3\nAKOLA\n1\n-0.008063\n\n\n4\nALLAHABAD\n1\n-0.029375\n\n\n...\n...\n...\n...\n\n\n99\nTRIVANDRUM\n1\n0.022892\n\n\n100\nUDAIPUR_DABOK\n1\n0.072424\n\n\n101\nVARANASI_BABATPUR\n1\n-0.012996\n\n\n102\nVERAVAL\n1\n0.024848\n\n\n103\nVISHAKHAPATNAM\n1\n-0.034050\n\n\n\n\n104 rows × 3 columns\n\n\n\n\npd.merge(df, coefs, on=\"NAME\")\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nYear\nMonth_x\nTemp\nyear_count\nMonth_y\n0\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\n1980\n1\n23.48\n34\n1\n0.026258\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\n1981\n1\n24.57\n34\n1\n0.026258\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\n1982\n1\n24.19\n34\n1\n0.026258\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\n1983\n1\n23.51\n34\n1\n0.026258\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\n1984\n1\n24.81\n34\n1\n0.026258\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nDARJEELING\n27.050\n88.270\n1983\n1\n5.10\n7\n1\n-0.040133\n\n\n3148\nDARJEELING\n27.050\n88.270\n1986\n1\n6.90\n7\n1\n-0.040133\n\n\n3149\nDARJEELING\n27.050\n88.270\n1994\n1\n8.10\n7\n1\n-0.040133\n\n\n3150\nDARJEELING\n27.050\n88.270\n1995\n1\n5.60\n7\n1\n-0.040133\n\n\n3151\nDARJEELING\n27.050\n88.270\n1997\n1\n5.70\n7\n1\n-0.040133\n\n\n\n\n3152 rows × 9 columns\n\n\n\nWe did it! We will now create a function that deals with plotting how the average yearly changes in temperature vary within a given country.\n\ndef temperature_coefficient_plot(country, year_begin, year_end, month, min_obs, **kwargs):\n    df = query_climate_database(country = country, \n                       year_begin = year_begin, \n                       year_end = year_end,\n                       month = month)\n    df[\"YearCount\"] = df.groupby(\"NAME\")[\"Year\"].transform(\"count\")\n    df = df[(df[\"YearCount\"] &gt;= min_obs)]\n    coefs = df.groupby([\"NAME\", \"Month\"]).apply(coef)\n    coefs = coefs.reset_index()\n    coefs = coefs.rename(columns={0: \"Estimated Yearly Increase (°C)\"})\n    df = pd.merge(df, coefs, on=\"NAME\")\n    df = df.round(4)\n    fig = px.scatter_mapbox(data_frame=df,\n                            lat=\"LATITUDE\",\n                            lon=\"LONGITUDE\",\n                            hover_name=\"NAME\",\n                            color=\"Estimated Yearly Increase (°C)\",\n                            **kwargs\n                            )\n    return fig\n\n\ncolor_map = px.colors.diverging.RdGy_r\nfig = temperature_coefficient_plot(\"India\", 1980, 2020, 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map\n                                   )\nfig.show()\n\n\n\n\nThis matches the plot that we were expecting to get! We can now see each station and look at the estimated yearly increase for each of them! Now let’s do another example, but for a different country.\n\nfig = temperature_coefficient_plot(\"United States\", 1960, 2010, 6, \n                                   min_obs = 12,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map\n                                   )\nfig.show()\n\n\n\n\nThe plot looks great! It seems that the estimated yearly increase has a much higher range than that of the previous plot."
  },
  {
    "objectID": "posts/climate/climate.html#noaa-climate-data",
    "href": "posts/climate/climate.html#noaa-climate-data",
    "title": "NOAA Climate Data Analysis",
    "section": "",
    "text": "Let’s study how extreme climate change is by using databases and Pandas!\nWe will start by importing all of the necessary packages we’ll need.\n\nimport pandas as pd\nimport sqlite3\nimport plotly.io as pio\nfrom plotly import express as px\npio.renderers.default=\"iframe\"\n\nWe will be creating a database with three tables: temperatures, stations, and countries. We first will transform the temperature data to make it more clean.\n\ndef prepare_df(df):\n    df = df.set_index(keys=[\"ID\", \"Year\"])\n    df = df.stack()\n    df = df.reset_index()\n    df = df.rename(columns = {\"level_2\": \"Month\", 0: \"Temp\"})\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"] = df[\"Temp\"]/100\n    df[\"FIPS 10-4\"] = df[\"ID\"].str[:2]\n    return df\n\nWe will now create a database with the filename climate.db\n\nconn = sqlite3.connect(\"climate.db\") #creates database in curr directory called climate.db\n\nAnd now we have a file in our current directory called climate.db! We will now use the iterator ability of pandas to write the dataframe into the database\n\ndf_iter = pd.read_csv(\"temps.csv\", chunksize=100000)\nfor df in df_iter:\n    df = prepare_df(df)\n    df.to_sql(\"temperatures\", conn, if_exists=\"append\", index=False)\n\nNow we will add tables for the stations and countries. Since these aren’t nearly as large, we won’t be reading them in by chunks.\n\nstations = pd.read_csv(\"station-metadata.csv\")\nstations.to_sql(\"stations\", conn, if_exists=\"replace\", index=False)\n\n27585\n\n\n\ncountries = pd.read_csv(\"countries.csv\")\ncountries.to_sql(\"countries\", conn, if_exists=\"replace\", index=False)\n\n279\n\n\nNow that we’ve entered these as tables into our databases, we’ll look at the data and figure out how to query a climate database.\n\ndf.head() #part of the temperatures data\n\n\n\n\n\n\n\n\nID\nYear\nMonth\nTemp\nFIPS 10-4\n\n\n\n\n0\nUSW00014924\n2016\n1\n-13.69\nUS\n\n\n1\nUSW00014924\n2016\n2\n-8.40\nUS\n\n\n2\nUSW00014924\n2016\n3\n-0.20\nUS\n\n\n3\nUSW00014924\n2016\n4\n3.21\nUS\n\n\n4\nUSW00014924\n2016\n5\n13.85\nUS\n\n\n\n\n\n\n\n\nstations.head()\n\n\n\n\n\n\n\n\nID\nLATITUDE\nLONGITUDE\nSTNELEV\nNAME\n\n\n\n\n0\nACW00011604\n57.7667\n11.8667\n18.0\nSAVE\n\n\n1\nAE000041196\n25.3330\n55.5170\n34.0\nSHARJAH_INTER_AIRP\n\n\n2\nAEM00041184\n25.6170\n55.9330\n31.0\nRAS_AL_KHAIMAH_INTE\n\n\n3\nAEM00041194\n25.2550\n55.3640\n10.4\nDUBAI_INTL\n\n\n4\nAEM00041216\n24.4300\n54.4700\n3.0\nABU_DHABI_BATEEN_AIR\n\n\n\n\n\n\n\n\ncountries.head()\n\n\n\n\n\n\n\n\nFIPS 10-4\nISO 3166\nName\n\n\n\n\n0\nAF\nAF\nAfghanistan\n\n\n1\nAX\n-\nAkrotiri\n\n\n2\nAL\nAL\nAlbania\n\n\n3\nAG\nDZ\nAlgeria\n\n\n4\nAQ\nAS\nAmerican Samoa\n\n\n\n\n\n\n\nLooks good! We will now set up our cursor for doing SQL commands from the tables, and check that we named them correctly\n\ncursor = conn.cursor()\ncursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\nprint(cursor.fetchall())\n\n[('temperatures',), ('stations',), ('countries',)]\n\n\nNow we will make sure that we correctly populated our tables.\n\ncursor.execute(\"SELECT sql FROM sqlite_master WHERE type='table';\")\nfor result in cursor.fetchall():\n    print(result[0])\n\nCREATE TABLE \"temperatures\" (\n\"ID\" TEXT,\n  \"Year\" INTEGER,\n  \"Month\" INTEGER,\n  \"Temp\" REAL,\n  \"FIPS 10-4\" TEXT\n)\nCREATE TABLE \"stations\" (\n\"ID\" TEXT,\n  \"LATITUDE\" REAL,\n  \"LONGITUDE\" REAL,\n  \"STNELEV\" REAL,\n  \"NAME\" TEXT\n)\nCREATE TABLE \"countries\" (\n\"FIPS 10-4\" TEXT,\n  \"ISO 3166\" TEXT,\n  \"Name\" TEXT\n)\n\n\nWe will test out if we can reproduce the sample output provided in the directions of the homework.\n\ncmd = \\\n\"\"\"\nSELECT S.name, S.latitude, S.longitude, T.year, T.month, T.temp\nFROM temperatures T\nLEFT JOIN stations S ON T.id = S.id\nLEFT JOIN countries C ON T.\"FIPS 10-4\" = C.\"FIPS 10-4\"\nWHERE T.year &gt;= 1980 AND T.year &lt;= 2020 AND T.month = 1 AND C.name = \"India\";\n\"\"\"\ndf = pd.read_sql_query(cmd, conn)\ndf\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\n1980\n1\n23.48\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\n1981\n1\n24.57\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\n1982\n1\n24.19\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\n1983\n1\n23.51\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\n1984\n1\n24.81\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nDARJEELING\n27.050\n88.270\n1983\n1\n5.10\n\n\n3148\nDARJEELING\n27.050\n88.270\n1986\n1\n6.90\n\n\n3149\nDARJEELING\n27.050\n88.270\n1994\n1\n8.10\n\n\n3150\nDARJEELING\n27.050\n88.270\n1995\n1\n5.60\n\n\n3151\nDARJEELING\n27.050\n88.270\n1997\n1\n5.70\n\n\n\n\n3152 rows × 6 columns\n\n\n\nWe did it! We can now proceed and create the general function for querying based on the user inputs.\n\ndef query_climate_database(country, year_begin, year_end, month):\n    cmd = \\\n    f\"\"\"\n    SELECT S.name, S.latitude, S.longitude, T.year, T.month, T.temp\n    FROM temperatures T\n    LEFT JOIN stations S ON T.id = S.id\n    LEFT JOIN countries C ON T.\"FIPS 10-4\" = C.\"FIPS 10-4\"\n    WHERE T.year &gt;= {year_begin} AND T.year &lt;= {year_end} AND T.month = {month} \n    AND C.name = \"{country}\";\n    \"\"\"\n    df = pd.read_sql_query(cmd, conn)\n    return df\n\n\nquery_climate_database(country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\n1980\n1\n23.48\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\n1981\n1\n24.57\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\n1982\n1\n24.19\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\n1983\n1\n23.51\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\n1984\n1\n24.81\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nDARJEELING\n27.050\n88.270\n1983\n1\n5.10\n\n\n3148\nDARJEELING\n27.050\n88.270\n1986\n1\n6.90\n\n\n3149\nDARJEELING\n27.050\n88.270\n1994\n1\n8.10\n\n\n3150\nDARJEELING\n27.050\n88.270\n1995\n1\n5.60\n\n\n3151\nDARJEELING\n27.050\n88.270\n1997\n1\n5.70\n\n\n\n\n3152 rows × 6 columns\n\n\n\nWe’ve done it! Now let’s test this for a different example just to make sure this is working.\n\nquery_climate_database(country = \"United States\", \n                       year_begin = 2000, \n                       year_end = 2010,\n                       month = 12)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nYear\nMonth\nTemp\n\n\n\n\n0\nADDISON\n34.2553\n-87.1814\n2001\n12\n8.43\n\n\n1\nADDISON\n34.2553\n-87.1814\n2002\n12\n4.73\n\n\n2\nADDISON\n34.2553\n-87.1814\n2005\n12\n3.65\n\n\n3\nADDISON\n34.2553\n-87.1814\n2009\n12\n5.33\n\n\n4\nADDISON\n34.2553\n-87.1814\n2010\n12\n2.00\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n75667\nLINCOLN_11_SW\n40.6953\n-96.8542\n2006\n12\n1.45\n\n\n75668\nLINCOLN_11_SW\n40.6953\n-96.8542\n2007\n12\n-4.18\n\n\n75669\nLINCOLN_11_SW\n40.6953\n-96.8542\n2008\n12\n-4.88\n\n\n75670\nLINCOLN_11_SW\n40.6953\n-96.8542\n2009\n12\n-6.97\n\n\n75671\nLINCOLN_11_SW\n40.6953\n-96.8542\n2010\n12\n-3.46\n\n\n\n\n75672 rows × 6 columns\n\n\n\n\ndf = query_climate_database(country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)\ndf\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\n1980\n1\n23.48\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\n1981\n1\n24.57\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\n1982\n1\n24.19\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\n1983\n1\n23.51\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\n1984\n1\n24.81\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nDARJEELING\n27.050\n88.270\n1983\n1\n5.10\n\n\n3148\nDARJEELING\n27.050\n88.270\n1986\n1\n6.90\n\n\n3149\nDARJEELING\n27.050\n88.270\n1994\n1\n8.10\n\n\n3150\nDARJEELING\n27.050\n88.270\n1995\n1\n5.60\n\n\n3151\nDARJEELING\n27.050\n88.270\n1997\n1\n5.70\n\n\n\n\n3152 rows × 6 columns\n\n\n\n\ndf[\"year_count\"] = df.groupby(\"NAME\")[\"Year\"].transform(\"count\")\ndf\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nYear\nMonth\nTemp\nyear_count\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\n1980\n1\n23.48\n34\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\n1981\n1\n24.57\n34\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\n1982\n1\n24.19\n34\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\n1983\n1\n23.51\n34\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\n1984\n1\n24.81\n34\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nDARJEELING\n27.050\n88.270\n1983\n1\n5.10\n7\n\n\n3148\nDARJEELING\n27.050\n88.270\n1986\n1\n6.90\n7\n\n\n3149\nDARJEELING\n27.050\n88.270\n1994\n1\n8.10\n7\n\n\n3150\nDARJEELING\n27.050\n88.270\n1995\n1\n5.60\n7\n\n\n3151\nDARJEELING\n27.050\n88.270\n1997\n1\n5.70\n7\n\n\n\n\n3152 rows × 7 columns\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\n\ndef coef(data_group):\n    x = data_group[[\"Year\"]] # 2 brackets because X should be a df\n    y = data_group[\"Temp\"]   # 1 bracket because y should be a series\n    LR = LinearRegression()\n    LR.fit(x, y)\n    return LR.coef_[0]\n\n\ncoefs = df.groupby([\"NAME\", \"Month\"]).apply(coef)\ncoefs = coefs.reset_index()\ncoefs\n\n\n\n\n\n\n\n\nNAME\nMonth\n0\n\n\n\n\n0\nAGARTALA\n1\n-0.006184\n\n\n1\nAGRA\n1\n-0.095413\n\n\n2\nAHMADABAD\n1\n0.006731\n\n\n3\nAKOLA\n1\n-0.008063\n\n\n4\nALLAHABAD\n1\n-0.029375\n\n\n...\n...\n...\n...\n\n\n99\nTRIVANDRUM\n1\n0.022892\n\n\n100\nUDAIPUR_DABOK\n1\n0.072424\n\n\n101\nVARANASI_BABATPUR\n1\n-0.012996\n\n\n102\nVERAVAL\n1\n0.024848\n\n\n103\nVISHAKHAPATNAM\n1\n-0.034050\n\n\n\n\n104 rows × 3 columns\n\n\n\n\npd.merge(df, coefs, on=\"NAME\")\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nYear\nMonth_x\nTemp\nyear_count\nMonth_y\n0\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\n1980\n1\n23.48\n34\n1\n0.026258\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\n1981\n1\n24.57\n34\n1\n0.026258\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\n1982\n1\n24.19\n34\n1\n0.026258\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\n1983\n1\n23.51\n34\n1\n0.026258\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\n1984\n1\n24.81\n34\n1\n0.026258\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nDARJEELING\n27.050\n88.270\n1983\n1\n5.10\n7\n1\n-0.040133\n\n\n3148\nDARJEELING\n27.050\n88.270\n1986\n1\n6.90\n7\n1\n-0.040133\n\n\n3149\nDARJEELING\n27.050\n88.270\n1994\n1\n8.10\n7\n1\n-0.040133\n\n\n3150\nDARJEELING\n27.050\n88.270\n1995\n1\n5.60\n7\n1\n-0.040133\n\n\n3151\nDARJEELING\n27.050\n88.270\n1997\n1\n5.70\n7\n1\n-0.040133\n\n\n\n\n3152 rows × 9 columns\n\n\n\nWe did it! We will now create a function that deals with plotting how the average yearly changes in temperature vary within a given country.\n\ndef temperature_coefficient_plot(country, year_begin, year_end, month, min_obs, **kwargs):\n    df = query_climate_database(country = country, \n                       year_begin = year_begin, \n                       year_end = year_end,\n                       month = month)\n    df[\"YearCount\"] = df.groupby(\"NAME\")[\"Year\"].transform(\"count\")\n    df = df[(df[\"YearCount\"] &gt;= min_obs)]\n    coefs = df.groupby([\"NAME\", \"Month\"]).apply(coef)\n    coefs = coefs.reset_index()\n    coefs = coefs.rename(columns={0: \"Estimated Yearly Increase (°C)\"})\n    df = pd.merge(df, coefs, on=\"NAME\")\n    df = df.round(4)\n    fig = px.scatter_mapbox(data_frame=df,\n                            lat=\"LATITUDE\",\n                            lon=\"LONGITUDE\",\n                            hover_name=\"NAME\",\n                            color=\"Estimated Yearly Increase (°C)\",\n                            **kwargs\n                            )\n    return fig\n\n\ncolor_map = px.colors.diverging.RdGy_r\nfig = temperature_coefficient_plot(\"India\", 1980, 2020, 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map\n                                   )\nfig.show()\n\n\n\n\nThis matches the plot that we were expecting to get! We can now see each station and look at the estimated yearly increase for each of them! Now let’s do another example, but for a different country.\n\nfig = temperature_coefficient_plot(\"United States\", 1960, 2010, 6, \n                                   min_obs = 12,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map\n                                   )\nfig.show()\n\n\n\n\nThe plot looks great! It seems that the estimated yearly increase has a much higher range than that of the previous plot."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Reetinav’s Blog",
    "section": "",
    "text": "Hand Gesture Media Player\n\n\n\n\n\n\n\nMachine Learning\n\n\nComputer Vision\n\n\nImage Recognition\n\n\nGUI\n\n\nPyTorch\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2023\n\n\nReetinav Das\n\n\n\n\n\n\n  \n\n\n\n\nHeat Diffusion Modeling\n\n\n\n\n\n\n\nNumba\n\n\nLinear Algebra\n\n\nCode Optimization\n\n\n\n\n\n\n\n\n\n\n\nDec 11, 2023\n\n\nReetinav Das\n\n\n\n\n\n\n  \n\n\n\n\nFake News Classification\n\n\n\n\n\n\n\nMachine Learning\n\n\nNatural Language Processing\n\n\nTensorFlow\n\n\nKeras\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2023\n\n\nReetinav Das\n\n\n\n\n\n\n  \n\n\n\n\nImage Classification: Cats and Dogs!\n\n\n\n\n\n\n\nMachine Learning\n\n\nImage Recognition\n\n\nComputer Vision\n\n\nTensorFlow\n\n\nKeras\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2023\n\n\nReetinav Das\n\n\n\n\n\n\n  \n\n\n\n\nWebscraping with Scrapy!\n\n\n\n\n\n\n\nWeb Scraping\n\n\nScrapy\n\n\nHTML\n\n\nData Analysis\n\n\n\n\n\n\n\n\n\n\n\nNov 12, 2023\n\n\nReetinav Das\n\n\n\n\n\n\n  \n\n\n\n\nNOAA Climate Data Analysis\n\n\n\n\n\n\n\nData Analysis\n\n\nPandas\n\n\nSQL\n\n\nDatabases\n\n\n\n\n\n\n\n\n\n\n\nOct 30, 2023\n\n\nReetinav Das\n\n\n\n\n\n\n  \n\n\n\n\nPlentiful Palmer Penguins\n\n\n\n\n\n\n\nData Analysis\n\n\n\n\n\n\n\n\n\n\n\nOct 16, 2023\n\n\nReetinav Das\n\n\n\n\n\n\nNo matching items"
  }
]